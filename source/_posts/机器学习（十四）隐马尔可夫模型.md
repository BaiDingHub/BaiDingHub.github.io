---
title: 机器学习（十四）隐马尔可夫模型
date: 2020-04-03 14:14:05
tags:
 - [机器学习]
 - [隐马尔可夫模型]
categories: 
 - [机器学习]
keyword: "机器学习,隐马尔可夫模型"
description: "机器学习（十四）隐马尔可夫模型"
cover: https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/cover.png?raw=true
---

<meta name="referrer" content="no-referrer"/>



# 一、模型介绍

 &emsp;&emsp;  概率图模型是一类用图来表达变量相关关系的概率模型。它以图为表示工具，最常见的是用一个节点表示一个或一组随机变量，结点之间的边表示变量间的概率相关关系，即"变量关系图"。

 &emsp;&emsp;  根据边的性质不同，概率图模型可大致分为两类：第一类是使用有向无环图表示变量间的依赖关系，称为**有向图模型或贝叶斯网**；第二类是使用无向图表示变量间的相关关系，称为**无向图模型或马尔可夫网**。

 &emsp;&emsp;  **隐马尔科夫模型(Hidden Markov Model，简称HMM)**是结构最简单的动态贝叶斯网，这是一种著名的有向图模型，**主要用于时序数据建模**，在**语音识别**、**自然语言处理**等领域有广泛应用。

 &emsp;&emsp;  马尔可夫模型（Markovmodel）描述了一类重要的随机过程，随机过程又称随机函数，是随时间而随机变化的过程。因此，我们要解决的就是描述系统随时间的变化情况。



<br>

## 1、算法模型

 &emsp;&emsp;  隐马尔科夫模型的图结构如下：

![1](https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/1.png?raw=true)

 &emsp;&emsp;  隐马尔可夫模型中的变量可分为两组：

- 第一组是**状态变量$\ \{y_1,y_2,...,y_n\}$** ，其中$\ y_i \in \mathcal{Y}$ 表示第$\ i$ 时刻的系统状态。通常假定状态变量是隐藏的、不可被观测的，因此状态变量亦称**隐变量**。下面用$\ I=(i_1,i_2,...,i_T)$ 来表示
- 第二组是**观测变量**$\ \{x_1,x_2,...,x_n\}$ ，其中$\ x_i \in \mathcal{X}$ 表示第$\ i$ 时刻的观测值。下面用$\ O=(o_1,o_2,...,o_T)$ 来表示。

 &emsp;&emsp;  设$\ Q$ 是所有可能的状态的集合，$\ V$ 是所有可能的观测的集合：
$$
Q=\{q_1,q_2,...,q_N\},\qquad V=\{v_1,v_2,..,v_M\}
$$
 &emsp;&emsp;  其中$\ N$ 是可能的状态数，$\ M$ 是可能的观测数。

**隐马尔可夫模型的假设**

 &emsp;&emsp;  隐马尔可夫模型作了两个基本假设：

- **齐次马尔可夫假设**，即假设隐藏的马尔可夫链在任意时刻$\ t$ 的状态只依赖于前一时刻的状态，与其他时刻的状态及观测无关，也于时刻$\ t$ 无关。
- **观测独立性假设**，即假设任一时刻的观测只依赖于该时刻的马尔科夫链的状态，与其他观测及状态无关。



 &emsp;&emsp;  这就是所谓的"**马尔科夫链**"，也称为**一阶马尔可夫模型**：即系统下一时刻的状态仅由当前状态决定，不依赖于以往的任何状态。若状态转移依赖于前n个状态，则称为**n阶马尔可夫模型**。

<br>

## 2、隐马尔可夫模型的其他参数

 &emsp;&emsp;  除了上面我们介绍的一些参数，如果想要完整的构建这样一个马尔可夫模型，还有要一些其他参数

**状态转移概率**

 &emsp;&emsp;  表示模型在各个状态间转换的概率，通常记为矩阵$\ A=[a_{ij}]_{N\times N}$ :
$$
a_{ij} = P(i_{t+1}=q_j|i_t=q_i),\qquad\qquad 1\le i,j\le N
$$
 &emsp;&emsp;  表示在任意时刻$\ t$ ，若状态为$\ q_i$ ，则在下一时刻状态为$\ q_j$ 的概率。

**输出观测概率**

 &emsp;&emsp;  模型根据当前状态获得各个观测值的概率，通常记为矩阵$\ B=[b_{ij}]_{N\times M}$ :
$$
b_{ij} = P(o_t = v_j|i_t=q_i),\qquad 1\le i\le N,1\le j\le M
$$
 &emsp;&emsp;  表示在任意时刻$\ t$ ，若状态为$\ q_i$ ，则观测值$\ v_j$ 被获取的概率

**初始状态概率**

 &emsp;&emsp;  模型在初始时刻各状态出现的概率，通常记为$\ \pi = (\pi_1,\pi_2,...,\pi_N)$ :
$$
\pi_i = P(i_1=q_i),\qquad 1\le i\le N
$$
 &emsp;&emsp;  表示模型的初始状态为$\ q_i$ 的概率。

<br>

## 3、隐马尔可夫模型构建过程

 &emsp;&emsp;  隐马尔可夫模型由初始状态概率向量$\ \pi$ ，状态转移概率矩阵$\ A$ 和观测概率矩阵$\ B$ 决定，$\ \pi$ 和$\ A$ 决定状态序列，$\ B$ 决定观测序列。因此，隐马尔可夫模型$\ \lambda$ 可以用三元符号表示，即
$$
\lambda = (A,B,\pi)
$$
 &emsp;&emsp;  $\ A,B,\pi$ 称为隐马尔可夫模型的三要素。

 &emsp;&emsp;  当我们已知马尔可夫模型时，其输出一个长度为$\ T$ 的观测序列$\ O=(o_1,o_2,...,o_T)$ 的生成过程为：

1. 设置$\ t=1$ ，并根据初始状态概率$\ \pi$ 来选择初始状态$\ i_1$ ;
2. 根据状态$\ i_t$ 和输出观测概率$\ B$ 选择观测变量取值$\ o_t$ ;
3. 根据状态$\ i_t$ 和状态转移矩阵$\ A$ 转移模型状态，即确定$\ i_{t+1}$ ;
4. 若$\ t<n$ ，设置$\ t=t+1$ ，并转移到第2步，否则停止。

<br>

## 4、隐马尔可夫模型的确定--示例

 &emsp;&emsp;  假设我们有3个盒子，每个盒子里都有红色和白色两种球，这三个盒子里球的数量分别是：

![2](https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/2.png?raw=true)

![3](https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/3.png?raw=true)

 &emsp;&emsp;  在这个例子中，有两个随机序列，一个是盒子的序列（状态序列），一个是球的颜色的观测序列（观测序列）。前者是隐藏的，只有后者是可观测的。这是一个隐马尔可夫模型的例子，根据所给条件，可以明确状态机和、观测集合、序列长度以及模型的三要素：

 &emsp;&emsp;  盒子对应状态，状态的集合是：
$$
Q=\{盒子1,盒子2,盒子3,盒子4\},\qquad N=4
$$
 &emsp;&emsp;  球的颜色对应观测，观测的集合是：
$$
V=\{红,白\},\qquad M=2
$$
 &emsp;&emsp;  状态序列和状态观测序列T=5.

 &emsp;&emsp;  初始概率分布为：
$$
\pi = (0.25,0.25,0.25,0.25)^T
$$
 &emsp;&emsp;  状态转移概率分布为：
$$
A= \left[ \begin{matrix}   0 & 1 & 0 & 0 \\   0.4 & 0 & 0.6 & 0 \\   0 & 0.4 & 0 & 0.6 \\   0 & 1 & 0.5 & 0.5  \end{matrix}  \right] \tag{3}
$$
 &emsp;&emsp;  观测概率分布为：
$$
A= \left[ \begin{matrix}   0.5 & 0.5\\   0.3 & 0.7\\   0.6 & 0.4\\   0.8 & 0.2  \end{matrix}  \right] \tag{3}
$$


<br>

## 5、隐马尔可夫模型的三个基本问题

- **概率计算问题**：给定模型$\ \lambda=(A,B,\pi)$ 和观测序列$\ O=(o_1,o_2,...,o_T)$ ，计算在模型$\ \lambda$ 下观测序列$\ O$ 出现的概率$\ P(O|\lambda)$ 。
- **学习问题：**给定观测序列$\ O=(o_1,o_2,...,o_T)$ ，如何调整模型参数$\ \lambda=[A,B,\pi]$ 使得该序列出现的概率$\ P(x|\lambda)$ 最大？即用极大似然估计、EM算法的方法估计参数
- **预测问题**，也称为解码问题：给定模型$\ \lambda=[A,B,\pi]$ 和观测序列$\ O=(o_1,o_2,...,o_T)$，求对给定观测序列条件概率$\ P(I|O)$ 最大的状态序列$\ I=(i_1,i_2,...,i_T)$ 。即给定观测序列，求最有可能的对应的状态序列。

对应三个基本问题的**使用场景**如下：

- **概率计算问题**：许多任务中需根据以往的观测序列$\ \{x_1,x_2,...,x_{n-1}\}$ 来推测当前时刻最有可能得观测值$\ x_n$ ，这显然可转换为求取概率$\ P(x|\lambda)$ 。
- **学习问题**：在大多数显示应用中，人工置顶模型参数已变得越来越不可行，如何根据训练样本学得最优得模型参数？
- **预测问题**：在语音识别等任务中，观测值为语音信号，隐藏状态为文字，目标就是根据观测信号来推断最有可能的状态序列（即对应的文字）

 



**参考链接**

- 周志华老师的《机器学习》
- 李航老师的《统计机器学习》