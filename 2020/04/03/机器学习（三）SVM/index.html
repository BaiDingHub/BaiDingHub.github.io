<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>机器学习（三）SVM | BaiDing's blog</title><meta name="description" content="机器学习（三）SVM"><meta name="keywords" content="机器学习,SVM"><meta name="author" content="白丁"><meta name="copyright" content="白丁"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="https://raw.githubusercontent.com/BaiDingHub/Blog_images/master/BlogSource/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="crossorigin"/><link rel="preconnect" href="http://ta.qq.com"/><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="x-1ef6P_miWkq-RJn_fmjd3KYumrXANNXYzK1myaLf0"/><meta name="baidu-site-verification" content="guD6l44Chk"/><meta name="twitter:card" content="summary"><meta name="twitter:title" content="机器学习（三）SVM"><meta name="twitter:description" content="机器学习（三）SVM"><meta name="twitter:image" content="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89SVM/cover.png?raw=true"><meta property="og:type" content="article"><meta property="og:title" content="机器学习（三）SVM"><meta property="og:url" content="http://baidinghub.github.io/2020/04/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89SVM/"><meta property="og:site_name" content="BaiDing's blog"><meta property="og:description" content="机器学习（三）SVM"><meta property="og:image" content="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89SVM/cover.png?raw=true"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>var autoChangeMode = '1'
var t = Cookies.get("theme")
if (autoChangeMode == '1'){
  var isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
  var isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
  var isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined){
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport){
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour < 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
  }
  } else if (t == 'light') activateLightMode()
  else activateDarkMode()

} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css" ><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://baidinghub.github.io/2020/04/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89SVM/"><link rel="prev" title="机器学习（四）Decision Tree决策树" href="http://baidinghub.github.io/2020/04/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89Decision%20Tree%E5%86%B3%E7%AD%96%E6%A0%91/"><link rel="next" title="机器学习（二）线性判别分析LDA" href="http://baidinghub.github.io/2020/04/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90LDA/"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.js" defer></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-162698439-1', 'auto');
ga('send', 'pageview');
</script><script src="https://tajs.qq.com/stats?sId=66540586" charset="UTF-8"></script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"4JGH9NW4XG","apiKey":"41cdb6d9ec4d21196956524e9c985b36","indexName":"baiding","hits":{"per_page":6},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  highlightShrink: 'false',
  isFontAwesomeV5: false,
  isPhotoFigcaption: false
  
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isSidebar: true  
  }</script><noscript><style>
#page-header {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/avatar.png?raw=true" onerror="onerror=null;src='https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/friend_404.gif?raw=true'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">97</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">60</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">17</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于作者</span></a></div></div></div></div><i class="fa fa-arrow-right on" id="toggle-sidebar" aria-hidden="true">     </i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1、模型介绍"><span class="toc-text">1、模型介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-模型"><span class="toc-text">1.1 模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-SVM原理介绍"><span class="toc-text">1.2 SVM原理介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-1-软间隔最大化"><span class="toc-text">1.2.1 软间隔最大化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-2-拉格朗日对偶"><span class="toc-text">1.2.2 拉格朗日对偶</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-3-最优化问题求解"><span class="toc-text">1.2.3 最优化问题求解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-4-核函数"><span class="toc-text">1.2.4 核函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-5-序列最小优化-Sequential-minimal-optimization"><span class="toc-text">1.2.5 序列最小优化 (Sequential minimal optimization)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2、模型分析"><span class="toc-text">2、模型分析</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-模型优缺点"><span class="toc-text">2.1 模型优缺点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-模型应用"><span class="toc-text">2.2 模型应用</span></a></li></ol></li></ol></div></div></div><div id="body-wrap"><div class="post-bg" id="nav" style="background-image: url(https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89SVM/cover.png?raw=true)"><div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">BaiDing's blog</a></span><span class="pull_right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于作者</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span></span></div><div id="post-info"><div id="post-title"><div class="posttitle">机器学习（三）SVM</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2020-04-03 14:03:05"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2020-04-03</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-12-02 11:11:09"><i class="fa fa-history" aria-hidden="true"></i> 更新于 2020-12-02</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><div class="meta-secondline"> <span class="post-meta-wordcount"><i class="post-meta__icon fa fa-file-word-o" aria-hidden="true"></i><span>字数总计:</span><span class="word-count">3.3k</span><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-clock-o" aria-hidden="true"></i><span>阅读时长: 13 分钟</span></span></div><div class="meta-thirdline"><span class="post-meta-pv-cv"><span class="post-meta__separator">|</span><i class="fa fa-eye post-meta__icon" aria-hidden="true"> </i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-comment-o" aria-hidden="true"></i><span>评论数:</span><a href="/2020/04/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89SVM/#post-comment"><span class="gitalk-comment-count comment-count"></span></a></span></div></div></div></div><main class="layout_post" id="content-inner"><article id="post"><div id="article-container"><meta name="referrer" content="no-referrer"/>

<h1 id="1、模型介绍"><a href="#1、模型介绍" class="headerlink" title="1、模型介绍"></a>1、模型介绍</h1><p> &emsp;&emsp; 支持向量机 SVM 模型，它利用了<strong>软间隔最大化</strong>、<strong>拉格朗日对偶</strong>、<strong>凸优化</strong>、<strong>核函数</strong>、<strong>序列最小优化</strong>等方法。支持向量机既可以解决线性可分的分类问题，也可完美解决线性不可分问题。</p>
<p>&emsp;&emsp; <strong>支持向量</strong>是距离分类超平面近的那些点，SVM 的思想就是使得<strong>支持向量到分类超平面的间隔最大化</strong>。</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89SVM/1.png?raw=true"  alt="1"></p>
<h2 id="1-1-模型"><a href="#1-1-模型" class="headerlink" title="1.1 模型"></a>1.1 模型</h2><p> &emsp;&emsp;  对于模型输入$ x$ ，模型参数$ w$ ，模型输出$ h(x) $ ，预测结果$ y\in{0,1}$ </p>
<script type="math/tex; mode=display">
h(x) = g(w^Tx) = \frac{1}{1+e^{-w^Tx}} \\
y =
\begin{cases}
0 & & if&h(x)<C\\
1 & & if&h(x)>C
\end{cases}</script><p> &emsp;&emsp; 其中$ g(x)$ 是<strong>Sigmoid函数</strong>，其函数形式如下：</p>
<script type="math/tex; mode=display">
g(x) = \frac{1}{1+e^{-x}}</script><p> &emsp;&emsp; 其中$ C$ 是一个常数，是分类阈值，通常取0.5</p>
<h2 id="1-2-SVM原理介绍"><a href="#1-2-SVM原理介绍" class="headerlink" title="1.2 SVM原理介绍"></a>1.2 SVM原理介绍</h2><p> &emsp;&emsp; SVM也是一种线性的分类器，我们需要得到其权重$ w$，首先，要找到其<strong>目标函数</strong>与<strong>约束条件</strong></p>
<h3 id="1-2-1-软间隔最大化"><a href="#1-2-1-软间隔最大化" class="headerlink" title="1.2.1 软间隔最大化"></a>1.2.1 软间隔最大化</h3><p> &emsp;&emsp; 我们假设，SVM的分类超平面为$ w^Tx+b=0$ ，点到平面的距离为$ d = \frac{|w^Tx+b|}{||w||}$，输入样本$ x_i$ ，对应的label为$ y_i \in {-1,1}$。</p>
<p> &emsp;&emsp; 我们可以得到，样本$ x_i$到分类超平面的距离</p>
<script type="math/tex; mode=display">
r_i = y_i(\frac{w^Tx+b}{||w||})</script><p> &emsp;&emsp; 根据SVM的原理，我们需要找到距离超平面最近的点，即支持向量：</p>
<script type="math/tex; mode=display">
r = min \ r_i</script><p> &emsp;&emsp; 同时，我们知道，支持向量离超平面越远越好，这就我们就得到了其<strong>目标函数和约束条件</strong>：</p>
<script type="math/tex; mode=display">
max \ r \\
s.t \quad r_i=y_i(\frac{w^Tx+b}{||w||}) \ge r</script><p> &emsp;&emsp; <strong>稍加转变</strong>，令$ r=\hat{r} /||w||$，则上面的目标函数变成：</p>
<script type="math/tex; mode=display">
max \ \frac{\hat{r}}{||w||} \\
s.t \quad y_i(w^Tx_i+b) \ge \hat{r}</script><p> &emsp;&emsp; 由于 w, b 成倍数变化并不会影响超平面的公式，我们不妨让$ w’ = w/\hat{r}，b’ = w/\hat{b}$ ，变换完成后，再令$ w = w’,b = b’$ 。得到<strong>目标函数</strong>与<strong>约束条件</strong>。</p>
<script type="math/tex; mode=display">
max \ \frac{1}{||w||} \\
s.t \quad y_i(w^Tx_i+b) \ge 1</script><p> &emsp;&emsp; 上面的目标函数和约束条件，对于所有的点必须严格成立才行。而总会有数据，是一个超平面分不开的，会有一些特异点：</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89SVM/2.png?raw=true"  alt="2"></p>
<p> &emsp;&emsp; 为了解决这个问题，我们要引入松弛变量，使其约束条件变成：</p>
<script type="math/tex; mode=display">
s.t. \quad y_i(w^Tx_i+b) + \epsilon_i \ge 1 \\
\epsilon_i \ge 0</script><p> &emsp;&emsp; 同时，我们可以将目标函数进行更改，修改为：</p>
<script type="math/tex; mode=display">
max\ \frac{1}{||w||} \Rightarrow min\ \frac{1}{2}||w||^2</script><p> &emsp;&emsp; 所以，我们<strong>最后得到的目标函数与约束条件</strong>为：</p>
<script type="math/tex; mode=display">
min\ \frac{1}{2}||w||^2+C\sum \epsilon_i \\
s.t. \quad y_i(w^Tx_i+b) + \epsilon_i \ge 1 \\
\epsilon_i \ge 0</script><p>​    其中，C为惩罚参数，它的目的是使得目标变量最小即几何间隔最大，且使得松弛变量最小化。加入松弛变量的目标函数就是软间隔最大化。</p>
<h3 id="1-2-2-拉格朗日对偶"><a href="#1-2-2-拉格朗日对偶" class="headerlink" title="1.2.2 拉格朗日对偶"></a>1.2.2 拉格朗日对偶</h3><p> &emsp;&emsp; 我们需要使用拉格朗日乘子法来对上面的凸二次优化问题进行求解，得到的拉格朗日函数如下：</p>
<script type="math/tex; mode=display">
L(w,b,\epsilon,\alpha,\mu) =\frac{1}{2}||w||^2+C\sum \epsilon_i - \sum \alpha_i(y_i(w^Tx_i+b)-1+\epsilon_i) - \sum \mu_i \epsilon_i \\
\alpha_i \ge 0,\mu_i \ge 0</script><p> &emsp;&emsp; 从上式可以看出，$ \alpha=0,\mu = 0$ 可使上式最大，即：</p>
<script type="math/tex; mode=display">
max_{\alpha,\mu}L(w,b,\epsilon,\alpha,\mu) =\frac{1}{2}||w||^2+C\sum \epsilon_i</script><p> &emsp;&emsp; 因此，原目标函数就变成了：</p>
<script type="math/tex; mode=display">
min_{w,b,\epsilon} max_{\alpha,\mu} L(w,b,\epsilon,\alpha,\mu)</script><p> &emsp;&emsp; 而求解上面的问题很困难，我们要对其进行转换，利用拉格朗日对偶性，可通过求解原最优化问题的对偶问题得到原问题的最优解。原最优化问题的<strong>对偶问题</strong>为：</p>
<script type="math/tex; mode=display">
max_{\alpha,\mu} min_{w,b,\epsilon}  L(w,b,\epsilon,\alpha,\mu)</script><p> &emsp;&emsp; 利用拉格朗日的对偶性，将问题转换成了极大极小化拉格朗日函数的问题</p>
<h3 id="1-2-3-最优化问题求解"><a href="#1-2-3-最优化问题求解" class="headerlink" title="1.2.3 最优化问题求解"></a>1.2.3 最优化问题求解</h3><p> &emsp;&emsp; 对于极大极小化拉格朗日函数的问题，首先要求解关于拉格朗日函数的极小化问题。</p>
<p> &emsp;&emsp; 对三个变量分别求偏导得：</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial w} = w - \sum \alpha_iy_ix_i=0 \\
\frac{\partial L}{\partial b} =  - \sum \alpha_iy_i=0 \\
\frac{\partial L}{\partial \epsilon_i} = C - \alpha_i - \mu_i =0</script><p> &emsp;&emsp; 将以上三式带入拉格朗日函数中得：</p>
<script type="math/tex; mode=display">
min_{w,b,\epsilon} L = -\frac{1}{2}\sum\sum\alpha_i\alpha_jy_iy_j(x_i·x_j) + \sum \alpha_i</script><p> &emsp;&emsp; 那么极大极小化拉格朗日函数转换成：</p>
<script type="math/tex; mode=display">
max_{\alpha,\mu} min_{w,b,\epsilon} L = min_{\alpha,\mu} \frac{1}{2}\sum\sum\alpha_i\alpha_jy_iy_j(x_i·x_j) - \sum \alpha_i \\
s.t. \quad \sum \alpha_iy_i=0 \\
\quad \quad \quad 0 \le \alpha_i \le C</script><h3 id="1-2-4-核函数"><a href="#1-2-4-核函数" class="headerlink" title="1.2.4 核函数"></a>1.2.4 核函数</h3><p> &emsp;&emsp; 对于线性不可分问题，这类问题是无法用超平面划分正负样本数据的，但如果我们可以把原本不可分的数据转换成线性可分的数据，那么我们就可以用SVM进行求解。如下图所示：</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89SVM/4.png?raw=true"  alt="3"></p>
<p> &emsp;&emsp; 对于左侧的情况，我们使用了一个曲面作为分类边界，而当我们转换成右侧的坐标系中后，我们就可以使用SVM进行解决了。</p>
<p> &emsp;&emsp; 对于曲面分类边界，其函数形式为：</p>
<script type="math/tex; mode=display">
k_1x_1^2 + k_2x_2^2 + k_3x_1 + k_4x_2 + k_5x_1x_2 + k_6 = 0</script><p> &emsp;&emsp; 映射到新坐标系中：</p>
<script type="math/tex; mode=display">
z_1 = x_1^2,z_2 = x_2^2,z_3 = \sqrt{2}x_1,z_4 = \sqrt{2}x_2,z_5 = \sqrt{2}x_1x_2</script><p> &emsp;&emsp; 那么在新的坐标系下，其超平面为：</p>
<script type="math/tex; mode=display">
k_1'z_1 + k_2'z_2 + k_3'z_3 + k_4'z_4 + k_5'z_5 + k_6'=0</script><p> &emsp;&emsp; 也就是将在二维空间(x1,x2)下线性不可分的问题转换成了在五维空间(z1,z2,z3,z4,z5)下线性可分的问题。</p>
<p> &emsp;&emsp; 那么，在二维空间中，对于两个点$ p,q$ 的内积，我们有$ (p·q) = (p_1·q_1+p_2·q_2)$ ,当将其转换到五维空间后，他们的内积，则为$ (\varphi(p),\varphi(q)) = p_1^2q_1^2+p_2^2q_2^2+2p_1q_1+2p_2q_2+2p_1q_1p_2q_2$ 。</p>
<p> &emsp;&emsp; 那么我们可以定义一个核函数$ k(p,q)$，使得：</p>
<script type="math/tex; mode=display">
k(p,q) = ((p·q)+1)^2 = p_1^2q_1^2+p_2^2q_2^2+2p_1q_1+2p_2q_2+2p_1q_1p_2q_2 = (\varphi(p),\varphi(q)) + 1</script><p> &emsp;&emsp; 所以，我们使用核函数，可以根据低维空间的数据，得到高维空间的内积，利用核函数，无需先将变量一一映射到高维空间再计算内积，而是简单得在低维空间中利用核函数完成这一操作。</p>
<p> &emsp;&emsp; 那么，我们为什么要用内积呢，因为在上面的优化函数里，我们只需要计算两个点的内积即可。因此，原目标函数变为：</p>
<script type="math/tex; mode=display">
max_{\alpha} min_{w,b,\epsilon} L = min_{\alpha,\mu} \frac{1}{2}\sum\sum\alpha_i\alpha_jy_iy_jK(x_i·x_j) - \sum \alpha_i \\
s.t. \quad \sum \alpha_iy_i=0 \\
\quad \quad \quad 0 \le \alpha_i \le C</script><h3 id="1-2-5-序列最小优化-Sequential-minimal-optimization"><a href="#1-2-5-序列最小优化-Sequential-minimal-optimization" class="headerlink" title="1.2.5 序列最小优化 (Sequential minimal optimization)"></a>1.2.5 序列最小优化 (Sequential minimal optimization)</h3><p> &emsp;&emsp; 到目前为止，优化问题已经转化成了一个包含 N 个 $ \alpha$ 自变量的目标变量和两个约束条件。由于目标变量中自变量 $ \alpha$  有 N 个，为了便与求解，每次选出一对自变量 $ \alpha$ ，然后求目标函数关于其中一个 $ \alpha$  的偏导，这样就可以得到这一对 $ \alpha$  的新值。给这一对  $ \alpha$  赋上新值，然后不断重复选出下一对  $ \alpha$  并执行上述操作，直到达到最大迭代数或没有任何自变量 $ \alpha$  再发生变化为止，这就是 SMO 的基本思想。说直白些，SMO 就是在约束条件下对目标函数的优化求解算法。 </p>
<p> &emsp;&emsp; 为何不能每次只选一个自变量进行优化？那是因为只选一个自变量 $ \alpha$ 的话，会违反第一个约束条件，即所有$ \alpha$ 和 y 值乘积的和等于 0。</p>
<p><strong>选择两个自变量</strong></p>
<p> &emsp;&emsp; 下面是详细的 <strong>SMO 过程</strong>。假设选出了两个自变量分别是$ \alpha_1$   和$ \alpha_2$  ，除了这两个自变量之外的其他自变量保持固定，则<strong>目标变量和约束条件</strong>转化为：</p>
<script type="math/tex; mode=display">
min_{\alpha_1,\alpha_2}L(\alpha_1,\alpha_2) = \frac{1}{2}K_{11}\alpha_1^2 + \frac{1}{2}K_{22}\alpha_2^2 + y_1y_2K_{12}\alpha_1\alpha_2- \\(\alpha_1+\alpha_2)+y_1\alpha_1\sum_{i=3}^Ny_i\alpha_iK_{i1}+y_2\alpha_2\sum_{i=3}^Ny_i\alpha_iK_{i2} \\
s.t. \quad \alpha_1y_1+\alpha_2y_2 = -\sum_{i=3}^Ny_i\alpha_i = \delta \\
 \quad 0 \le \alpha_i \le C</script><p><strong>求 $ \alpha_2^{new,unc}$  </strong></p>
<p> &emsp;&emsp; 将约束条件中的 $ \alpha_1$ 用 $ \alpha_2$  表示，并代入目标函数中，则将目标函数转化成只包含 $ \alpha_2$  的目标函数，让该目标函数对 $ \alpha_2$ 的偏导等于 0，可<strong>求得 $ \alpha_2$  未经修剪的值</strong>：</p>
<script type="math/tex; mode=display">
\alpha_2^{new,unc} = \alpha_2^{old} + \frac{y_2(E_1 - E_2)}{\epsilon} \\
\epsilon = K_{11}+K_{22}-2K_{12} \\
E_i = f(x_i) - y_i = \sum_{j=1}^N\alpha_j y_j K(x_j,x_i)+b - y_i</script><p><strong>修剪 $ \alpha_2$  </strong></p>
<p> &emsp;&emsp; 之所以说$ \alpha_2$ 是未经修剪的值是因为所有 alpha 都必须满足大于等于 0 且小于等于 C 的约束条件，用此约束条件将 $ \alpha_2$ 进行修剪，<strong>修剪过程</strong>如下：</p>
<script type="math/tex; mode=display">
\alpha_1^{old}y_1+\alpha_2^{old}y_2 = \alpha_1^{new}y_1 + \alpha_2^{new}y_2 \\
0 \le \alpha_1^{new} \le C \\
0 \le \alpha_2^{new} \le C</script><p> &emsp;&emsp; 由此可得：</p>
<script type="math/tex; mode=display">
0 \le \frac{\alpha_1^{old}y_1+\alpha_2^{old}y_2 -  \alpha_2^{new}y_2}{y_1} \le C \\
0 \le \alpha_2^{new} \le C</script><p> &emsp;&emsp; 分两种情况讨论：</p>
<p> &emsp;&emsp; 情况 1.当 y1 等于 y2 时，有：</p>
<script type="math/tex; mode=display">
\alpha_1^{old}+\alpha_2^{old}-C \le \alpha_2^{new} \le \alpha_1^{old} + \alpha_2^{old} \\
0 \le \alpha_2^{new} \le C \\
L = max(0,\alpha_1^{old}+\alpha_2^{old}-C) \\
H = min(C,\alpha_1^{old}+\alpha_2^{old})</script><p> &emsp;&emsp; 情况 2.当 y1 不等于 y2 时，有：</p>
<script type="math/tex; mode=display">
\alpha_2^{old}-\alpha_2^{old} \le \alpha_2^{new} \le C - \alpha_1^{old} + \alpha_2^{old} \\
0 \le \alpha_2^{new} \le C \\
L = max(0,\alpha_2^{old}-\alpha_2^{old}) \\
H = min(C,C - \alpha_1^{old} + \alpha_2^{old} )</script><p> &emsp;&emsp; 修剪后，可得 alpha2 的取值如下：</p>
<script type="math/tex; mode=display">
\alpha_2^{new} =
\begin{cases}
H & & \alpha_2^{new,unc} \ge H\\
\alpha_2^{new,unc} & & L \le \alpha_2^{new,unc} \le H \\
L & & \alpha_2^{new,unc} \le L
\end{cases}</script><p><strong>得到$ \alpha_1$ </strong></p>
<p> &emsp;&emsp; 由 $ \alpha_2$ 和 $ \alpha_1$ 的关系，可得：</p>
<script type="math/tex; mode=display">
\alpha_1^{new} = \alpha_1^{old} + y_1y_2(\alpha_2^{old} - \alpha_2^{new})</script><p><strong>更新阈值b</strong></p>
<p> &emsp;&emsp; 当我们更新了一对$ \alpha_1,\alpha_2$之后都需要重新计算阈值 $ b$ ，因为 $ b$关系到我们$ f(x)$ 的计算，关系到下次优化的时候误差$ E_i$ 的计算。在完成 $ \alpha_1$ 和 $ \alpha_2$  的一轮更新后，我们来更新 b 的值，</p>
<p> &emsp;&emsp; 由$ y = w^Tx+b$ 以及KKT条件$ w=\sum \alpha_i y_i x_i$ 可知:</p>
<p> &emsp;&emsp; 当 $ \alpha_1$   更新后的值满足 $ 0 \le \alpha_1 \le C$  时：</p>
<script type="math/tex; mode=display">
\sum \alpha_iy_iK(x_i,x_1) + b = y_1</script><p> &emsp;&emsp; 根据上式，我们可以得出：</p>
<script type="math/tex; mode=display">
b_1 = -E_1 -y_1K_{11}(\alpha_1^{new} - \alpha_1^{old}) - y_2K_{21}(\alpha_2^{new} - \alpha_2^{old}) + b^{old}</script><p> &emsp;&emsp; 同样的， 当 $ \alpha_2$   更新后的值满足 $ 0 \le \alpha_2 \le C$  时：</p>
<script type="math/tex; mode=display">
b_2 = -E_2 -y_1K_{12}(\alpha_1^{new} - \alpha_1^{old}) - y_2K_{22}(\alpha_2^{new} - \alpha_2^{old}) + b^{old}</script><p> &emsp;&emsp; 若更新后的 $ \alpha_1$ 和 $ \alpha_2$ 同时满足大于 0 且小于 C 的条件，那么 b = b1=b2;否则，b =(b1+b2)/2。</p>
<p><strong>如何选择 $ \alpha_1$  和 $ \alpha_2$</strong>  </p>
<p> &emsp;&emsp; 在程序开始时，初始化所有的$ \alpha$ 为0（当$ \alpha$ 已知时，所有的参数都能够计算得到）</p>
<p> <strong>$ \alpha_1$ 的选择</strong></p>
<p> &emsp;&emsp; SMO称选择第1个变量的过程为<strong>外层循环</strong>。在这里我们在<strong>整个样本集</strong>和<strong>非边界样本集</strong>间进行交替。</p>
<p> &emsp;&emsp; 首先，我们对整个训练集进行遍历，要选择一个违背下面的KKT条件的$ \alpha$  为$ \alpha_1$ ：</p>
<script type="math/tex; mode=display">
y_i f(x_i) 
\begin{cases}
\ge 1 & & \alpha=0(样本点落在最大间隔外(分类完全正确的那些样本))\\
= 1 & & L \le \alpha \le H(样本点刚好落在最大间隔边界上) \\
\le 1 & & \alpha=C (样本点落在最大间隔内部) 
\end{cases}</script><p> &emsp;&emsp; 在遍历了整个训练集并优化了相应的$ \alpha$ 后第二轮迭代我们仅仅需要遍历其中的非边界$ \alpha$ . 所谓的非边界$ \alpha$ 就是指那些不等于边界0或者C的$ \alpha$ 值。因为这些样本点更容易违反KKT条件</p>
<p> &emsp;&emsp; 之后就是不断地在两个数据集中来回交替，最终所有的$ \alpha$  都满足KKT条件的时候，算法中止。</p>
<p> <strong>$ \alpha_2$ 的选择</strong></p>
<p> &emsp;&emsp; SMO称选择第2个变量为<strong>内层循环</strong>。</p>
<p> &emsp;&emsp; 当我们已经选取第一个 $ \alpha_1$ 之后，我们希望我们选取的第二个变量 $ \alpha_2$ 优化后能有较大的变化。根据我们之前推导的式子$ \alpha_2^{new,unc} = \alpha_2^{old} + \frac{y_2(E_1 - E_2)}{\epsilon}$ 可以知道，新的 $ \alpha_2$ 的变化依赖于|E1−E2|, 当E1为正时， 那么选择最小的Ei作为E2，通常将每个样本的Ei缓存到一个列表中，通过在列表中选择具有|E1−E2|的α2来近似最大化步长。</p>
<h1 id="2、模型分析"><a href="#2、模型分析" class="headerlink" title="2、模型分析"></a>2、模型分析</h1><h2 id="2-1-模型优缺点"><a href="#2-1-模型优缺点" class="headerlink" title="2.1 模型优缺点"></a>2.1 模型优缺点</h2><p><strong>优点</strong></p>
<ul>
<li>对于边界清晰的分类问题效果好；</li>
<li>对高维分类问题效果好；</li>
<li>当维度高于样本数的时候，SVM 较为有效；</li>
<li>因为最终只使用训练集中的支持向量，所以节约内存</li>
<li>SVM能够忽略离散值，对离散值（单独的分类错误的点）具有鲁棒性</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>当数据量较大时，训练时间会较长；</li>
<li>当数据集的噪音过多时，表现不好；</li>
<li>经典的 SVM 算法仅支持二分类，对于多分类问题需要改动模型;</li>
</ul>
<h2 id="2-2-模型应用"><a href="#2-2-模型应用" class="headerlink" title="2.2 模型应用"></a>2.2 模型应用</h2><ul>
<li>SVM可以应用于<strong>二分类任务</strong></li>
<li>改动后，可以进行<strong>多分类任务</strong>的求解</li>
</ul>
<p><strong>参考链接</strong></p>
<ul>
<li><a href="https://www.ibm.com/developerworks/cn/analytics/library/machine-learning-hands-on1-svn/index.html" target="_blank" rel="noopener">支持向量机的原理和实现</a></li>
</ul>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">白丁</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://baidinghub.github.io/2020/04/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89SVM/">http://baidinghub.github.io/2020/04/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89SVM/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://baidinghub.github.io" target="_blank">BaiDing's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a class="post-meta__tags" href="/tags/SVM/">SVM</a></div><div class="post_share"><div class="addthis_inline_share_toolbox"></div><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDyIdOn/M/aXcGQOiSDVMqvN2gxDdiESHUfFuB2YMy48fvNN9SZOQUbVlGF4Pk6nDXIAir+br/EWuEnNLtgOCYMo/BTxl29gqS/QGHPiDaIQedzmLcuRZpfDuGit61N/b9pyktpZLagBgbl5Ox9mAgWQxXyhxYB092gyOXqrBULBeZUYQ+H7Eupha10QTQghHv4nLk+oYWo2UXEiijQpE3qMXT32G8v8k0KbRdd1hIFPyNEx6eZ6Buc2ZdbMtoutdGjvdnw5B1+dxgl1egESPChzcwCMxt3NW/3faQ5lsjRhQD4fRU+Ua/aqXe+0xg2+xr0BTjNr/JZIAaNiLuo71lH 1564026260@qq.com" async="async"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/wechat.png?raw=true" alt="微信"/><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/alipay.jpg?raw=true" alt="支付寶"/><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/04/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89Decision%20Tree%E5%86%B3%E7%AD%96%E6%A0%91/"><img class="prev_cover lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89Decision%20Tree%E5%86%B3%E7%AD%96%E6%A0%91/cover.png?raw=true" onerror="onerror=null;src='https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/404.jpg?raw=true'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">机器学习（四）Decision Tree决策树</div></div></a></div><div class="next-post pull_right"><a href="/2020/04/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90LDA/"><img class="next_cover lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90LDA/cover.png?raw=true" onerror="onerror=null;src='https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/404.jpg?raw=true'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">机器学习（二）线性判别分析LDA</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/04/03/机器学习（一）Logistic回归/" title="机器学习（一）Logistic回归"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89Logistic%E5%9B%9E%E5%BD%92/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">机器学习（一）Logistic回归</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/机器学习（七）集成学习/" title="机器学习（七）集成学习"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%83%EF%BC%89%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">机器学习（七）集成学习</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/机器学习（九）集成学习之GDBT/" title="机器学习（九）集成学习之GDBT"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8BGDBT/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">机器学习（九）集成学习之GDBT</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/机器学习（二）线性判别分析LDA/" title="机器学习（二）线性判别分析LDA"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90LDA/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">机器学习（二）线性判别分析LDA</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/机器学习（五）K近邻算法/" title="机器学习（五）K近邻算法"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%94%EF%BC%89K%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">机器学习（五）K近邻算法</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/机器学习（十一）K-means/" title="机器学习（十一）K-means"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89K-means/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">机器学习（十一）K-means</div></div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: 'd3948be001a91411dfd9',
  clientSecret: 'ebddf2a2a5a039922fb373a8a8c0efcc439bf6ca',
  repo: 'BaiDingHub.github.io',
  owner: 'BaiDingHub',
  admin: ['BaiDingHub'],
  id: md5(decodeURI(location.pathname)),
  language: 'zh-CN',
  perPage: 10,
  distractionFreeMode: false,
  pagerDirection: 'last',
  createIssueManually: false,
  updateCountCallback: commentCount
})
gitalk.render('gitalk-container')

function commentCount(n){
  try {
    document.getElementsByClassName('gitalk-comment-count')[0].innerHTML= n
  } catch (e) {
    return false
  }
}</script></div></article></main><footer id="footer" style="background-image: url(https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89SVM/cover.png?raw=true)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By 白丁</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://jerryc.me/" target="_blank" rel="noopener">blog</a>!</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js" ></script><script src="/js/utils.js" ></script><script src="/js/main.js" ></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><script src="/js/search/algolia.js"></script></body></html>