<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>目标检测（六）之YOLO v2 | BaiDing's blog</title><meta name="description" content="目标检测（六）之YOLO v2"><meta name="keywords" content="深度基础知识,目标检测"><meta name="author" content="白丁"><meta name="copyright" content="白丁"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="https://raw.githubusercontent.com/BaiDingHub/Blog_images/master/BlogSource/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="crossorigin"/><link rel="preconnect" href="http://ta.qq.com"/><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="x-1ef6P_miWkq-RJn_fmjd3KYumrXANNXYzK1myaLf0"/><meta name="baidu-site-verification" content="guD6l44Chk"/><meta name="twitter:card" content="summary"><meta name="twitter:title" content="目标检测（六）之YOLO v2"><meta name="twitter:description" content="目标检测（六）之YOLO v2"><meta name="twitter:image" content="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B(%E5%85%AD)%E4%B9%8BYOLO%20v2/cover.png?raw=true"><meta property="og:type" content="article"><meta property="og:title" content="目标检测（六）之YOLO v2"><meta property="og:url" content="http://baidinghub.github.io/2020/04/03/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B(%E5%85%AD)%E4%B9%8BYOLO%20v2/"><meta property="og:site_name" content="BaiDing's blog"><meta property="og:description" content="目标检测（六）之YOLO v2"><meta property="og:image" content="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B(%E5%85%AD)%E4%B9%8BYOLO%20v2/cover.png?raw=true"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>var autoChangeMode = '1'
var t = Cookies.get("theme")
if (autoChangeMode == '1'){
  var isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
  var isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
  var isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined){
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport){
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour < 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
  }
  } else if (t == 'light') activateLightMode()
  else activateDarkMode()

} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css" ><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://baidinghub.github.io/2020/04/03/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B(%E5%85%AD)%E4%B9%8BYOLO%20v2/"><link rel="prev" title="目标检测（七）之YOLO v3" href="http://baidinghub.github.io/2020/04/03/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%88%E4%B8%83%EF%BC%89%E4%B9%8BYOLO%20v3/"><link rel="next" title="目标检测（五）之YOLOv1" href="http://baidinghub.github.io/2020/04/03/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B(%E4%BA%94)%E4%B9%8BYOLOv1/"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.js" defer></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-162698439-1', 'auto');
ga('send', 'pageview');
</script><script src="https://tajs.qq.com/stats?sId=66540586" charset="UTF-8"></script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"4JGH9NW4XG","apiKey":"41cdb6d9ec4d21196956524e9c985b36","indexName":"baiding","hits":{"per_page":6},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  highlightShrink: 'false',
  isFontAwesomeV5: false,
  isPhotoFigcaption: false
  
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isSidebar: true  
  }</script><noscript><style>
#page-header {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/avatar.png?raw=true" onerror="onerror=null;src='https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/friend_404.gif?raw=true'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">97</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">60</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">17</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于作者</span></a></div></div></div></div><i class="fa fa-arrow-right on" id="toggle-sidebar" aria-hidden="true">     </i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#一、论文相关信息"><span class="toc-text">一、论文相关信息</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#emsp-emsp-1-论文题目"><span class="toc-text">&amp;emsp;&amp;emsp;1.论文题目</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#emsp-emsp-2-论文时间"><span class="toc-text">&amp;emsp;&amp;emsp;2.论文时间</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#emsp-emsp-3-论文文献"><span class="toc-text">&amp;emsp;&amp;emsp;3.论文文献</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#emsp-emsp-4-论文源码"><span class="toc-text">&amp;emsp;&amp;emsp;4.论文源码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#二、论文背景及简介"><span class="toc-text">二、论文背景及简介</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#三、YOLO-v2的一系列改进"><span class="toc-text">三、YOLO v2的一系列改进</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1、总览"><span class="toc-text">1、总览</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2、Better"><span class="toc-text">2、Better</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Batch-Normalization"><span class="toc-text">(1) Batch Normalization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-High-Resolution-Classifier"><span class="toc-text">(2) High Resolution Classifier</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Convolutional-With-Anchor-Boxes"><span class="toc-text">(3) Convolutional With Anchor Boxes</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Dimension-Clusters（维度聚类）"><span class="toc-text">(4) Dimension Clusters（维度聚类）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Direct-loaction-prediction（最后的回归修正过程）"><span class="toc-text">(5) Direct loaction prediction（最后的回归修正过程）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#emsp-emsp-emsp-5-Fine-Grained-Features（细粒度特征）"><span class="toc-text">&amp;emsp;&amp;emsp;&amp;emsp; (5) Fine-Grained Features（细粒度特征）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Multi-Scale-Training"><span class="toc-text">(6) Multi-Scale Training</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3、Faster"><span class="toc-text">3、Faster</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Darknet-19"><span class="toc-text">(1) Darknet-19</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Training-for-classification"><span class="toc-text">(2) Training for classification</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Training-for-detection"><span class="toc-text">(3) Training for detection</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4、Stronger"><span class="toc-text">4、Stronger</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#四、实验结果"><span class="toc-text">四、实验结果</span></a></li></ol></div></div></div><div id="body-wrap"><div class="post-bg" id="nav" style="background-image: url(https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B(%E5%85%AD)%E4%B9%8BYOLO%20v2/cover.png?raw=true)"><div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">BaiDing's blog</a></span><span class="pull_right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于作者</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span></span></div><div id="post-info"><div id="post-title"><div class="posttitle">目标检测（六）之YOLO v2</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2020-04-03 12:06:05"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2020-04-03</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-12-02 11:11:09"><i class="fa fa-history" aria-hidden="true"></i> 更新于 2020-12-02</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><i class="fa fa-angle-right post-meta__separator" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">目标检测</a></span></div><div class="meta-secondline"> <span class="post-meta-wordcount"><i class="post-meta__icon fa fa-file-word-o" aria-hidden="true"></i><span>字数总计:</span><span class="word-count">3.3k</span><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-clock-o" aria-hidden="true"></i><span>阅读时长: 11 分钟</span></span></div><div class="meta-thirdline"><span class="post-meta-pv-cv"><span class="post-meta__separator">|</span><i class="fa fa-eye post-meta__icon" aria-hidden="true"> </i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-comment-o" aria-hidden="true"></i><span>评论数:</span><a href="/2020/04/03/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B(%E5%85%AD)%E4%B9%8BYOLO%20v2/#post-comment"><span class="gitalk-comment-count comment-count"></span></a></span></div></div></div></div><main class="layout_post" id="content-inner"><article id="post"><div id="article-container"><meta name="referrer" content="no-referrer"/>

<h1 id="一、论文相关信息"><a href="#一、论文相关信息" class="headerlink" title="一、论文相关信息"></a>一、论文相关信息</h1><h2 id="emsp-emsp-1-论文题目"><a href="#emsp-emsp-1-论文题目" class="headerlink" title="&emsp;&emsp;1.论文题目"></a>&emsp;&emsp;1.论文题目</h2><p>&emsp;&emsp;&emsp;&emsp;<strong>YOLO9000: Better, Faster, Stronger</strong></p>
<h2 id="emsp-emsp-2-论文时间"><a href="#emsp-emsp-2-论文时间" class="headerlink" title="&emsp;&emsp;2.论文时间"></a>&emsp;&emsp;2.论文时间</h2><p>&emsp;&emsp;&emsp;&emsp;<strong>2016年</strong></p>
<h2 id="emsp-emsp-3-论文文献"><a href="#emsp-emsp-3-论文文献" class="headerlink" title="&emsp;&emsp;3.论文文献"></a>&emsp;&emsp;3.论文文献</h2><p>&emsp;&emsp;&emsp;&emsp; <a href="https://arxiv.org/abs/1612.08242" target="_blank" rel="noopener">论文文献</a></p>
<h2 id="emsp-emsp-4-论文源码"><a href="#emsp-emsp-4-论文源码" class="headerlink" title="&emsp;&emsp;4.论文源码"></a>&emsp;&emsp;4.论文源码</h2><p>&emsp;&emsp;&emsp;&emsp;  pytroch</p>
<h1 id="二、论文背景及简介"><a href="#二、论文背景及简介" class="headerlink" title="二、论文背景及简介"></a>二、论文背景及简介</h1><p>&emsp;&emsp;&emsp;&emsp;YOLO v2是对YOLO v1版本的改进，使得YOLO Better, Faster, Stronger<br>&emsp;&emsp;&emsp;&emsp;其主要有两个大方面的改进：</p>
<p>&emsp;&emsp;&emsp;&emsp; <strong>第一，作者使用了一系列的方法对原来的YOLO多目标检测框架进行了改进，在保持原有速度的优势之下，精度上得以提升</strong>。VOC 2007数据集测试，67FPS下mAP达到76.8%，40FPS下mAP达到78.6%，基本上可以与Faster R-CNN和SSD一战。这一部分是本文主要关心的地方。</p>
<p>&emsp;&emsp;&emsp;&emsp; <strong>第二，作者提出了一种目标分类与检测的联合训练方法，通过这种方法，YOLO9000可以同时在COCO和ImageNet数据集中进行训练，训练后的模型可以实现多达9000种物体的实时检测</strong>。</p>
<h1 id="三、YOLO-v2的一系列改进"><a href="#三、YOLO-v2的一系列改进" class="headerlink" title="三、YOLO v2的一系列改进"></a>三、YOLO v2的一系列改进</h1><h2 id="1、总览"><a href="#1、总览" class="headerlink" title="1、总览"></a>1、总览</h2><p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://img-blog.csdnimg.cn/20190808151450656.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N0YXJkdXN0WXU=,size_16,color_FFFFFF,t_70"  alt="在这里插入图片描述"></p>
<h2 id="2、Better"><a href="#2、Better" class="headerlink" title="2、Better"></a>2、Better</h2><h2 id="1-Batch-Normalization"><a href="#1-Batch-Normalization" class="headerlink" title="(1) Batch Normalization"></a>(1) Batch Normalization</h2><p>&emsp;&emsp;&emsp;&emsp;CNN在训练过程中网络每层输入的分布一直在改变, 会使训练过程难度加大，但可以通过normalize每层的输入解决这个问题。新的YOLO网络在每一个卷积层后添加batch normalization，通过这一方法，mAP获得了2%的提升。batch normalization 也有助于规范化模型，可以在舍弃dropout优化后依然不会过拟合。</p>
<h2 id="2-High-Resolution-Classifier"><a href="#2-High-Resolution-Classifier" class="headerlink" title="(2) High Resolution Classifier"></a>(2) High Resolution Classifier</h2><p>&emsp;&emsp;&emsp;&emsp; 目前的目标检测方法中，基本上都会使用ImageNet预训练过的模型（classifier）来提取特征，如果用的是AlexNet网络，那么输入图片会被resize到不足256 <em> 256，导致分辨率不够高，给检测带来困难。为此，新的YOLO网络把分辨率直接提升到了448 </em> 448，这也意味之原有的网络模型必须进行某种调整以适应新的分辨率输入。</p>
<p>&emsp;&emsp;&emsp;&emsp; 对于YOLOv2，作者首先对自己在ImageNet上训练好的分类网络（自定义的darknet）进行了fine tune，分辨率改成448 * 448，在ImageNet数据集上训练10轮（10 epochs），训练后的网络就可以适应高分辨率的输入了。然后，作者对检测网络部分（也就是后半部分）也进行fine tune。这样通过提升输入的分辨率，mAP获得了4%的提升。</p>
<h2 id="3-Convolutional-With-Anchor-Boxes"><a href="#3-Convolutional-With-Anchor-Boxes" class="headerlink" title="(3) Convolutional With Anchor Boxes"></a>(3) Convolutional With Anchor Boxes</h2><p>&emsp;&emsp;&emsp;&emsp;  之前的YOLO利用全连接层的数据完成边框的预测，导致丢失较多的空间信息，定位不准。作者在这一版本中借鉴了Faster R-CNN中的anchor思想。<br>&emsp;&emsp;&emsp;&emsp; 作者去掉了后面的一个池化层以确保输出的卷积特征图有更高的分辨率。然后，通过缩减网络，让图片输入分辨率为416 <em> 416，这一步的目的是为了让后面产生的卷积特征图宽高都为奇数，这样就可以产生一个center cell。作者观察到，大物体通常占据了图像的中间位置， 就可以只用中心的一个cell来预测这些物体的位置，否则就要用中间的4个cell来进行预测，这个技巧可稍稍提升效率。最后，YOLOv2使用了卷积层降采样（factor为32），使得输入卷积网络的416 </em> 416图片最终得到13 * 13的卷积特征图（416/32=13）。加入了anchor boxes后，使得结果是召回率上升，准确率小幅度下降。具体数据为：没有anchor boxes，模型recall为81%，mAP为69.5%；加入anchor boxes，模型recall为88%，mAP为69.2%。</p>
<h2 id="4-Dimension-Clusters（维度聚类）"><a href="#4-Dimension-Clusters（维度聚类）" class="headerlink" title="(4) Dimension Clusters（维度聚类）"></a>(4) Dimension Clusters（维度聚类）</h2><p>&emsp;&emsp;&emsp;&emsp;  作者在使用anchor的时候遇到了两个问题，第一个是anchor boxes的宽高维度往往是精选的先验框（hand-picked priors），虽说在训练过程中网络也会学习调整boxes的宽高维度，最终得到准确的bounding boxes。但是，如果一开始就选择了更好的、更有代表性的先验boxes维度，那么网络就更容易学到准确的预测位置。</p>
<p>&emsp;&emsp;&emsp;&emsp;  和以前的精选boxes维度不同，作者使用了<strong>K-means聚类方法</strong>训练bounding boxes，可以自动找到更好的boxes宽高维度。传统的K-means聚类方法使用的是欧氏距离函数，也就意味着较大的boxes会比较小的boxes产生更多的error，聚类结果可能会偏离。为此，作者采用的评判标准是IOU得分（也就是boxes之间的交集除以并集），这样的话，error就和box的尺度无关了，最终的距离函数为：<br><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://img-blog.csdnimg.cn/20190808152447868.png"  alt="在这里插入图片描述"><br>&emsp;&emsp;&emsp;&emsp; 作者在VOC和COCO数据集上均做了聚类，其结果如下：<br><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://img-blog.csdnimg.cn/20190808152710710.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N0YXJkdXN0WXU=,size_16,color_FFFFFF,t_70"  alt="在这里插入图片描述"><br>&emsp;&emsp;&emsp;&emsp; 作者在平衡平均IOU和网络计算复杂度的情况下选择了k = 5，同时作者发现聚类的结果显示，在检测框中细长的框多，扁宽的少。同时，作者将其与传统选择anchor的方法进行了比较，结果如下：<br><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://img-blog.csdnimg.cn/20190808152915177.png"  alt="在这里插入图片描述"><br>&emsp;&emsp;&emsp;&emsp; 可以看到在聚类情况下，5个anchor已经可以与没有聚类的9个anchor相比了。 </p>
<h2 id="5-Direct-loaction-prediction（最后的回归修正过程）"><a href="#5-Direct-loaction-prediction（最后的回归修正过程）" class="headerlink" title="(5) Direct loaction prediction（最后的回归修正过程）"></a>(5) Direct loaction prediction（最后的回归修正过程）</h2><p>&emsp;&emsp;&emsp;&emsp; <strong>在基于region proposal的目标检测算法中，是通过预测<code>tx</code>和<code>ty</code>来得到<code>(x,y)</code>值，也就是预测的是<code>offsets</code>。</strong><br>&emsp;&emsp;&emsp;&emsp; 论文这里公式是错的，应该是“+”号。依据是下文中的例子，以及Faster R-CNN中的公式。<br><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://img-blog.csdnimg.cn/20190808163432859.jpg"  alt="在这里插入图片描述"></p>
<p>&emsp;&emsp;&emsp;&emsp; <strong>这个公式是无约束的，预测的边界框很容易向任何方向偏移。</strong><br>&emsp;&emsp;&emsp;&emsp; &emsp;&emsp; 当<code>tx=1</code>时，box将向右偏移一个anchor box的宽度；<br>&emsp;&emsp;&emsp;&emsp; &emsp;&emsp;   当<code>tx=-1</code>时，box将向左偏移一个anchor box的宽度；<br>&emsp;&emsp;&emsp;&emsp; <strong>因此，每个位置预测的边界框可以落在图片任何位置，这导致模型的不稳定性，在训练时需要很长时间来预测出正确的offsets。</strong></p>
<p>&emsp;&emsp;&emsp;&emsp;YOLOv2中没有采用这种预测方式，而是沿用了YOLOv1的方法，就是预测边界框中心点相对于对应<code>cell</code><strong>左上角位置的相对偏移值。</strong><br>&emsp;&emsp;&emsp;&emsp;网络在最后一个卷积层输出<code>13*13</code>的<code>feature map</code>，有<code>13*13</code>个cell，每个cell有5个anchor box来预测5个bounding box，每个bounding box预测得到5个值。<br>&emsp;&emsp;&emsp;&emsp;分别为：<code>tx</code>、<code>ty</code>、<code>tw</code>、<code>th</code>和<code>to</code>（类似YOLOv1的confidence）<br>&emsp;&emsp;&emsp;&emsp;为了将bounding box的中心点约束在当前cell中，使用<code>sigmoid函数</code>将<code>tx</code>、<code>ty</code>归一化处理，将值约束在<code>0~1</code>，这使得模型训练更稳定。<br><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://img-blog.csdnimg.cn/20190808163903124.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N0YXJkdXN0WXU=,size_16,color_FFFFFF,t_70"  alt="在这里插入图片描述"></p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://img-blog.csdnimg.cn/20190808155021350.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N0YXJkdXN0WXU=,size_16,color_FFFFFF,t_70"  alt="在这里插入图片描述"></p>
<h2 id="emsp-emsp-emsp-5-Fine-Grained-Features（细粒度特征）"><a href="#emsp-emsp-emsp-5-Fine-Grained-Features（细粒度特征）" class="headerlink" title="&emsp;&emsp;&emsp; (5) Fine-Grained Features（细粒度特征）"></a>&emsp;&emsp;&emsp; (5) Fine-Grained Features（细粒度特征）</h2><p>&emsp;&emsp;&emsp;&emsp; 作者在网络中添加了个<code>passthrough layer</code>，这个layer也就是把高低两种分辨率的特征图做了一次连接，连接方式是叠加特征到不同的通道而不是空间位置，类似于Resnet中的identity mappings。这个方法把26 <em> 26 </em> 512的特征图连接到了13 <em> 13 </em> 2048的特征图，这个特征图与原来的特征相连接。YOLO的检测器使用的就是经过扩张的特征图，它可以拥有更好的细粒度特征，使得模型的性能获得了1%的提升。<br><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://img-blog.csdnimg.cn/20190808155313984.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N0YXJkdXN0WXU=,size_16,color_FFFFFF,t_70"  alt="在这里插入图片描述"></p>
<h2 id="6-Multi-Scale-Training"><a href="#6-Multi-Scale-Training" class="headerlink" title="(6) Multi-Scale Training"></a>(6) Multi-Scale Training</h2><p>&emsp;&emsp;&emsp;&emsp; YOLO v2的网络结构中由于没有了全连接层，全部由卷积和池化构成，因此输入的图片大小可以是任意的。作者为了是网络适应不同大小的输入，进行了Multi-Scale Training。作者是将输入图片，每10batches，就更改一下输入图片的尺寸（32的倍数{320,352,…..,608}）<br>&emsp;&emsp;&emsp;&emsp; 这种机制使得网络可以更好地预测不同尺寸的图片，意味着同一个网络可以进行不同分辨率的检测任务，在小尺寸图片上YOLOv2运行更快，在速度和精度上达到了平衡。</p>
<p>&emsp;&emsp;&emsp;&emsp; 在小尺寸图片检测中，YOLOv2成绩很好，输入为228 * 228的时候，帧率达到90FPS，mAP几乎和Faster R-CNN的水准相同。使得其在低性能GPU、高帧率视频、多路视频场景中更加适用。</p>
<p>&emsp;&emsp;&emsp;&emsp; 在大尺寸图片检测中，YOLOv2达到了先进水平，VOC2007 上mAP为78.6%，仍然高于平均水准，下图是YOLOv2和其他网络的成绩对比：<br><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://img-blog.csdnimg.cn/20190808155747356.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N0YXJkdXN0WXU=,size_16,color_FFFFFF,t_70"  alt="在这里插入图片描述"></p>
<h2 id="3、Faster"><a href="#3、Faster" class="headerlink" title="3、Faster"></a>3、Faster</h2><blockquote>
<p>作者为了改善检测速度，也作了一些相关工作。</p>
<h2 id="1-Darknet-19"><a href="#1-Darknet-19" class="headerlink" title="(1) Darknet-19"></a>(1) Darknet-19</h2><p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://img-blog.csdnimg.cn/20190808160239576.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N0YXJkdXN0WXU=,size_16,color_FFFFFF,t_70"  alt="在这里插入图片描述"></p>
<h2 id="2-Training-for-classification"><a href="#2-Training-for-classification" class="headerlink" title="(2) Training for classification"></a>(2) Training for classification</h2><p>&emsp;&emsp;&emsp;&emsp;  作者使用Darknet-19在标准1000类的ImageNet上训练了160次，用的随机梯度下降法，starting learning rate 为0.1，polynomial rate decay 为4，weight decay为0.0005 ，momentum 为0.9。训练的时候仍然使用了很多常见的数据扩充方法（data augmentation），包括random crops, rotations, and hue, saturation, and exposure shifts。 （这些训练参数是基于darknet框架，和caffe不尽相同）</p>
</blockquote>
<p>&emsp;&emsp;&emsp;&emsp;  初始的224 <em> 224训练后，作者把分辨率上调到了448 </em> 448，然后又训练了10次，学习率调整到了0.001。高分辨率下训练的分类网络在top-1准确率76.5%，top-5准确率93.3%。</p>
<h2 id="3-Training-for-detection"><a href="#3-Training-for-detection" class="headerlink" title="(3) Training for detection"></a>(3) Training for detection</h2><p>&emsp;&emsp;&emsp;&emsp;  分类网络训练完后，就该训练检测网络了，作者去掉了原网络最后一个卷积层，转而增加了三个3 <em> 3 </em> 1024的卷积层，并且在每一个上述卷积层后面跟一个1 <em> 1的卷积层，输出维度是检测所需的数量。对于VOC数据集，预测5种boxes大小，每个box包含5个坐标值和20个类别，所以总共是5 </em> （5+20）= 125个输出维度。同时也添加了转移层（passthrough layer ），从最后那个3 <em> 3 </em> 512的卷积层连到倒数第二层，使模型有了细粒度特征。</p>
<p>作者的检测模型以0.001的初始学习率训练了160次，在60次和90次的时候，学习&emsp;&emsp;&emsp;&emsp;  率减为原来的十分之一。其他的方面，weight decay为0.0005，momentum为0.9，依然使用了类似于Faster-RCNN和SSD的数据扩充（data augmentation）策略。</p>
<h2 id="4、Stronger"><a href="#4、Stronger" class="headerlink" title="4、Stronger"></a>4、Stronger</h2><blockquote>
<p>YOLO 9000的由来，可以同时检测9000中类别。（ps：YOLO v1是20分类）<br>在这里我们简介一下思想，具体内容请查看论文（我理解的也不是很好）</p>
</blockquote>
<p>&emsp;&emsp;&emsp;&emsp; 人为对图像的目标进行标注的代价是巨大的，因此没有那么大的数据集来支撑那么多类别的检测，那么作者是如何解决这个问题的呢？<br>&emsp;&emsp;&emsp;&emsp; 原来，作者将目光瞄向了Image Net。作者通过一种神奇的训练方法将分类与目标检测联合起来训练，这使得该网络具备了更多分类的能力，当然，在这些数据上，精度稍微小一点。那么作者是如何做的呢？<br>&emsp;&emsp;&emsp;&emsp; 首先介绍一下，作者是怎样进行训练的。在网络处理过程中，如果输入的是图片分类的图片，那么网络只反向传播分类的那一块loss，如果输入的是目标检测的图片，那么网络将进行正常的反向传播，来对目标检测和一块进行优化。通过这种方法，作者就使得网络具备了更多分类的能力。因此在分类输出那一块是经过softmax直接输出1000多种的分类结果<br>&emsp;&emsp;&emsp;&emsp; 但这样处理的时候，又出现了新的问题。假设ImageNet收录了哈士奇和藏獒的图片，但没有牧羊犬的图片，那么当放入牧羊犬的图片是，网络该如何做呢？<br>&emsp;&emsp;&emsp;&emsp; 我们知道哈士奇和藏獒都属于狗，那能不能在softmax输出的时候，输出多个标签，比如输出哈士奇和狗，这样在遇到牧羊犬的时候，只输出狗的标签就可以了。因此作者对ImageNet数据集做出了修改。<br>&emsp;&emsp;&emsp;&emsp;ImageNet数据集的标签是来自于WordNet的，一个语言的有向图，这里面包含了各种分类之间的关系。为了简化，作者将ImageNet的标签构造成了一颗树，这棵树从上到下就是隶属关系。之后作者也将COCO的数据集加入到了这颗树里面。<br><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://img-blog.csdnimg.cn/20190808162007736.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N0YXJkdXN0WXU=,size_16,color_FFFFFF,t_70"  alt="在这里插入图片描述"><br>&emsp;&emsp;&emsp;&emsp; 假如ImageNet 有1000个分类，经过构造树后，得到了369个根节点，那么softmax就输出1369个分类结果，在给出某个类别的预测概率时，需要找到其所在的位置，遍历这个path，然后计算path上各个节点的概率之积。根据最大的置信度来对该图片进行分类，这也就解决了上述问题。<br>&emsp;&emsp;&emsp;&emsp;  最后作者联合了多个数据集，将种类数提高到了9418。也就是说YOLO9000可以同时对9000多种目标进行检测。这就是论文题目的由来</p>
<h1 id="四、实验结果"><a href="#四、实验结果" class="headerlink" title="四、实验结果"></a>四、实验结果</h1><p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://img-blog.csdnimg.cn/20190808162635121.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1N0YXJkdXN0WXU=,size_16,color_FFFFFF,t_70"  alt="在这里插入图片描述"></p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">白丁</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://baidinghub.github.io/2020/04/03/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B(%E5%85%AD)%E4%B9%8BYOLO%20v2/">http://baidinghub.github.io/2020/04/03/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B(%E5%85%AD)%E4%B9%8BYOLO%20v2/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://baidinghub.github.io" target="_blank">BaiDing's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/">深度基础知识</a><a class="post-meta__tags" href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">目标检测</a></div><div class="post_share"><div class="addthis_inline_share_toolbox"></div><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDyIdOn/M/aXcGQOiSDVMqvN2gxDdiESHUfFuB2YMy48fvNN9SZOQUbVlGF4Pk6nDXIAir+br/EWuEnNLtgOCYMo/BTxl29gqS/QGHPiDaIQedzmLcuRZpfDuGit61N/b9pyktpZLagBgbl5Ox9mAgWQxXyhxYB092gyOXqrBULBeZUYQ+H7Eupha10QTQghHv4nLk+oYWo2UXEiijQpE3qMXT32G8v8k0KbRdd1hIFPyNEx6eZ6Buc2ZdbMtoutdGjvdnw5B1+dxgl1egESPChzcwCMxt3NW/3faQ5lsjRhQD4fRU+Ua/aqXe+0xg2+xr0BTjNr/JZIAaNiLuo71lH 1564026260@qq.com" async="async"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/wechat.png?raw=true" alt="微信"/><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/alipay.jpg?raw=true" alt="支付寶"/><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/04/03/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%EF%BC%88%E4%B8%83%EF%BC%89%E4%B9%8BYOLO%20v3/"><img class="prev_cover lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B(%E4%B8%83)%E4%B9%8BYOLO%20v3/cover.png?raw=true" onerror="onerror=null;src='https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/404.jpg?raw=true'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">目标检测（七）之YOLO v3</div></div></a></div><div class="next-post pull_right"><a href="/2020/04/03/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B(%E4%BA%94)%E4%B9%8BYOLOv1/"><img class="next_cover lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B(%E4%BA%94)%E4%B9%8BYOLOv1/cover.png?raw=true" onerror="onerror=null;src='https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/404.jpg?raw=true'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">目标检测（五）之YOLOv1</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/04/03/目标检测(一)之 R-CNN/" title="目标检测（一）之 R-CNN"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B(%E4%B8%80)%E4%B9%8B%20R-CNN/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">目标检测（一）之 R-CNN</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/目标检测(三)之Faster R-CNN/" title="目标检测（三）之Faster R-CNN"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B(%E4%B8%89)%E4%B9%8BFaster%20R-CNN/cover.jpg?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">目标检测（三）之Faster R-CNN</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/目标检测(二)之Fast R-CNN/" title="目标检测（二）之Fast R-CNN"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B(%E4%BA%8C)%E4%B9%8BFast%20R-CNN/cover.jpg?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">目标检测（二）之Fast R-CNN</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/目标检测(五)之YOLOv1/" title="目标检测（五）之YOLOv1"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B(%E4%BA%94)%E4%B9%8BYOLOv1/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">目标检测（五）之YOLOv1</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/目标检测(四)之Mask-RCNN/" title="目标检测（四）之Mask-RCNN"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B(%E5%9B%9B)%E4%B9%8BMask-RCNN/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">目标检测（四）之Mask-RCNN</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/目标检测之目录/" title="目标检测之目录"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B9%8B%E7%9B%AE%E5%BD%95/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">目标检测之目录</div></div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: 'd3948be001a91411dfd9',
  clientSecret: 'ebddf2a2a5a039922fb373a8a8c0efcc439bf6ca',
  repo: 'BaiDingHub.github.io',
  owner: 'BaiDingHub',
  admin: ['BaiDingHub'],
  id: md5(decodeURI(location.pathname)),
  language: 'zh-CN',
  perPage: 10,
  distractionFreeMode: false,
  pagerDirection: 'last',
  createIssueManually: false,
  updateCountCallback: commentCount
})
gitalk.render('gitalk-container')

function commentCount(n){
  try {
    document.getElementsByClassName('gitalk-comment-count')[0].innerHTML= n
  } catch (e) {
    return false
  }
}</script></div></article></main><footer id="footer" style="background-image: url(https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B(%E5%85%AD)%E4%B9%8BYOLO%20v2/cover.png?raw=true)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By 白丁</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://jerryc.me/" target="_blank" rel="noopener">blog</a>!</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js" ></script><script src="/js/utils.js" ></script><script src="/js/main.js" ></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><script src="/js/search/algolia.js"></script></body></html>