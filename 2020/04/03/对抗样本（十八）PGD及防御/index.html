<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>对抗样本（十八）PGD及防御 | BaiDing's blog</title><meta name="description" content="对抗样本（十八）PGD及防御"><meta name="keywords" content="深度学习,对抗攻击,对抗防御"><meta name="author" content="白丁"><meta name="copyright" content="白丁"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="https://raw.githubusercontent.com/BaiDingHub/Blog_images/master/BlogSource/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="crossorigin"/><link rel="preconnect" href="http://ta.qq.com"/><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="x-1ef6P_miWkq-RJn_fmjd3KYumrXANNXYzK1myaLf0"/><meta name="baidu-site-verification" content="guD6l44Chk"/><meta name="twitter:card" content="summary"><meta name="twitter:title" content="对抗样本（十八）PGD及防御"><meta name="twitter:description" content="对抗样本（十八）PGD及防御"><meta name="twitter:image" content="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89PGD%E5%8F%8A%E9%98%B2%E5%BE%A1/cover.png?raw=true"><meta property="og:type" content="article"><meta property="og:title" content="对抗样本（十八）PGD及防御"><meta property="og:url" content="http://baidinghub.github.io/2020/04/03/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89PGD%E5%8F%8A%E9%98%B2%E5%BE%A1/"><meta property="og:site_name" content="BaiDing's blog"><meta property="og:description" content="对抗样本（十八）PGD及防御"><meta property="og:image" content="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89PGD%E5%8F%8A%E9%98%B2%E5%BE%A1/cover.png?raw=true"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>var autoChangeMode = '1'
var t = Cookies.get("theme")
if (autoChangeMode == '1'){
  var isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
  var isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
  var isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined){
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport){
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour < 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
  }
  } else if (t == 'light') activateLightMode()
  else activateDarkMode()

} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css" ><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://baidinghub.github.io/2020/04/03/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89PGD%E5%8F%8A%E9%98%B2%E5%BE%A1/"><link rel="prev" title="对抗样本（十九）MI-FGSM" href="http://baidinghub.github.io/2020/04/03/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%EF%BC%88%E5%8D%81%E4%B9%9D%EF%BC%89MI-FGSM/"><link rel="next" title="对抗样本（十七）对抗领域方法简述（一）" href="http://baidinghub.github.io/2020/04/03/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89%E5%AF%B9%E6%8A%97%E9%A2%86%E5%9F%9F%E6%96%B9%E6%B3%95%E7%AE%80%E8%BF%B0%EF%BC%88%E4%B8%80%EF%BC%89/"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.js" defer></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-162698439-1', 'auto');
ga('send', 'pageview');
</script><script src="https://tajs.qq.com/stats?sId=66540586" charset="UTF-8"></script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"4JGH9NW4XG","apiKey":"41cdb6d9ec4d21196956524e9c985b36","indexName":"baiding","hits":{"per_page":6},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  highlightShrink: 'false',
  isFontAwesomeV5: false,
  isPhotoFigcaption: false
  
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isSidebar: true  
  }</script><noscript><style>
#page-header {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/avatar.png?raw=true" onerror="onerror=null;src='https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/friend_404.gif?raw=true'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">97</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">60</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">17</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于作者</span></a></div></div></div></div><i class="fa fa-arrow-right on" id="toggle-sidebar" aria-hidden="true">     </i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#一、Towards-deep-learning-models-resistant-to-adversarial-attacks"><span class="toc-text">一、Towards deep learning models resistant to adversarial attacks.</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Paper-Information"><span class="toc-text">1. Paper Information</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Motivation"><span class="toc-text">2. Motivation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Main-Arguments"><span class="toc-text">3. Main Arguments</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Framework"><span class="toc-text">4. Framework</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-An-Optimization-View-on-Adverarial-Robustness"><span class="toc-text">4.1 An Optimization View on Adverarial Robustness</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Towards-Universally-Robust-Networks"><span class="toc-text">4.2 Towards Universally Robust Networks</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-1-The-Landscape-of-Adversarial-Examples"><span class="toc-text">4.2.1 The Landscape of Adversarial Examples</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-2-First-Order-Adversaries"><span class="toc-text">4.2.2 First-Order Adversaries</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-3-Descent-Directions-for-Adversarial-Training"><span class="toc-text">4.2.3 Descent Directions for Adversarial Training</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3"><span class="toc-text">4.3</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Result"><span class="toc-text">5.Result</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-Network-Capacity-and-Adversarial-Robustness"><span class="toc-text">5.1 Network Capacity and Adversarial Robustness</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-Adversarially-Robust-Deep-Learning-Models"><span class="toc-text">5.2 Adversarially Robust Deep Learning Models</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-1-MNIST"><span class="toc-text">5.2.1 MNIST</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-2-CIFAR10"><span class="toc-text">5.2.2 CIFAR10</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-3-不同-epsilon-值的影响"><span class="toc-text">5.2.3 不同$ \epsilon$ 值的影响</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Argument"><span class="toc-text">6. Argument</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-Further-research"><span class="toc-text">7. Further research</span></a></li></ol></li></ol></div></div></div><div id="body-wrap"><div class="post-bg" id="nav" style="background-image: url(https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89PGD%E5%8F%8A%E9%98%B2%E5%BE%A1/cover.png?raw=true)"><div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">BaiDing's blog</a></span><span class="pull_right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于作者</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span></span></div><div id="post-info"><div id="post-title"><div class="posttitle">对抗样本（十八）PGD及防御</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2020-04-03 13:18:05"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2020-04-03</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-12-03 12:26:51"><i class="fa fa-history" aria-hidden="true"></i> 更新于 2020-12-03</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><i class="fa fa-angle-right post-meta__separator" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/">对抗样本</a></span></div><div class="meta-secondline"> <span class="post-meta-wordcount"><i class="post-meta__icon fa fa-file-word-o" aria-hidden="true"></i><span>字数总计:</span><span class="word-count">3.2k</span><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-clock-o" aria-hidden="true"></i><span>阅读时长: 10 分钟</span></span></div><div class="meta-thirdline"><span class="post-meta-pv-cv"><span class="post-meta__separator">|</span><i class="fa fa-eye post-meta__icon" aria-hidden="true"> </i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-comment-o" aria-hidden="true"></i><span>评论数:</span><a href="/2020/04/03/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89PGD%E5%8F%8A%E9%98%B2%E5%BE%A1/#post-comment"><span class="gitalk-comment-count comment-count"></span></a></span></div></div></div></div><main class="layout_post" id="content-inner"><article id="post"><div id="article-container"><meta name="referrer" content="no-referrer"/>

<h1 id="一、Towards-deep-learning-models-resistant-to-adversarial-attacks"><a href="#一、Towards-deep-learning-models-resistant-to-adversarial-attacks" class="headerlink" title="一、Towards deep learning models resistant to adversarial attacks."></a>一、Towards deep learning models resistant to adversarial attacks.</h1><h2 id="1-Paper-Information"><a href="#1-Paper-Information" class="headerlink" title="1. Paper Information"></a>1. Paper Information</h2><blockquote>
<p>时间：2017年</p>
<p>关键词：Adversarial Attack，CV，PGD</p>
<p>论文位置：<a href="https://arxiv.org/pdf/1706.06083" target="_blank" rel="noopener">https://arxiv.org/pdf/1706.06083</a>)</p>
<p>引用：Madry A, Makelov A, Schmidt L, et al. Towards deep learning models resistant to adversarial attacks[J]. arXiv preprint arXiv:1706.06083, 2017.</p>
</blockquote>
<h2 id="2-Motivation"><a href="#2-Motivation" class="headerlink" title="2. Motivation"></a>2. Motivation</h2><p> &emsp;&emsp; 研究表明，对抗攻击的存在可能是深度学习模型的一个固有弱点。为了解决这个问题，我们从鲁棒性优化的角度研究了神经网络的对抗鲁棒性。这些方法为我们研究这些问题提供了基准。同时，这些方法可以让模型提高对对抗攻击的抵抗力。作者<strong>定义了first-order adversary的概念</strong>作为一种自然和广泛的安全保障。使模型更加鲁棒是走向更好的深度学习模型的重要一步。</p>
<p> &emsp;&emsp; 在对抗样本领域，我们永远不能确定一个攻击方法找到了最具对抗性的样本，也不能说一个防御方法能够抵御所有的对抗样本，这使得我们很难评估模型的鲁棒性。所以，我们怎么样才能够训练一个更加鲁棒的模型呢?</p>
<h2 id="3-Main-Arguments"><a href="#3-Main-Arguments" class="headerlink" title="3. Main Arguments"></a>3. Main Arguments</h2><p> &emsp;&emsp; 在这篇论文中，我们<strong>从鲁棒性优化的角度研究了神经网络的对抗鲁棒性</strong>，我们<strong>使用了一种natural saddle point (min-max) 公式来定义对对抗攻击的安全性</strong>。这个公式将攻击和防御放到了同一个理论框架中，使得我们对攻击和防御的性能有了良好的量化。</p>
<ul>
<li><strong>我们对此鞍点公式相对应的优化场景进行了仔细的实验研究</strong>，<strong>提出了PGD</strong>这个<strong>一阶方法</strong>（利用局部一阶信息）来解决这个问题。</li>
<li>我们<strong>探讨了网络架构对对抗鲁棒性的影响，发现模型容量在其中扮演了重要的角色</strong>。网络通常需要比正常分类情况更大的模型容量，才能够抵御强大的对抗攻击。这表明鞍点问题的鲁棒决策边界比简单地分离正常数据点的决策边界要复杂得多。</li>
<li><strong>基于上述鞍点的优化，并使用PGD作为攻击手段，我们在MNIST和CIFAR10上训练出了鲁棒的分类模型</strong>。我们使用了大量的攻击手段进行测试，MNIST模型可以在攻击下保持89%的分类准确率，甚至可以抵御迭代的白盒攻击；CIFAR模型可以保持46%的分类准确率。另外，在黑盒攻击下，MNIST和CIFAR10模型分别可以实现95%和64%的准确率。</li>
</ul>
<h2 id="4-Framework"><a href="#4-Framework" class="headerlink" title="4. Framework"></a>4. Framework</h2><h3 id="4-1-An-Optimization-View-on-Adverarial-Robustness"><a href="#4-1-An-Optimization-View-on-Adverarial-Robustness" class="headerlink" title="4.1 An Optimization View on Adverarial Robustness"></a>4.1 An Optimization View on Adverarial Robustness</h3><p> &emsp;&emsp; 我们的大部分讨论将围绕着<strong>对抗健壮性的优化视角</strong>。这一视角不仅准确地捕捉了我们想要研究的现象，而且还将为我们的调查提供信息。</p>
<p> &emsp;&emsp; 在训练模型时，我们通常是<strong>通过经验损失最小化（ERM）来训练</strong>的，即最小化$ \mathbb{E}_{(x,y)\sim \mathcal{D}}[L(x,y,\theta)]$ ，不幸的是，ERM通常并不能够得到鲁棒的模型。那么，为了能够训练出鲁棒的模型，我们需要去<strong>增强ERM过程</strong>。首先，我们提出了一个基准，即一个鲁棒的模型应该满足的条件，即guarantee。之后，我们调整训练方法来满足这个guarantee。</p>
<p> &emsp;&emsp; 第一步，<strong>我们要先选择一个要攻击的模型</strong>。对于每一个数据点，我们引入了扰动边界$ \mathcal{S} \in \mathbb{R}^d$ ，即对抗扰动允许的扰动范围。比如，有些方法中扰动边界选择$ x$ 的$ l_{\infty}$ 球作为扰动边界。</p>
<p> &emsp;&emsp; 之后，<strong>我们将上面的扰动因素考虑到训练过程中去</strong>。我们并<strong>不是直接将分布为$ \mathcal{D}$ 的样本直接送入损失函数中去，而是先允许样本有轻微的扰动</strong>，这就得到了下面的<strong>鞍点问题</strong>，也是我们研究的主要问题，即：</p>
<script type="math/tex; mode=display">
min_{\theta}\rho(\theta),\ \text{where}\ \rho(\theta) = \mathbb{E}_{(x,y)\sim \mathcal{D}}[max_{\delta \in \mathcal{S}}L(\theta,x+\delta,y)]</script><p> &emsp;&emsp; 首先，这个公式为我们提供了一个统一的视角，它包含了以前关于对抗鲁棒性的许多工作。<strong>上面的公式包含一个最大化和一个最小化过程。最大化的意思是找到一个给定数据点$ x$ 的能使损失函数最大化的对抗版本，这正是攻击问题。最小化的目标是优化模型参数，使得内部的对抗损失最小化，而这正式一个防御问题。</strong></p>
<p> &emsp;&emsp; 第二，<strong>这个鞍点公式明确了理想鲁棒分类器应该达到的目标，以及鲁棒分类器鲁棒性的定量度量</strong>。</p>
<h3 id="4-2-Towards-Universally-Robust-Networks"><a href="#4-2-Towards-Universally-Robust-Networks" class="headerlink" title="4.2 Towards Universally Robust Networks"></a>4.2 Towards Universally Robust Networks</h3><p> &emsp;&emsp; 有了上面的鞍点公式，我们可以发现，在数据点周围的小的对抗扰动都会无效，因此，我<strong>们把我们的精力集中于如何解决这个问题</strong>。</p>
<p> &emsp;&emsp; 不幸的是，这个鞍点问题并不是那么容易解决。该问题涉及到出处理一个非凸的外部最小化问题和一个非凹的内部最大化问题。</p>
<h4 id="4-2-1-The-Landscape-of-Adversarial-Examples"><a href="#4-2-1-The-Landscape-of-Adversarial-Examples" class="headerlink" title="4.2.1 The Landscape of Adversarial Examples"></a>4.2.1 The Landscape of Adversarial Examples</h4><p> &emsp;&emsp; 内部的最大化问题，对应于，给定模型和数据点，找到一个对抗样本的问题。因为，这个问题要求我们最大化一个非凹的函数，人们会认为它很难处理。事实上，<strong>我们可以线性化这个问题，然后进行解决，比如FGSM方法</strong>。不过，FGSM这种单步方法有一些缺点，无法应对复杂的对抗攻击。</p>
<p> &emsp;&emsp; 为了更好地理解这个问题，我们研究了MNIST和CIFAR10上多个模型的局部最大值。我们主要用到的工具是PGD方法，因为，它是大规模约束优化的标准方法。我们<strong>在数据点的$ l_{\infty}$ 球边界的许多点运行PGD</strong>，来探索损失函数中的大部分位置（即在这个球的范围内，探索到绝大多数地方，以能够找到最强的对抗样本）。</p>
<p> &emsp;&emsp; 通过这种方法，我们发现我们可以解决这个问题了。虽然在$ x_i + \mathcal{S}$ 的范围内有许多局部最大值，但是他们的损失值往往相似，也就是说，解决了这个问题，我们就可以去训练神经网络了。</p>
<p> &emsp;&emsp; 另外，在实验中，我们发现了一下的现象：</p>
<ul>
<li>我们发现，当我们在$ x_i + \mathcal{S}$ 内随意选取起始点进行PGD时，<strong>发现损失值以相当一致的方式增加，并且逐渐收敛</strong></li>
</ul>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89PGD%E5%8F%8A%E9%98%B2%E5%BE%A1/1.png?raw=true"  alt="1"></p>
<ul>
<li>对上述问题进一步分析，我们可以发现，在大量的随机启动后，<strong>最终迭代的损失遵循一个良好的集中分布，没有极端异常值</strong></li>
</ul>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89PGD%E5%8F%8A%E9%98%B2%E5%BE%A1/2.png?raw=true"  alt="2" style="zoom:50%;" /></p>
<p> &emsp;&emsp; 上述的观点也说明了PGD是一个通用的攻击方法。</p>
<h4 id="4-2-2-First-Order-Adversaries"><a href="#4-2-2-First-Order-Adversaries" class="headerlink" title="4.2.2 First-Order Adversaries"></a>4.2.2 First-Order Adversaries</h4><p> &emsp;&emsp; 实验发现，通过PGD发现的局部最大值在正常训练的网络和对抗训练的网络中都有着相似的损失值。这也说明，<strong>只要能够防御住PGD，就会对所有的一阶攻击手段具有鲁棒性</strong>。这在第五节做了实验。</p>
<p> &emsp;&emsp; 这种鲁棒性会在黑盒攻击下变得更强。我们在附录中讨论了迁移性。我们发现，提<strong>高模型容量和攻击的健壮性，可以提高我们模型对迁移攻击的抵抗能力。</strong></p>
<h4 id="4-2-3-Descent-Directions-for-Adversarial-Training"><a href="#4-2-3-Descent-Directions-for-Adversarial-Training" class="headerlink" title="4.2.3 Descent Directions for Adversarial Training"></a>4.2.3 Descent Directions for Adversarial Training</h4><p> &emsp;&emsp; 我们利用PGD很好的解决了内部优化问题，那么下一步就是要解决外部优化问题，即寻找使“对抗损失”最小化的模型参数，即内部最大化问题的值。</p>
<p> &emsp;&emsp; 而到了这一步就是正常的网络的优化问题，我们可以使用SGD来进行解决。</p>
<h3 id="4-3"><a href="#4-3" class="headerlink" title="4.3"></a>4.3</h3><p> &emsp;&emsp; <strong>只是能解决这个问题还是不够的，我们还需要做出实验证明我们可以使得loss足够的小。</strong>对于一组固定的输入扰动$ S$ 来说，其loss值由分类器架构决定。而，<strong>要使得分类器更加鲁棒，就需要增加分类器的容量，这是因为，我们的方法使得决策边界变得复杂</strong>，因此需要更大容量的分类器，如下：</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89PGD%E5%8F%8A%E9%98%B2%E5%BE%A1/3.png?raw=true"  alt="3" style="zoom:50%;" /></p>
<h2 id="5-Result"><a href="#5-Result" class="headerlink" title="5.Result"></a>5.Result</h2><h3 id="5-1-Network-Capacity-and-Adversarial-Robustness"><a href="#5-1-Network-Capacity-and-Adversarial-Robustness" class="headerlink" title="5.1 Network Capacity and Adversarial Robustness"></a>5.1 Network Capacity and Adversarial Robustness</h3><p> &emsp;&emsp; 为了验证这个观点，作者进行了实验。</p>
<p> &emsp;&emsp; 在MNIST数据集上，采用了一个简单的卷积网络，一个2个卷积核的CNN+4个卷积核的CNN+64个单元的FC，每个CNN后跟2x2的max-pooling。作者通过城北的增加CNN的卷积核和FC的单元数来提高模型容量。对抗样本所采用的扰动值$ \epsilon = 0.3$ 。</p>
<p> &emsp;&emsp; 在CIFAR10数据集上，使用了ResNet网络，作者使用了裁剪和反转的数据增强方法。为了增强模型容量，在原先模型的基础上增加了10倍的层数，包含了5个残差单元，分别是16、160、320、640卷积核。</p>
<p> &emsp;&emsp; 实验结果如下，第一张图表示在原始数据上训练，在原始数据、FGSM对抗样本、PGD对抗样本下的分类准确率。第二张图表示借用FGSM对抗样本进行训练（作者所说的方法）下的准确率，第三张图是借用PGD对抗样本进行训练下的准确率。第四张图表示在增强模型容量时loss的下降程度。</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89PGD%E5%8F%8A%E9%98%B2%E5%BE%A1/4.png?raw=true"  alt="4" style="zoom:50%;" /></p>
<ul>
<li><strong>模型容量影响</strong>：在仅使用正常样本进行训练时，可以发现增强对抗样本可以增加对单步扰动（FGSM）的鲁棒性，而且$ \epsilon$ 越小，影响越大。</li>
<li><strong>FGSM对抗样本训练的研究</strong>：可以发现，在模型容量较小时，模型对FGSM对抗样本产生了过拟合，这种行为被称为label leaking。而且，模型无法防御PGD攻击</li>
<li><strong>PGD对抗样本训练的研究</strong>：可以发现，在模型容量较小时，模型无法拟合，而模型较大时就可以有防御效果了。</li>
<li><strong>鞍点问题的研究</strong>：可以看到，随着模型容量的增大，loss值也在变小，也就是说增大模型容量可以使模型更好地适应对抗攻击。</li>
<li><strong>迁移性研究</strong>：增大模型容量、使用更强的攻击方法，会使模型越不容易受到对抗样本迁移性的攻击。不过，当模型容量增大到一定程度时，关联性就没有那么大了。</li>
</ul>
<h3 id="5-2-Adversarially-Robust-Deep-Learning-Models"><a href="#5-2-Adversarially-Robust-Deep-Learning-Models" class="headerlink" title="5.2 Adversarially Robust Deep Learning Models"></a>5.2 Adversarially Robust Deep Learning Models</h3><p> &emsp;&emsp; 在有了上面想法后，我们就要训练出一个足够鲁棒的分类器，主要从两方面下手，即<strong>训练一个高容量的网络、使用更强的攻击方法作为辅助</strong>。</p>
<p> &emsp;&emsp; 对于MNIST和CIFAR10数据集来说，PGD作为攻击手段就已经足够。在使用PGD时，每个epoch，每轮样本只扰动一次，等下次epoch再扰动。</p>
<h4 id="5-2-1-MNIST"><a href="#5-2-1-MNIST" class="headerlink" title="5.2.1 MNIST"></a>5.2.1 MNIST</h4><p> &emsp;&emsp; 在MNIST数据集上，作者使用40迭代次数的PGD作为攻击方法，step size设为0.01，使用的是梯度的符号。模型由两个CNN和一个FC组成，CNN的卷积核数分别是32和64，每一个CNN后跟2x2的max pooling，之后接1024的FC。该模型在不同的攻击下的效果如下，其中A表示白盒攻击，A‘表示黑盒攻击（从一个完全相同的网络中迁移，不过不是同一个网络），$ A_{nat}$ 表示黑盒攻击（从同一个网络中迁移，仅使用原始样本训练），B表示黑盒统计（从不同架构的网络中迁移），：</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89PGD%E5%8F%8A%E9%98%B2%E5%BE%A1/5.png?raw=true"  alt="5" style="zoom:50%;" /></p>
<h4 id="5-2-2-CIFAR10"><a href="#5-2-2-CIFAR10" class="headerlink" title="5.2.2 CIFAR10"></a>5.2.2 CIFAR10</h4><p> &emsp;&emsp; 对于CIFAR10，考虑了两个模型结构，原始ResNet和10倍大的变体。step为7，size为2。</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89PGD%E5%8F%8A%E9%98%B2%E5%BE%A1/6.png?raw=true"  alt="6" style="zoom:50%;" /></p>
<h4 id="5-2-3-不同-epsilon-值的影响"><a href="#5-2-3-不同-epsilon-值的影响" class="headerlink" title="5.2.3 不同$ \epsilon$ 值的影响"></a>5.2.3 不同$ \epsilon$ 值的影响</h4><p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89PGD%E5%8F%8A%E9%98%B2%E5%BE%A1/7.png?raw=true"  alt="7" style="zoom:50%;" /></p>
<h2 id="6-Argument"><a href="#6-Argument" class="headerlink" title="6. Argument"></a>6. Argument</h2><p> &emsp;&emsp; 该论文提出了PGD攻击方法，使用迭代多步的方法寻找对抗样本，比FGSM这种单步方法要强，同时设计了新的损失函数，用来增加模型的鲁棒性，取得了不错的成果。</p>
<h2 id="7-Further-research"><a href="#7-Further-research" class="headerlink" title="7. Further research"></a>7. Further research</h2><p> &emsp;&emsp; PGD是类似SGD那样的迭代方法，我们可以对其进行扩展，使用Momentum等方法来进行迭代，衍生了一下论文，Boosting adversarial attacks with momentum。</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">白丁</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://baidinghub.github.io/2020/04/03/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89PGD%E5%8F%8A%E9%98%B2%E5%BE%A1/">http://baidinghub.github.io/2020/04/03/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89PGD%E5%8F%8A%E9%98%B2%E5%BE%A1/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://baidinghub.github.io" target="_blank">BaiDing's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB/">对抗攻击</a><a class="post-meta__tags" href="/tags/%E5%AF%B9%E6%8A%97%E9%98%B2%E5%BE%A1/">对抗防御</a></div><div class="post_share"><div class="addthis_inline_share_toolbox"></div><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDyIdOn/M/aXcGQOiSDVMqvN2gxDdiESHUfFuB2YMy48fvNN9SZOQUbVlGF4Pk6nDXIAir+br/EWuEnNLtgOCYMo/BTxl29gqS/QGHPiDaIQedzmLcuRZpfDuGit61N/b9pyktpZLagBgbl5Ox9mAgWQxXyhxYB092gyOXqrBULBeZUYQ+H7Eupha10QTQghHv4nLk+oYWo2UXEiijQpE3qMXT32G8v8k0KbRdd1hIFPyNEx6eZ6Buc2ZdbMtoutdGjvdnw5B1+dxgl1egESPChzcwCMxt3NW/3faQ5lsjRhQD4fRU+Ua/aqXe+0xg2+xr0BTjNr/JZIAaNiLuo71lH 1564026260@qq.com" async="async"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/wechat.png?raw=true" alt="微信"/><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/alipay.jpg?raw=true" alt="支付寶"/><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/04/03/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%EF%BC%88%E5%8D%81%E4%B9%9D%EF%BC%89MI-FGSM/"><img class="prev_cover lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%EF%BC%88%E5%8D%81%E4%B9%9D%EF%BC%89MI-FGSM/cover.png?raw=true" onerror="onerror=null;src='https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/404.jpg?raw=true'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">对抗样本（十九）MI-FGSM</div></div></a></div><div class="next-post pull_right"><a href="/2020/04/03/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89%E5%AF%B9%E6%8A%97%E9%A2%86%E5%9F%9F%E6%96%B9%E6%B3%95%E7%AE%80%E8%BF%B0%EF%BC%88%E4%B8%80%EF%BC%89/"><img class="next_cover lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89%E5%AF%B9%E6%8A%97%E9%A2%86%E5%9F%9F%E6%96%B9%E6%B3%95%E7%AE%80%E8%BF%B0%EF%BC%88%E4%B8%80%EF%BC%89/cover.png?raw=true" onerror="onerror=null;src='https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/404.jpg?raw=true'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">对抗样本（十七）对抗领域方法简述（一）</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/04/03/对抗攻击之目录/" title="对抗样本之目录"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B9%8B%E7%9B%AE%E5%BD%95/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">对抗样本之目录</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/对抗样本（三）FGSM/" title="对抗样本（三）FGSM"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%EF%BC%88%E4%B8%89%EF%BC%89FGSM/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">对抗样本（三）FGSM</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/对抗样本（七）网络蒸馏/" title="对抗样本（七）网络蒸馏"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%EF%BC%88%E4%B8%83%EF%BC%89%E7%BD%91%E7%BB%9C%E8%92%B8%E9%A6%8F/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">对抗样本（七）网络蒸馏</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/对抗样本（九）C&W’s Attack/" title="对抗样本（九）C&W’s Attack"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%EF%BC%88%E4%B9%9D%EF%BC%89C&W%E2%80%99s%20Attack/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">对抗样本（九）C&W’s Attack</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/对抗样本（二）L-BFGS/" title="对抗样本（二）L-BFGS"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%EF%BC%88%E4%BA%8C%EF%BC%89L-BFGS/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">对抗样本（二）L-BFGS</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/对抗样本（八）JSMA/" title="对抗样本（八）JSMA"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%EF%BC%88%E5%85%AB%EF%BC%89JSMA/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">对抗样本（八）JSMA</div></div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: 'd3948be001a91411dfd9',
  clientSecret: 'ebddf2a2a5a039922fb373a8a8c0efcc439bf6ca',
  repo: 'BaiDingHub.github.io',
  owner: 'BaiDingHub',
  admin: ['BaiDingHub'],
  id: md5(decodeURI(location.pathname)),
  language: 'zh-CN',
  perPage: 10,
  distractionFreeMode: false,
  pagerDirection: 'last',
  createIssueManually: false,
  updateCountCallback: commentCount
})
gitalk.render('gitalk-container')

function commentCount(n){
  try {
    document.getElementsByClassName('gitalk-comment-count')[0].innerHTML= n
  } catch (e) {
    return false
  }
}</script></div></article></main><footer id="footer" style="background-image: url(https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89PGD%E5%8F%8A%E9%98%B2%E5%BE%A1/cover.png?raw=true)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By 白丁</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://jerryc.me/" target="_blank" rel="noopener">blog</a>!</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js" ></script><script src="/js/utils.js" ></script><script src="/js/main.js" ></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><script src="/js/search/algolia.js"></script></body></html>