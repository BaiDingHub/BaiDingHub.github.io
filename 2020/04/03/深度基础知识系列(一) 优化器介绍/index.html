<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>深度基础知识系列(一) 优化器介绍 | BaiDing's blog</title><meta name="description" content="深度基础知识系列(一) 优化器介绍"><meta name="keywords" content="深度基础知识,优化器"><meta name="author" content="白丁"><meta name="copyright" content="白丁"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="https://raw.githubusercontent.com/BaiDingHub/Blog_images/master/BlogSource/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="crossorigin"/><link rel="preconnect" href="http://ta.qq.com"/><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="x-1ef6P_miWkq-RJn_fmjd3KYumrXANNXYzK1myaLf0"/><meta name="baidu-site-verification" content="guD6l44Chk"/><meta name="twitter:card" content="summary"><meta name="twitter:title" content="深度基础知识系列(一) 优化器介绍"><meta name="twitter:description" content="深度基础知识系列(一) 优化器介绍"><meta name="twitter:image" content="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97/%E6%B7%B1%E5%BA%A6%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%B8%80)%20%E4%BC%98%E5%8C%96%E5%99%A8%E4%BB%8B%E7%BB%8D/cover.gif?raw=true"><meta property="og:type" content="article"><meta property="og:title" content="深度基础知识系列(一) 优化器介绍"><meta property="og:url" content="http://baidinghub.github.io/2020/04/03/%E6%B7%B1%E5%BA%A6%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%B8%80)%20%E4%BC%98%E5%8C%96%E5%99%A8%E4%BB%8B%E7%BB%8D/"><meta property="og:site_name" content="BaiDing's blog"><meta property="og:description" content="深度基础知识系列(一) 优化器介绍"><meta property="og:image" content="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97/%E6%B7%B1%E5%BA%A6%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%B8%80)%20%E4%BC%98%E5%8C%96%E5%99%A8%E4%BB%8B%E7%BB%8D/cover.gif?raw=true"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>var autoChangeMode = '1'
var t = Cookies.get("theme")
if (autoChangeMode == '1'){
  var isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
  var isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
  var isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined){
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport){
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour < 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
  }
  } else if (t == 'light') activateLightMode()
  else activateDarkMode()

} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css" ><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://baidinghub.github.io/2020/04/03/%E6%B7%B1%E5%BA%A6%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%B8%80)%20%E4%BC%98%E5%8C%96%E5%99%A8%E4%BB%8B%E7%BB%8D/"><link rel="prev" title="深度学习知识系列(二) 各种卷积形式" href="http://baidinghub.github.io/2020/04/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%BA%8C)%20%E5%90%84%E7%A7%8D%E5%8D%B7%E7%A7%AF%E5%BD%A2%E5%BC%8F/"><link rel="next" title="AI小知识系列(五) 面试小知识(2)" href="http://baidinghub.github.io/2020/04/03/AI%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%BA%94)%20%E9%9D%A2%E8%AF%95%E5%B0%8F%E7%9F%A5%E8%AF%86(2)/"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.js" defer></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-162698439-1', 'auto');
ga('send', 'pageview');
</script><script src="https://tajs.qq.com/stats?sId=66540586" charset="UTF-8"></script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"4JGH9NW4XG","apiKey":"41cdb6d9ec4d21196956524e9c985b36","indexName":"baiding","hits":{"per_page":6},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  highlightShrink: 'false',
  isFontAwesomeV5: false,
  isPhotoFigcaption: false
  
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isSidebar: true  
  }</script><noscript><style>
#page-header {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/avatar.png?raw=true" onerror="onerror=null;src='https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/friend_404.gif?raw=true'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">94</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">60</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">17</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于作者</span></a></div></div></div></div><i class="fa fa-arrow-right on" id="toggle-sidebar" aria-hidden="true">     </i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#一、梯度下降法-Gradient-Descent"><span class="toc-text">一、梯度下降法(Gradient Descent)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1、批量梯度下降法-BGD-Batch-Gradient-Descent"><span class="toc-text">1、批量梯度下降法(BGD, Batch Gradient Descent)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2、随机梯度下降法-SGD-Stochastic-Gradient-Descent"><span class="toc-text">2、随机梯度下降法(SGD, Stochastic Gradient Descent)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3、小批量梯度下降法-Mini-batch-Gradient-Descent"><span class="toc-text">3、小批量梯度下降法(Mini-batch Gradient Descent)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#二、动量优化法"><span class="toc-text">二、动量优化法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1、Momentum"><span class="toc-text">1、Momentum</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2、NAG（Nesterov-accelerated-gradient）"><span class="toc-text">2、NAG（Nesterov accelerated gradient）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#三、自适应学习率优化算法"><span class="toc-text">三、自适应学习率优化算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1、AdaGrad"><span class="toc-text">1、AdaGrad</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2、RMSprop"><span class="toc-text">2、RMSprop</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3、Adadelta"><span class="toc-text">3、Adadelta</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4、Adam-Adaptive-Moment-Estimation"><span class="toc-text">4、Adam: Adaptive Moment Estimation</span></a></li></ol></li></ol></div></div></div><div id="body-wrap"><div class="post-bg" id="nav" style="background-image: url(https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97/%E6%B7%B1%E5%BA%A6%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%B8%80)%20%E4%BC%98%E5%8C%96%E5%99%A8%E4%BB%8B%E7%BB%8D/cover.gif?raw=true)"><div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">BaiDing's blog</a></span><span class="pull_right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于作者</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span></span></div><div id="post-info"><div id="post-title"><div class="posttitle">深度基础知识系列(一) 优化器介绍</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2020-04-03 11:21:05"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2020-04-03</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-12-02 11:11:09"><i class="fa fa-history" aria-hidden="true"></i> 更新于 2020-12-02</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><i class="fa fa-angle-right post-meta__separator" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97/">深度基础知识系列</a></span></div><div class="meta-secondline"> <span class="post-meta-wordcount"><i class="post-meta__icon fa fa-file-word-o" aria-hidden="true"></i><span>字数总计:</span><span class="word-count">2.7k</span><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-clock-o" aria-hidden="true"></i><span>阅读时长: 9 分钟</span></span></div><div class="meta-thirdline"><span class="post-meta-pv-cv"><span class="post-meta__separator">|</span><i class="fa fa-eye post-meta__icon" aria-hidden="true"> </i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-comment-o" aria-hidden="true"></i><span>评论数:</span><a href="/2020/04/03/%E6%B7%B1%E5%BA%A6%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%B8%80)%20%E4%BC%98%E5%8C%96%E5%99%A8%E4%BB%8B%E7%BB%8D/#post-comment"><span class="gitalk-comment-count comment-count"></span></a></span></div></div></div></div><main class="layout_post" id="content-inner"><article id="post"><div id="article-container"><meta name="referrer" content="no-referrer"/>

<h1 id="一、梯度下降法-Gradient-Descent"><a href="#一、梯度下降法-Gradient-Descent" class="headerlink" title="一、梯度下降法(Gradient Descent)"></a>一、梯度下降法(Gradient Descent)</h1><p> &emsp;  &emsp;   微积分中，对多元函数的参数求$\ \theta$偏导数，把求得的各个参数的导数以向量的形式写出来就是梯度。梯度就是函数变化最快的地方。梯度下降是迭代法的一种，在求解机器学习算法的模型参数 $\ \theta$ 时，即无约束问题时，梯度下降是最常采用的方法之一。顾名思义，<strong>梯度下降法的计算过程就是沿梯度下降的方向求解极小值，也可以沿梯度上升方向求解最大值。</strong> 假设模型参数为$\ \theta$，损失函数为$\ J(\theta)$ ，损失函数$\ J(\theta)$关于参数$\ \theta$的偏导数，也就是梯度为 $\ \nabla_{\theta}J(\theta)$ ，学习率为 $\ \alpha$，则使用梯度下降法更新参数为：</p>
<script type="math/tex; mode=display">
\theta_{t+1} = \theta_t - \alpha ·\nabla_{\theta}J(\theta)</script><p> &emsp;  &emsp;   梯度下降法目前主要分为三种方法,区别在于每次参数更新时计算的样本数据量不同：<strong>批量梯度下降法(BGD, Batch Gradient Descent)，随机梯度下降法(SGD, Stochastic Gradient Descent)及小批量梯度下降法(Mini-batch Gradient Descent)。</strong></p>
<h2 id="1、批量梯度下降法-BGD-Batch-Gradient-Descent"><a href="#1、批量梯度下降法-BGD-Batch-Gradient-Descent" class="headerlink" title="1、批量梯度下降法(BGD, Batch Gradient Descent)"></a>1、批量梯度下降法(BGD, Batch Gradient Descent)</h2><p> &emsp;  &emsp;   假设训练样本总数为n，样本为 $\ {(x^1,y^1) ···(x^n,y^n)}$ ，模型参数为$\ \theta$，损失函数为$\ J(\theta)$ ，在第i对样本 $\ (x^i,y^i) $上损失函数关于参数的梯度为 $\ \nabla_{\theta}J(\theta,x^i,y^i)$  , 学习率为 $\ \alpha$，则使用BGD更新参数为：</p>
<script type="math/tex; mode=display">
\theta_{t+1} = \theta_t - \alpha_t ·\sum_{i=1}^n\nabla_{\theta}J_i(\theta,x^i,y^i)</script><p><strong>特点</strong>：</p>
<p>1、每进行一次参数更新，需要计算整个数据样本集，因此导致批量梯度下降法的速度会比较慢，尤其是数据集非常大的情况下，收敛速度就会非常慢</p>
<p>2、由于每次的下降方向为总体平均梯度，它得到的会是一个全局最优解。</p>
<p><br></p>
<h2 id="2、随机梯度下降法-SGD-Stochastic-Gradient-Descent"><a href="#2、随机梯度下降法-SGD-Stochastic-Gradient-Descent" class="headerlink" title="2、随机梯度下降法(SGD, Stochastic Gradient Descent)"></a>2、随机梯度下降法(SGD, Stochastic Gradient Descent)</h2><p> &emsp;  &emsp;   随机梯度下降法，不像BGD每一次参数更新，需要计算整个数据样本集的梯度，而是每次参数更新时，仅仅选取一个样本$\ (x^i,y^i) $计算其梯度，参数更新公式为：</p>
<script type="math/tex; mode=display">
\theta_{t+1} = \theta_t - \alpha_t ·\nabla_{\theta}J_i(\theta,x^i,y^i)</script><p><strong>特点</strong>：</p>
<p>1、可以看到BGD和SGD是两个极端，SGD由于每次参数更新仅仅需要计算一个样本的梯度，训练速度很快，即使在样本量很大的情况下，可能只需要其中一部分样本就能迭代到最优解</p>
<p>2、由于每次迭代并不是都向着整体最优化方向，导致梯度下降的波动非常大，更容易从一个局部最优跳到另一个局部最优，准确度下降。</p>
<p><br></p>
<h2 id="3、小批量梯度下降法-Mini-batch-Gradient-Descent"><a href="#3、小批量梯度下降法-Mini-batch-Gradient-Descent" class="headerlink" title="3、小批量梯度下降法(Mini-batch Gradient Descent)"></a>3、小批量梯度下降法(Mini-batch Gradient Descent)</h2><p> &emsp;  &emsp;   小批量梯度下降法就是结合BGD和SGD的折中，对于含有n个训练样本的数据集，每次参数更新，选择一个大小为m (m&lt;n)的mini-batch数据样本计算其梯度，其参数更新公式如下：</p>
<script type="math/tex; mode=display">
\theta_{t+1} = \theta_t - \alpha ·\sum_{i=x}^{i=x+m-1}\nabla_{\theta}J_i(\theta,x^i,y^i)</script><p> &emsp;  &emsp;   <strong>超参数建议值</strong>：$\ m\in[50,256]$ </p>
<p><strong>特点:</strong></p>
<p>1、小批量梯度下降法即保证了训练的速度，又能保证最后收敛的准确率，目前的SGD默认是小批量梯度下降算法。</p>
<p><br></p>
<p>PS：<strong>SGD的缺点</strong></p>
<p>1、选择合适的learning rate比较困难 ，学习率太低会收敛缓慢，学习率过高会使收敛时的波动过大</p>
<p>2、所有参数都是用同样的learning rate</p>
<p>3、SGD容易收敛到局部最优，并且在某些情况下可能被困在鞍点</p>
<p><br></p>
<h1 id="二、动量优化法"><a href="#二、动量优化法" class="headerlink" title="二、动量优化法"></a>二、动量优化法</h1><p> &emsp;  &emsp;   动量优化方法引入物理学中的动量思想，<strong>加速梯度下降</strong>，有Momentum和Nesterov两种算法。当我们将一个小球从山上滚下来，没有阻力时，它的动量会越来越大，但是如果遇到了阻力，速度就会变小，动量优化法就是借鉴此思想，使得梯度方向在不变的维度上，参数更新变快，梯度有所改变时，更新参数变慢，这样就能够加快收敛并且减少动荡。</p>
<h2 id="1、Momentum"><a href="#1、Momentum" class="headerlink" title="1、Momentum"></a>1、Momentum</h2><p> &emsp;  &emsp;   momentum算法思想：参数更新时在一定程度上保留之前更新的方向，同时又利用当前batch的梯度微调最终的更新方向，简言之就是通过积累之前的动量来加速当前的梯度。假设$\ m_t$表示t时刻的动量，$\ \mu$ 表示动量因子，通常取值0.9或者近似值，在SGD的基础上增加动量，则参数更新公式如下：</p>
<script type="math/tex; mode=display">
m_{t+1} = \mu ·m_t + \alpha·\nabla_{\theta}J(\theta) \\
\theta_{t+1} = \theta_t - m_{t+1}</script><p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://img-blog.csdnimg.cn/2020022819480188.png"  alt="在这里插入图片描述"></p>
<p> &emsp;  &emsp;   <strong>超参数建议值</strong>：$\ \mu=0.9$ </p>
<p><strong>特点</strong>：</p>
<p>1、momentum能够降低参数更新速度，从而减少震荡。</p>
<p>2、在梯度方向相同时，momentum可以加速参数更新， 从而加速收敛。</p>
<p><br></p>
<h2 id="2、NAG（Nesterov-accelerated-gradient）"><a href="#2、NAG（Nesterov-accelerated-gradient）" class="headerlink" title="2、NAG（Nesterov accelerated gradient）"></a>2、NAG（Nesterov accelerated gradient）</h2><p> &emsp;  &emsp;   但是，让“球”无脑地沿着斜坡向下滚并不总能得到让人满意的解。我们更希望有这样的一个小球，它能够对下一步怎么滚有个基本的“主见“，在滚过斜坡遇到上坡时，能够减速，防止越过最优解。NAG则在动量法的基础上引入了”预测“的功能。在动量法中，我们用$\ \mu·m_t$更新参数$\ \theta$，及先让小球按照惯性前进，计算出该处的梯度，便可以对其进行一个初步的预估。</p>
<script type="math/tex; mode=display">
m_{t+1} = \mu ·m_t + \alpha·\nabla_{\theta}J(\theta-\mu·m_t) \\
\theta_{t+1} = \theta_t - m_{t+1}</script><p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://img-blog.csdnimg.cn/20200228194753444.png"  alt="在这里插入图片描述"></p>
<p> &emsp;  &emsp;   当动量法第一次计算当前梯度时（短蓝线），并在累积梯度方向上进行了一大步参数更新（长蓝线）；NAG是首先在之前的梯度方向上走了一大步（棕色线），然后在此位置上评估一下梯度，最后做出一个相对正确的参数更新（绿线）。这种“先知”更新方法可以避免走过头而逃离了最优解——这在RNN相关的各种任务中收益甚好。</p>
<p> &emsp;  &emsp;   <strong>超参数建议值</strong>：$\ \mu=0.9$ </p>
<p><strong>特点：</strong></p>
<p>1、该方法用到了loss函数的二姐信息，比momentum收敛速度更快，波动更小</p>
<p><br></p>
<h1 id="三、自适应学习率优化算法"><a href="#三、自适应学习率优化算法" class="headerlink" title="三、自适应学习率优化算法"></a>三、自适应学习率优化算法</h1><p> &emsp;  &emsp;   在机器学习中，学习率是一个非常重要的超参数，但是学习率是非常难确定的，虽然可以通过多次训练来确定合适的学习率，但是一般也不太确定多少次训练能够得到最优的学习率，玄学事件，对人为的经验要求比较高，所以是否存在一些策略自适应地调节学习率的大小，从而提高训练速度。 目前的自适应学习率优化算法主要有：AdaGrad算法，RMSProp算法，Adam算法以及AdaDelta算法。</p>
<h2 id="1、AdaGrad"><a href="#1、AdaGrad" class="headerlink" title="1、AdaGrad"></a>1、AdaGrad</h2><p> &emsp;  &emsp;   AdaGrad可以自适应的调节学习率：对于很少更新的参数采用较大的学习率，对于频繁更新的参数采取更小的学习率——非常适合处理稀疏的数据。谷歌的Dean等人发现，在使用它训练大规模神经网络时，Adagrad很大程度上提升了SGD的鲁棒性。同时，Penningto等人用它来训练GloVe词嵌入模型，因为低频词相对高频词需要更大的学习步长。</p>
<script type="math/tex; mode=display">
g_{t,i} = \nabla J(\theta_{e,i}) \\
G_{t,i} = G_{t-1,i} + g_{t,i}^2 \\
\theta_{t+1,i} = \theta_{t,i} - \frac{\eta}{\sqrt{G+\epsilon}} ·g_{t,i} \\
\epsilon 是一个小的常数，用来防止分母未0，一般取10^{-6}</script><p> &emsp;  &emsp;   <strong>超参数建议值</strong>：$\ \eta=0.01$ </p>
<p><strong>特点:</strong></p>
<p>1、Adagrad的好处在于，它避开了人工调参学习率的问题，绝大部分开源库实现只需要在开始的时候设置η=0.01即可。</p>
<p>2、主要缺点在于它累积了历史梯度作为分母：因为每一个新梯度平方后都是非负值，在训练过程中，分母会越来越大，导致学习率整体会减小直至最后无限小——意味着算法不再能够学习到新的知识。</p>
<p><br></p>
<h2 id="2、RMSprop"><a href="#2、RMSprop" class="headerlink" title="2、RMSprop"></a>2、RMSprop</h2><p> &emsp;  &emsp;   RMSprop是Hinton在他的Coursera课程上提出的（未公布）的自适应学习率的梯度优化方法。它和Adadelta关注和解决的问题是一样的——学习率的单调递减问题。</p>
<script type="math/tex; mode=display">
g_{t} = \nabla J(\theta_{e}) \\
E[g^2]_t = \rho E[g^2]_{t-1} +(1-\rho) g_{t}^2 \\
\Delta \theta_t = \frac{\eta}{\sqrt{E[g^2]_t+\epsilon}} ·g_{t} \\
\theta_{t+1} = \theta_{t} - \Delta \theta_t</script><p> &emsp;  &emsp;   <strong>超参数建议值</strong>：$\ \rho=0.9，\eta=0.001$ </p>
<p><strong>特点：</strong></p>
<p>1、其实RMSprop依然依赖于全局学习率 [公式]<br>2、RMSprop算是Adagrad的一种发展，和Adadelta的变体，效果趋于二者之间<br>3、适合处理非平稳目标——对于RNN效果很好</p>
<p><br></p>
<h2 id="3、Adadelta"><a href="#3、Adadelta" class="headerlink" title="3、Adadelta"></a>3、Adadelta</h2><p> &emsp;  &emsp;   Adadelta是AdaGrad的一种拓展形式——Adadelta仅考虑了一个历史时间窗口下的累积梯度（在实现上并非存储历史梯度，而是借助衰减因子），避免了Adagrad中学习率总是单调递减的问题。</p>
<script type="math/tex; mode=display">
E[g^2]_t = \rho E[g^2]_{t-1} +(1-\rho) g_{t}^2 \\
初始化 \Delta \theta_t = \frac{\eta}{\sqrt{E[g^2]_t+\epsilon}} ·g_{t} \\
E[\Delta \theta^2]_t = \rho E[\Delta \theta^2]_{t-1} +(1-\rho) \Delta \theta_t^2 \\
记\ \ RMS[g]_t = \sqrt{E[g^2]_t+\epsilon} \\
记\ \ RMS[\Delta \theta]_t = \sqrt{E[\Delta \theta^2]_t+\epsilon} \\
\Delta \theta_t = \frac{RMS[\Delta \theta]_t}{RMS[g]_t} ·g_t \\
\theta_{t+1} = \theta_{t} - \Delta \theta_t</script><p> &emsp;  &emsp;   <strong>超参数建议值</strong>：$\ \rho=0.9$ </p>
<p><strong>特点：</strong></p>
<p>1、训练初中期，加速效果不错，很快。</p>
<p>2、训练后期，反复在局部最小值附近抖动。</p>
<p><br></p>
<h2 id="4、Adam-Adaptive-Moment-Estimation"><a href="#4、Adam-Adaptive-Moment-Estimation" class="headerlink" title="4、Adam: Adaptive Moment Estimation"></a>4、Adam: Adaptive Moment Estimation</h2><p> &emsp;  &emsp;   这个算法是另一种计算每个参数的自适应学习率的方法。<strong>相当于 RMSprop + Momentum</strong></p>
<p> &emsp;  &emsp;   除了像 Adadelta 和 RMSprop 一样存储了过去梯度的平方 $\ v_t$ 的指数衰减平均值 ，也像 momentum 一样保持了过去梯度 $\ m_t​$ 的<strong>指数衰减平均值</strong></p>
<script type="math/tex; mode=display">
初始化m,v=0 \\
m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t \\
v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2 \\
对m、v做偏差矫正(初始化为0带来的向0偏置的问题) \\
\hat{m}_t = \frac{m_t}{1-\beta_1} \\
\hat{v}_t = \frac{v_t}{1-\beta_2} \\
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t+\epsilon}}·\hat{m}_t</script><p> &emsp;  &emsp;   <strong>超参数建议值</strong>：$\ \beta_1=0.9，\beta_2=0.999，\epsilon=10^{-8}$</p>
<p><strong>特点</strong>：</p>
<p>1、相比于其他的自适应学习率优化算法，效果更好</p>
<p><strong>参考文献</strong></p>
<p><a href="https://zhuanlan.zhihu.com/p/55150256" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/55150256</a></p>
<p><a href="https://www.jianshu.com/p/0e561f62e64f" target="_blank" rel="noopener">https://www.jianshu.com/p/0e561f62e64f</a></p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">白丁</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://baidinghub.github.io/2020/04/03/%E6%B7%B1%E5%BA%A6%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%B8%80)%20%E4%BC%98%E5%8C%96%E5%99%A8%E4%BB%8B%E7%BB%8D/">http://baidinghub.github.io/2020/04/03/%E6%B7%B1%E5%BA%A6%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%B8%80)%20%E4%BC%98%E5%8C%96%E5%99%A8%E4%BB%8B%E7%BB%8D/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://baidinghub.github.io" target="_blank">BaiDing's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/">深度基础知识</a><a class="post-meta__tags" href="/tags/%E4%BC%98%E5%8C%96%E5%99%A8/">优化器</a></div><div class="post_share"><div class="addthis_inline_share_toolbox"></div><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDyIdOn/M/aXcGQOiSDVMqvN2gxDdiESHUfFuB2YMy48fvNN9SZOQUbVlGF4Pk6nDXIAir+br/EWuEnNLtgOCYMo/BTxl29gqS/QGHPiDaIQedzmLcuRZpfDuGit61N/b9pyktpZLagBgbl5Ox9mAgWQxXyhxYB092gyOXqrBULBeZUYQ+H7Eupha10QTQghHv4nLk+oYWo2UXEiijQpE3qMXT32G8v8k0KbRdd1hIFPyNEx6eZ6Buc2ZdbMtoutdGjvdnw5B1+dxgl1egESPChzcwCMxt3NW/3faQ5lsjRhQD4fRU+Ua/aqXe+0xg2+xr0BTjNr/JZIAaNiLuo71lH 1564026260@qq.com" async="async"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/wechat.png?raw=true" alt="微信"/><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/alipay.jpg?raw=true" alt="支付寶"/><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/04/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%BA%8C)%20%E5%90%84%E7%A7%8D%E5%8D%B7%E7%A7%AF%E5%BD%A2%E5%BC%8F/"><img class="prev_cover lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%BA%8C)%20%E5%90%84%E7%A7%8D%E5%8D%B7%E7%A7%AF%E5%BD%A2%E5%BC%8F/cover.gif?raw=true" onerror="onerror=null;src='https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/404.jpg?raw=true'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">深度学习知识系列(二) 各种卷积形式</div></div></a></div><div class="next-post pull_right"><a href="/2020/04/03/AI%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%BA%94)%20%E9%9D%A2%E8%AF%95%E5%B0%8F%E7%9F%A5%E8%AF%86(2)/"><img class="next_cover lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/AI%E5%B0%8F%E7%9F%A5%E8%AF%86/AI%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%BA%94)%20%20%E9%9D%A2%E8%AF%95%E5%B0%8F%E7%9F%A5%E8%AF%86(2)/cover.jpg?raw=true" onerror="onerror=null;src='https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/404.jpg?raw=true'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">AI小知识系列(五) 面试小知识(2)</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/04/03/深度学习知识系列(三) 卷积网络模型介绍/" title="深度学习知识系列(三) 卷积网络模型介绍"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%B8%89)%20%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">深度学习知识系列(三) 卷积网络模型介绍</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/深度学习知识系列(二) 各种卷积形式/" title="深度学习知识系列(二) 各种卷积形式"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%BA%8C)%20%E5%90%84%E7%A7%8D%E5%8D%B7%E7%A7%AF%E5%BD%A2%E5%BC%8F/cover.gif?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">深度学习知识系列(二) 各种卷积形式</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/目标检测(一)之 R-CNN/" title="目标检测（一）之 R-CNN"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B(%E4%B8%80)%E4%B9%8B%20R-CNN/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">目标检测（一）之 R-CNN</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/目标检测(二)之Fast R-CNN/" title="目标检测（二）之Fast R-CNN"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B(%E4%BA%8C)%E4%B9%8BFast%20R-CNN/cover.jpg?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">目标检测（二）之Fast R-CNN</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/目标检测(三)之Faster R-CNN/" title="目标检测（三）之Faster R-CNN"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B(%E4%B8%89)%E4%B9%8BFaster%20R-CNN/cover.jpg?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">目标检测（三）之Faster R-CNN</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/目标检测(五)之YOLOv1/" title="目标检测（五）之YOLOv1"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B(%E4%BA%94)%E4%B9%8BYOLOv1/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">目标检测（五）之YOLOv1</div></div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: 'd3948be001a91411dfd9',
  clientSecret: 'ebddf2a2a5a039922fb373a8a8c0efcc439bf6ca',
  repo: 'BaiDingHub.github.io',
  owner: 'BaiDingHub',
  admin: ['BaiDingHub'],
  id: md5(decodeURI(location.pathname)),
  language: 'zh-CN',
  perPage: 10,
  distractionFreeMode: false,
  pagerDirection: 'last',
  createIssueManually: false,
  updateCountCallback: commentCount
})
gitalk.render('gitalk-container')

function commentCount(n){
  try {
    document.getElementsByClassName('gitalk-comment-count')[0].innerHTML= n
  } catch (e) {
    return false
  }
}</script></div></article></main><footer id="footer" style="background-image: url(https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97/%E6%B7%B1%E5%BA%A6%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%B8%80)%20%E4%BC%98%E5%8C%96%E5%99%A8%E4%BB%8B%E7%BB%8D/cover.gif?raw=true)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By 白丁</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://jerryc.me/" target="_blank" rel="noopener">blog</a>!</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js" ></script><script src="/js/utils.js" ></script><script src="/js/main.js" ></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><script src="/js/search/algolia.js"></script></body></html>