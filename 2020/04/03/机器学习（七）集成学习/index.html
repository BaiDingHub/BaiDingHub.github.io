<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>机器学习（七）集成学习 | BaiDing's blog</title><meta name="description" content="机器学习（七）集成学习"><meta name="keywords" content="机器学习,集成学习"><meta name="author" content="白丁"><meta name="copyright" content="白丁"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="https://raw.githubusercontent.com/BaiDingHub/Blog_images/master/BlogSource/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="crossorigin"/><link rel="preconnect" href="http://ta.qq.com"/><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="x-1ef6P_miWkq-RJn_fmjd3KYumrXANNXYzK1myaLf0"/><meta name="baidu-site-verification" content="guD6l44Chk"/><meta name="twitter:card" content="summary"><meta name="twitter:title" content="机器学习（七）集成学习"><meta name="twitter:description" content="机器学习（七）集成学习"><meta name="twitter:image" content="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%83%EF%BC%89%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/cover.png?raw=true"><meta property="og:type" content="article"><meta property="og:title" content="机器学习（七）集成学习"><meta property="og:url" content="http://baidinghub.github.io/2020/04/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%83%EF%BC%89%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"><meta property="og:site_name" content="BaiDing's blog"><meta property="og:description" content="机器学习（七）集成学习"><meta property="og:image" content="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%83%EF%BC%89%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/cover.png?raw=true"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>var autoChangeMode = '1'
var t = Cookies.get("theme")
if (autoChangeMode == '1'){
  var isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
  var isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
  var isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined){
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport){
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour < 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
  }
  } else if (t == 'light') activateLightMode()
  else activateDarkMode()

} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css" ><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://baidinghub.github.io/2020/04/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%83%EF%BC%89%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"><link rel="prev" title="机器学习（八）集成学习之AdaBoost" href="http://baidinghub.github.io/2020/04/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AB%EF%BC%89%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8BAdaBoost/"><link rel="next" title="机器学习（六）贝叶斯分类器" href="http://baidinghub.github.io/2020/04/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.js" defer></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-162698439-1', 'auto');
ga('send', 'pageview');
</script><script src="https://tajs.qq.com/stats?sId=66540586" charset="UTF-8"></script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"4JGH9NW4XG","apiKey":"41cdb6d9ec4d21196956524e9c985b36","indexName":"baiding","hits":{"per_page":6},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  highlightShrink: 'false',
  isFontAwesomeV5: false,
  isPhotoFigcaption: false
  
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isSidebar: true  
  }</script><noscript><style>
#page-header {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/avatar.png?raw=true" onerror="onerror=null;src='https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/friend_404.gif?raw=true'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">97</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">60</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">17</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于作者</span></a></div></div></div></div><i class="fa fa-arrow-right on" id="toggle-sidebar" aria-hidden="true">     </i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#一、模型介绍"><span class="toc-text">一、模型介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1、Boosting"><span class="toc-text">1、Boosting</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1）AdaBoost"><span class="toc-text">1）AdaBoost</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#训练过程"><span class="toc-text">训练过程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2）GDBT"><span class="toc-text">2）GDBT</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#训练过程-1"><span class="toc-text">训练过程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3）XGBoost"><span class="toc-text">3）XGBoost</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2、Bagging"><span class="toc-text">2、Bagging</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1）随机森林"><span class="toc-text">1）随机森林</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3、Stacking"><span class="toc-text">3、Stacking</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#二、其他需知"><span class="toc-text">二、其他需知</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1、结合策略"><span class="toc-text">1、结合策略</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1）平均法"><span class="toc-text">1）平均法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2）投票法"><span class="toc-text">2）投票法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3）学习法"><span class="toc-text">3）学习法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2、集成学习的好处"><span class="toc-text">2、集成学习的好处</span></a></li></ol></li></ol></div></div></div><div id="body-wrap"><div class="post-bg" id="nav" style="background-image: url(https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%83%EF%BC%89%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/cover.png?raw=true)"><div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">BaiDing's blog</a></span><span class="pull_right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于作者</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span></span></div><div id="post-info"><div id="post-title"><div class="posttitle">机器学习（七）集成学习</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2020-04-03 14:07:05"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2020-04-03</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-12-02 11:11:09"><i class="fa fa-history" aria-hidden="true"></i> 更新于 2020-12-02</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><div class="meta-secondline"> <span class="post-meta-wordcount"><i class="post-meta__icon fa fa-file-word-o" aria-hidden="true"></i><span>字数总计:</span><span class="word-count">2.8k</span><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-clock-o" aria-hidden="true"></i><span>阅读时长: 9 分钟</span></span></div><div class="meta-thirdline"><span class="post-meta-pv-cv"><span class="post-meta__separator">|</span><i class="fa fa-eye post-meta__icon" aria-hidden="true"> </i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-comment-o" aria-hidden="true"></i><span>评论数:</span><a href="/2020/04/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%83%EF%BC%89%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/#post-comment"><span class="gitalk-comment-count comment-count"></span></a></span></div></div></div></div><main class="layout_post" id="content-inner"><article id="post"><div id="article-container"><meta name="referrer" content="no-referrer"/>



<h1 id="一、模型介绍"><a href="#一、模型介绍" class="headerlink" title="一、模型介绍"></a>一、模型介绍</h1><p> &emsp;&emsp;  集成学习(ensemble learning)通过<strong>构建并结合多个学习器来完成学习任务</strong>。</p>
<p> &emsp;&emsp;  集成学习一般结构为：先产生一组“个体学习器”(individual learner),再用某种策略将它们结合起来。个体学习器通常由一个现有的学习算法从训练数据产生。</p>
<p> &emsp;&emsp;  集成学习包括：</p>
<p> &emsp;&emsp;   &emsp;&emsp;  同质集成的个体学习器，也称为“基学习器”，即由同种类型的个体学习器集成得到的。</p>
<p> &emsp;&emsp;   &emsp;&emsp;  异质继承的个体学习器，也称为“组件学习器”，是由不同类型的个体学习器继承得到的。</p>
<p> &emsp;&emsp;  集成学习，主要可以分为三大类，<strong>Boosting</strong>，<strong>Bagging</strong>， <strong>Stacking</strong>。Boosting的代表有<strong>AdaBoost</strong>，<strong>GDBT</strong>， <strong>xgboost</strong>。而Bagging的代表则是<strong>随机森林 (Random Forest)</strong>。</p>
<h2 id="1、Boosting"><a href="#1、Boosting" class="headerlink" title="1、Boosting"></a>1、Boosting</h2><p> &emsp;&emsp;  Boosting 是一族可以<strong>将弱学习器提升为强学习器的算法</strong>。</p>
<p> &emsp;&emsp;  这族算法的工作机制类似：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基分类器，如此重复进行，直至基学习器数目达到事先指定的值T，最终将这T个基学习器进行加权结合。</p>
<h3 id="1）AdaBoost"><a href="#1）AdaBoost" class="headerlink" title="1）AdaBoost"></a>1）AdaBoost</h3><p> &emsp;&emsp;   <strong>AdaBoost方法的自适应在于：前一个分类器分错的样本会被用来训练下一个分类器</strong>。AdaBoost方法是一种迭代算法，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率。<strong>每一个训练样本都被赋予一个权重，表明它被某个分类器选入训练集的概率</strong>。如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它被选中的概率就被降低；相反，如果某个样本点没有被准确地分类，那么它的权重就得到提高。通过这样的方式，AdaBoost方法能“聚焦于”那些较难分（更富信息）的样本上。虽然AdaBoost方法<strong>对于噪声数据和异常数据很敏感</strong>。但相对于大多数其它学习算法而言，却又不会很容易出现过拟合现象。</p>
<p> &emsp;&emsp;  AdaBoost是一种”加性模型”，即基学习器的线性组合：</p>
<script type="math/tex; mode=display">
H(x) = \sum_{t=1}^T \alpha_th_t(x)</script><p> &emsp;&emsp;  整体要做的事情，就是最小化指数损失函数：</p>
<script type="math/tex; mode=display">
l_{exp}(H|D) = \mathbb{E}_{x \thicksim D}[e^{-f(x)H(x)}]</script><h4 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h4><p> &emsp;&emsp;  最初，训练集会有一个初始的样本权值分布（平均分布），基于训练集$ D$ 与该分布$ D_t$ 训练一个基学习器$ h_t$ ，估计$ h_t$ 的分类误差，根据误差确定分类器$ h_t$的权重，根据误差等信息重新确定样本的权值分布$ D_{t+1}$ （为分类错误的样本增加权重），伪代码如下图所示：</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%83%EF%BC%89%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/1.png?raw=true"  alt="1"></p>
<p> &emsp;&emsp;  具体的公式推导，请查看博主博客—集成学习之AdaBoost</p>
<h3 id="2）GDBT"><a href="#2）GDBT" class="headerlink" title="2）GDBT"></a>2）GDBT</h3><p> &emsp;&emsp;   提升树模型是以分类树或回归树为基本分类器的提升方法，其采用<strong>加法模型</strong>和<strong>前向分布算法</strong>。基于处理过程中所使用的损失函数的不同，我们有<strong>用平方误差损失函数的回归问题</strong>，使<strong>用指数损失函数的分类问题</strong>，以及<strong>一般损失函数的一般决策问题</strong>。</p>
<p> &emsp;&emsp;  <strong>GDBT（Gradient Descent Boosting Tree）</strong>，梯度提升树，是以回归树为基本分类器的提升方法。是一种<strong>基于残差</strong>的处理方法，<strong>常用来处理回归类问题</strong>。</p>
<p> &emsp;&emsp;  提升树模型可以表示为决策树的<strong>加法模型</strong>：</p>
<script type="math/tex; mode=display">
f_M(x) = \sum_{m=1}^MT(x;\Theta_m)</script><p> &emsp;&emsp;  其中，$ T(x;\Theta_m)$ 表示第m颗决策树，$ \Theta_m$ 表示决策树的参数，$ M$ 表示树的个数。</p>
<p> &emsp;&emsp;  在GDBT中采用的损失函数为<strong>平方差损失函数</strong>：</p>
<script type="math/tex; mode=display">
L(y,f(x)) = (y-f(x))^2</script><h4 id="训练过程-1"><a href="#训练过程-1" class="headerlink" title="训练过程"></a><strong>训练过程</strong></h4><p> &emsp;&emsp;  首先初始化$ f_0(x)=0$ ，初始化第一个残差数据集$ r_i=y_i-f_0(x_i)$ （原数据集），根据原数据建立一颗回归树$ T(x;\Theta_0)$ ，根据回归树得到的结果，再次计算残差，将这次得到的残差作为下一颗回归树的数据集，得到回归树$ T(x;\Theta_m)$ ，并且更新$ f_m(x) = f_{m-1}(x)+T(x;\Theta_m)$ ，不断地迭代，我们就能够得到最终的集成模型：</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%83%EF%BC%89%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/2.png?raw=true"  alt="1"></p>
<p> &emsp;&emsp;  具体的GDBT详解，请查看博主博客—集成学习之GDBT</p>
<h3 id="3）XGBoost"><a href="#3）XGBoost" class="headerlink" title="3）XGBoost"></a>3）XGBoost</h3><p> &emsp;&emsp;   <strong>XGBoost</strong>的全称是 eXtremeGradient Boosting，2014年2月诞生的专注于梯度提升算法的机器学习函数库，作者为华盛顿大学研究机器学习的大牛——陈天奇。他在研究中深深的体会到现有库的<strong>计算速度和精度</strong>问题，为此而着手搭建完成 xgboost 项目。xgboost问世后，因其优良的学习效果以及高效的训练速度而获得广泛的关注，并在各种算法大赛上大放光彩。</p>
<p> &emsp;&emsp;  在我们使用GDBT时，我们采用多次小步的迭代方式来拟合我们的集成模型，GDBT只利用了训练集的一阶信息，这使得我们拟合的速度比较慢，这就造成了我们在训练时可能会生成大量的树模型。为了解决这个问题，XGBoost，采用了训练集的<strong>二阶信息</strong>，加快了拟合速度，同时<strong>修改了在属性划分时的规则</strong>，提高了精度。</p>
<p> &emsp;&emsp;  具体的GDBT详解，请查看博主博客—集成学习之XGBoost</p>
<h2 id="2、Bagging"><a href="#2、Bagging" class="headerlink" title="2、Bagging"></a>2、Bagging</h2><p> &emsp;&emsp;  Bagging是<strong>并行集成学习方法</strong>最著名的代表，Bagging要求”<strong>不稳定</strong>“（指数据集的小的变动能够使得分类结果显著的变动）的分类方法，比如：决策树、神经网络。</p>
<p> &emsp;&emsp;  Bagging通过自主采样法，从包含m个样本的数据集中，采样出T个含m个训练样本的采样集，然后基于每一个采样集训练出一个基学习器，再将这些基学习器进行结合。</p>
<p> &emsp;&emsp;  Bagging通常对分类任务使用简单投票法，对回归任务采用简单平均法，其算法描述如下图：</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%83%EF%BC%89%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/3.png?raw=true"  alt="3"></p>
<h3 id="1）随机森林"><a href="#1）随机森林" class="headerlink" title="1）随机森林"></a>1）随机森林</h3><p> &emsp;&emsp;  随机森林(Random Forest 简称RF)，是Bagging的一个拓展变体。RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中<strong>引入了随机属性选择</strong>。传统决策树在选择划分属性时是在当前结点的属性集合(假定有d个属性)中选择一个最优属性，而在RF中，对决策树的每个结点，先从该结点的属性集合中<strong>随机选择一个包含k个属性的子集</strong>，然后再从这个子集中选择一个最优属性用于划分，这里参数k控制了随机性的引入程度。当k=d时，基决策树跟传统决策树相同；k=1时，则是随机选择一个属性用于划分；一般情况下推荐$ k=log_2d$ </p>
<p><strong>优点</strong>：RF简单，容易实现，计算开销小，性能强大。它的多样性不仅来自于样本扰动，还来自于属性扰动，这使得它的泛化性能进一步上升。</p>
<p><strong>缺点</strong>：它在训练和预测时都比较慢，而且如果需要区分的类别很多时，随机森林的表现并不会很好</p>
<p><br></p>
<h2 id="3、Stacking"><a href="#3、Stacking" class="headerlink" title="3、Stacking"></a>3、Stacking</h2><p> &emsp;&emsp;  Stacking是一种将模型输出作为特征，再送入训练器进行分类的一种算法，其主要的算法流程如下：</p>
<ol>
<li>在训练集D上，使用一种集成策略（AdaBoost，GBDT，RF等都可以），训练出T个基学习器</li>
<li>利用这T个基学习器，对训练集上的每一个样本进行预测，对于第$ i$ 个样本的输出$ x_1,x_2,…,x_T$ ，将其作为一个新的样本，新样本的标签就是原始训练集上第$ i$ 个样本的标签。这样，就产生了一个新的训练集$ D’$</li>
<li><p>以$ D’$ 为训练集，训练一个新的分类器</p>
<p>&emsp;&emsp;  上面的Stacking只做了一层，我们可以做好多层。但是层数并不是越多越好，一般一两层就够了。这其实也是一种集成学习的结合策略。</p>
</li>
</ol>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%83%EF%BC%89%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/4.png?raw=true"  alt="4"></p>
<h1 id="二、其他需知"><a href="#二、其他需知" class="headerlink" title="二、其他需知"></a>二、其他需知</h1><h2 id="1、结合策略"><a href="#1、结合策略" class="headerlink" title="1、结合策略"></a>1、结合策略</h2><p> &emsp;&emsp;  当我们使用集成学习训练出多个基分类器后，我们要使用结合策略来对多个基分类器的结果进行结合</p>
<h3 id="1）平均法"><a href="#1）平均法" class="headerlink" title="1）平均法"></a>1）平均法</h3><p> &emsp;&emsp;  对于数值型输出$ h_i(x)\in \mathbb{R}$ ，最常见的结合策略就是平均法</p>
<p><strong>简单平均法</strong></p>
<script type="math/tex; mode=display">
H(x) = \frac{1}{T}\sum_{i=1}^Th_i(x)</script><p><strong>加权平均法</strong></p>
<script type="math/tex; mode=display">
H(x) = \sum_{i=1}^Tw_ih_i(x)</script><h3 id="2）投票法"><a href="#2）投票法" class="headerlink" title="2）投票法"></a>2）投票法</h3><p> &emsp;&emsp;  对于分类任务来说，学习器$ h_i$ 将从类别标记集合$ {c_1,c_2,…,c_N}$ 中预测出一个标记，最常见的结合策略就是投票法。我们将$ h_i$ 在样本$ x$ 上的预测输出表示成一个N维向量$ (h_i^1(x);h_i^2(x);…h_i^N(x);)$ ，其中$ h_i^j(x)$ 是$ h_i$ 在类别标记$ c_j$ 上的输出。</p>
<p> <strong>绝大多数投票法</strong></p>
<script type="math/tex; mode=display">
H(x) =
\begin{cases}
c_j & & if\ \sum_{i=1}^Th_i^j(x)>0.5\sum_{k=1}^N\sum_{i=1}^Th_i^k(x)\\
reject & & otherwise
\end{cases} \\</script><p><strong>相对多数投票法</strong></p>
<script type="math/tex; mode=display">
H(X) = c_{argmax_j\ \sum_{i=1}^Th_i^j(x)}</script><p><strong>加权投票法</strong></p>
<script type="math/tex; mode=display">
H(X) = c_{argmax_j\ \sum_{i=1}^Tw_ih_i^j(x)}</script><h3 id="3）学习法"><a href="#3）学习法" class="headerlink" title="3）学习法"></a>3）学习法</h3><p> &emsp;&emsp;  上面讲到的Stacking方法</p>
<h2 id="2、集成学习的好处"><a href="#2、集成学习的好处" class="headerlink" title="2、集成学习的好处"></a>2、集成学习的好处</h2><ul>
<li>从统计的方面来看，由于学习任务的假设空间往往很大，可能有多个假设在训练集上达到同等性能，此时若但学习器的泛化性能往往不佳，而我们对其进行集成后，集成后的学习器的拟合空间将会变大，会<strong>增加学习器的泛化性能</strong>。</li>
<li>从计算的方面来看，学习算法往往会陷入局部极小，有的局部极小点所对应的泛化性能可能会很糟糕，而通过多次运行之后进行结合，可<strong>降低陷入糟糕局部极小点的风险</strong></li>
<li>从表示的方面来看，某些学习任务的真实假设可能不在当前学习算法所考虑的假设空间中，此时若使用但学习器则肯定无效，而通过结合多个学习器，由于假设空间有所扩大，所以就有可能学的更好的近似。同第一点。</li>
</ul>
<p><strong>参考链接</strong></p>
<ul>
<li><a href="https://blog.csdn.net/zhuangxiaobin/article/details/26075667" target="_blank" rel="noopener">AdaBoost算法详解</a></li>
<li>周志华老师的《机器学习》</li>
</ul>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">白丁</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://baidinghub.github.io/2020/04/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%83%EF%BC%89%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/">http://baidinghub.github.io/2020/04/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%83%EF%BC%89%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://baidinghub.github.io" target="_blank">BaiDing's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a class="post-meta__tags" href="/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/">集成学习</a></div><div class="post_share"><div class="addthis_inline_share_toolbox"></div><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDyIdOn/M/aXcGQOiSDVMqvN2gxDdiESHUfFuB2YMy48fvNN9SZOQUbVlGF4Pk6nDXIAir+br/EWuEnNLtgOCYMo/BTxl29gqS/QGHPiDaIQedzmLcuRZpfDuGit61N/b9pyktpZLagBgbl5Ox9mAgWQxXyhxYB092gyOXqrBULBeZUYQ+H7Eupha10QTQghHv4nLk+oYWo2UXEiijQpE3qMXT32G8v8k0KbRdd1hIFPyNEx6eZ6Buc2ZdbMtoutdGjvdnw5B1+dxgl1egESPChzcwCMxt3NW/3faQ5lsjRhQD4fRU+Ua/aqXe+0xg2+xr0BTjNr/JZIAaNiLuo71lH 1564026260@qq.com" async="async"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/wechat.png?raw=true" alt="微信"/><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/alipay.jpg?raw=true" alt="支付寶"/><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/04/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AB%EF%BC%89%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8BAdaBoost/"><img class="prev_cover lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AB%EF%BC%89%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8BAdaBoost/cover.png?raw=true" onerror="onerror=null;src='https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/404.jpg?raw=true'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">机器学习（八）集成学习之AdaBoost</div></div></a></div><div class="next-post pull_right"><a href="/2020/04/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/"><img class="next_cover lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/cover.png?raw=true" onerror="onerror=null;src='https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/404.jpg?raw=true'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">机器学习（六）贝叶斯分类器</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/04/03/机器学习（九）集成学习之GDBT/" title="机器学习（九）集成学习之GDBT"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8BGDBT/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">机器学习（九）集成学习之GDBT</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/机器学习（八）集成学习之AdaBoost/" title="机器学习（八）集成学习之AdaBoost"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AB%EF%BC%89%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8BAdaBoost/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">机器学习（八）集成学习之AdaBoost</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/机器学习（十）集成学习之XGBoost/" title="机器学习（十）集成学习之XGBoost"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%8D%81%EF%BC%89%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8BXGBoost/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">机器学习（十）集成学习之XGBoost</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/机器学习（一）Logistic回归/" title="机器学习（一）Logistic回归"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89Logistic%E5%9B%9E%E5%BD%92/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">机器学习（一）Logistic回归</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/机器学习（三）SVM/" title="机器学习（三）SVM"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89SVM/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">机器学习（三）SVM</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/机器学习（二）线性判别分析LDA/" title="机器学习（二）线性判别分析LDA"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90LDA/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">机器学习（二）线性判别分析LDA</div></div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: 'd3948be001a91411dfd9',
  clientSecret: 'ebddf2a2a5a039922fb373a8a8c0efcc439bf6ca',
  repo: 'BaiDingHub.github.io',
  owner: 'BaiDingHub',
  admin: ['BaiDingHub'],
  id: md5(decodeURI(location.pathname)),
  language: 'zh-CN',
  perPage: 10,
  distractionFreeMode: false,
  pagerDirection: 'last',
  createIssueManually: false,
  updateCountCallback: commentCount
})
gitalk.render('gitalk-container')

function commentCount(n){
  try {
    document.getElementsByClassName('gitalk-comment-count')[0].innerHTML= n
  } catch (e) {
    return false
  }
}</script></div></article></main><footer id="footer" style="background-image: url(https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%83%EF%BC%89%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/cover.png?raw=true)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By 白丁</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://jerryc.me/" target="_blank" rel="noopener">blog</a>!</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js" ></script><script src="/js/utils.js" ></script><script src="/js/main.js" ></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><script src="/js/search/algolia.js"></script></body></html>