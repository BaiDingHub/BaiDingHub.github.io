<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>机器学习（四）Decision Tree决策树 | BaiDing's blog</title><meta name="description" content="机器学习（四）Decision Tree决策树"><meta name="keywords" content="机器学习,Decision Tree"><meta name="author" content="白丁"><meta name="copyright" content="白丁"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="https://raw.githubusercontent.com/BaiDingHub/Blog_images/master/BlogSource/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="crossorigin"/><link rel="preconnect" href="http://ta.qq.com"/><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="x-1ef6P_miWkq-RJn_fmjd3KYumrXANNXYzK1myaLf0"/><meta name="baidu-site-verification" content="guD6l44Chk"/><meta name="twitter:card" content="summary"><meta name="twitter:title" content="机器学习（四）Decision Tree决策树"><meta name="twitter:description" content="机器学习（四）Decision Tree决策树"><meta name="twitter:image" content="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89Decision%20Tree%E5%86%B3%E7%AD%96%E6%A0%91/cover.png?raw=true"><meta property="og:type" content="article"><meta property="og:title" content="机器学习（四）Decision Tree决策树"><meta property="og:url" content="http://baidinghub.github.io/2020/04/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89Decision%20Tree%E5%86%B3%E7%AD%96%E6%A0%91/"><meta property="og:site_name" content="BaiDing's blog"><meta property="og:description" content="机器学习（四）Decision Tree决策树"><meta property="og:image" content="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89Decision%20Tree%E5%86%B3%E7%AD%96%E6%A0%91/cover.png?raw=true"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>var autoChangeMode = '1'
var t = Cookies.get("theme")
if (autoChangeMode == '1'){
  var isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
  var isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
  var isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined){
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport){
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour < 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
  }
  } else if (t == 'light') activateLightMode()
  else activateDarkMode()

} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css" ><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://baidinghub.github.io/2020/04/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89Decision%20Tree%E5%86%B3%E7%AD%96%E6%A0%91/"><link rel="prev" title="机器学习（五）K近邻算法" href="http://baidinghub.github.io/2020/04/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%94%EF%BC%89K%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/"><link rel="next" title="机器学习（三）SVM" href="http://baidinghub.github.io/2020/04/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89SVM/"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.js" defer></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-162698439-1', 'auto');
ga('send', 'pageview');
</script><script src="https://tajs.qq.com/stats?sId=66540586" charset="UTF-8"></script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"4JGH9NW4XG","apiKey":"41cdb6d9ec4d21196956524e9c985b36","indexName":"baiding","hits":{"per_page":6},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  highlightShrink: 'false',
  isFontAwesomeV5: false,
  isPhotoFigcaption: false
  
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isSidebar: true  
  }</script><noscript><style>
#page-header {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/avatar.png?raw=true" onerror="onerror=null;src='https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/friend_404.gif?raw=true'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">94</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">60</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">17</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于作者</span></a></div></div></div></div><i class="fa fa-arrow-right on" id="toggle-sidebar" aria-hidden="true">     </i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1、模型介绍"><span class="toc-text">1、模型介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-模型"><span class="toc-text">1.1 模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-决策树学习算法"><span class="toc-text">1.2 决策树学习算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-决策树构建过程"><span class="toc-text">1.3 决策树构建过程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-特征选择"><span class="toc-text">1) 特征选择</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#信息增益"><span class="toc-text">信息增益</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#增益率"><span class="toc-text">增益率</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#基尼指数"><span class="toc-text">基尼指数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-剪枝处理"><span class="toc-text">2) 剪枝处理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#预剪枝"><span class="toc-text">预剪枝</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#后剪枝"><span class="toc-text">后剪枝</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3）-连续与缺失值处理"><span class="toc-text">3） 连续与缺失值处理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#连续值处理"><span class="toc-text">连续值处理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#缺失值处理"><span class="toc-text">缺失值处理</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2、模型分析"><span class="toc-text">2、模型分析</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-模型优缺点"><span class="toc-text">2.1 模型优缺点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-多变量决策树"><span class="toc-text">2.2 多变量决策树</span></a></li></ol></li></ol></div></div></div><div id="body-wrap"><div class="post-bg" id="nav" style="background-image: url(https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89Decision%20Tree%E5%86%B3%E7%AD%96%E6%A0%91/cover.png?raw=true)"><div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">BaiDing's blog</a></span><span class="pull_right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于作者</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span></span></div><div id="post-info"><div id="post-title"><div class="posttitle">机器学习（四）Decision Tree决策树</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2020-04-03 14:04:05"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2020-04-03</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-12-02 11:11:09"><i class="fa fa-history" aria-hidden="true"></i> 更新于 2020-12-02</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><div class="meta-secondline"> <span class="post-meta-wordcount"><i class="post-meta__icon fa fa-file-word-o" aria-hidden="true"></i><span>字数总计:</span><span class="word-count">2.3k</span><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-clock-o" aria-hidden="true"></i><span>阅读时长: 7 分钟</span></span></div><div class="meta-thirdline"><span class="post-meta-pv-cv"><span class="post-meta__separator">|</span><i class="fa fa-eye post-meta__icon" aria-hidden="true"> </i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-comment-o" aria-hidden="true"></i><span>评论数:</span><a href="/2020/04/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89Decision%20Tree%E5%86%B3%E7%AD%96%E6%A0%91/#post-comment"><span class="gitalk-comment-count comment-count"></span></a></span></div></div></div></div><main class="layout_post" id="content-inner"><article id="post"><div id="article-container"><meta name="referrer" content="no-referrer"/>

<h1 id="1、模型介绍"><a href="#1、模型介绍" class="headerlink" title="1、模型介绍"></a>1、模型介绍</h1><p> &emsp;&emsp; 机器学习中决策树是一个预测模型，它表示<strong>对象属性和对象值之间的一种映射</strong>，树中的每一个节点表示对象属性的判断条件，其分支表示符合节点条件的对象。树的叶子节点表示对象所属的预测结果。</p>
<p> &emsp;&emsp; 根据数据的属性采用树状结构建立决策模型。决策树模型常常用来解决分类和回归问题。常见的算法包括 <strong>CART</strong> (Classification And Regression Tree)、<strong>ID3</strong>、<strong>C4.5</strong>、<strong>随机森林 (Random Forest)</strong> 等。</p>
<h2 id="1-1-模型"><a href="#1-1-模型" class="headerlink" title="1.1 模型"></a>1.1 模型</h2><p> &emsp;&emsp;  一棵树包含一个根节点，若干个内部节点和若干个叶节点，叶节点对应决策结果，其他节点对应一个属性测试</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89Decision%20Tree%E5%86%B3%E7%AD%96%E6%A0%91/1.png?raw=true"  alt="1"></p>
<h2 id="1-2-决策树学习算法"><a href="#1-2-决策树学习算法" class="headerlink" title="1.2 决策树学习算法"></a>1.2 决策树学习算法</h2><p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89Decision%20Tree%E5%86%B3%E7%AD%96%E6%A0%91/3.png?raw=true"  alt="2"></p>
<p>决策树是一个递归过程，有三种情况导致递归返回：</p>
<ul>
<li>当前样本包含的样本全属于同一类别，无需划分</li>
<li>当前样本属性集为空，或是所有样本在所有属性上取值相同，无法划分(取当前节点样本中类别最多的那一类作为分类)</li>
<li>当前节点包含的样本集合为空，不能划分(取父节点样本类别最多的作为分类)</li>
</ul>
<h2 id="1-3-决策树构建过程"><a href="#1-3-决策树构建过程" class="headerlink" title="1.3 决策树构建过程"></a>1.3 决策树构建过程</h2><h3 id="1-特征选择"><a href="#1-特征选择" class="headerlink" title="1) 特征选择"></a>1) 特征选择</h3><p> &emsp;&emsp; 特征选择是指在内部节点中选择一个特征来作为分类特征，特征选择决定了使用哪些特征来做判断。在训练数据集中，每个样本的属性可能有很多个，不同属性的作用有大有小。因而特征选择的作用就是筛选出跟分类结果相关性较高的特征，也就是分类能力较强的特征。</p>
<p> &emsp;&emsp; 在特征选择中通常使用的准则是：<strong>信息增益</strong>、<strong>增益率</strong>、<strong>基尼指数</strong></p>
<h4 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a><strong>信息增益</strong></h4><p> &emsp;&emsp; 信息熵(information entropy)是度量样本集合纯度的常用指标.</p>
<p> &emsp;&emsp; 假定当前样本集合$\ D$ 中第$\ k$类样本所占的比例为 $\ p_k(k=1,2,…,|Y|)$,则$\ D$的信息熵为:</p>
<script type="math/tex; mode=display">
Ent(D) = -\sum_{i=1}^{|Y|} p_k log_2(p_k)</script><p> &emsp;&emsp;  <code>Ent(D)</code>的值越小，D的纯度越高(约定：若$\ p=0$则$\ plog_2p=0$)</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89Decision%20Tree%E5%86%B3%E7%AD%96%E6%A0%91/2.png?raw=true"  alt="3"></p>
<p> &emsp;&emsp; 一般而言，信息增益越大，则意味着用属性a来进行划分所获得的纯度提升越大，因此，我们选择能够使信息增益最大的属性$\ a_*$ 作为该节点的分类特征：</p>
<script type="math/tex; mode=display">
a_* = arg\ max_{a\in A} Gain(D,a)</script><p> &emsp;&emsp; <code>ID3</code>就是以信息增益为准则来选择划分属性的</p>
<h4 id="增益率"><a href="#增益率" class="headerlink" title="增益率"></a><strong>增益率</strong></h4><p> &emsp;&emsp; 实际上，信息增益对可取值数目较多的属性有所偏好(如编号，在西瓜集中若以编号为划分属性，则其信息增益最大)，为减少由于偏好而带来的不利影响，<code>C4.5</code>算法使用增益率(<code>gain ratio</code>)来选择最优划分属性:</p>
<script type="math/tex; mode=display">
IV(a) = -\sum_{v=1}^{V} \frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}\\
Gain\ ratio(D,a) = \frac{Gain(D,a)}{IV(a)}</script><p> &emsp;&emsp; <code>IV(a)</code>被称为称为属性a的固有值(intrinsic value),属性<code>a</code>的可能数目越多，则<code>IV(a)</code>的值通常越大。</p>
<p> &emsp;&emsp; 然而，增益率准则对可取值数目较少的属性有所偏好，<code>C4.5</code>采用的是先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的</p>
<h4 id="基尼指数"><a href="#基尼指数" class="headerlink" title="基尼指数"></a><strong>基尼指数</strong></h4><p> &emsp;&emsp; 基尼指数是另一种数据的不纯度的度量方法，<code>CART</code>(Clasification and Regression Tree)使用基尼指数(Gini index)来选择划分属性</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89Decision%20Tree%E5%86%B3%E7%AD%96%E6%A0%91/4.png?raw=true"  alt="4"></p>
<p> &emsp;&emsp; 属性a的基尼指数定义为：</p>
<script type="math/tex; mode=display">
Gini(D,a) = \sum_{v=1}^V\frac{|D^v|}{|D|}Gini(D^v)</script><p> &emsp;&emsp; 我们选择能够使基尼指数最大的属性$\ a_*$ 作为该节点的分类特征：</p>
<script type="math/tex; mode=display">
a_* = arg\ max_{a\in A} Gini(D,a)</script><p>注意：每次选择一个离散特征后，都要将该特征从特征集合中去除，即之后的节点不会再用该节点进行划分</p>
<h3 id="2-剪枝处理"><a href="#2-剪枝处理" class="headerlink" title="2) 剪枝处理"></a>2) 剪枝处理</h3><p> &emsp;&emsp; 剪枝(pruning)是决策树学习算法对付过拟合的主要手段，基本策略有预剪枝(prepruning)和后剪枝(post-pruning)</p>
<ul>
<li>预剪枝：在决策树的生成过程中，对每个节点在划分前先进行估计，若当前节点的划分不能带来泛化性能提升则停止划分</li>
<li>后剪枝：先生成一个完整的树，然后自底向上对非叶节点考察，若将该节点对应的子数替换为叶节点能提升泛化性能则替换</li>
</ul>
<h4 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a><strong>预剪枝</strong></h4><p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89Decision%20Tree%E5%86%B3%E7%AD%96%E6%A0%91/5.png?raw=true"  alt="5"></p>
<p> &emsp;&emsp; 预剪枝使决策树的很多分支都没有展开，不仅降低了过拟合的风险，还显著减少了训练时间和测试时间，但是可能会引起过拟合</p>
<h4 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h4><p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89Decision%20Tree%E5%86%B3%E7%AD%96%E6%A0%91/6.png?raw=true"  alt="6"></p>
<p> &emsp;&emsp; 后剪枝通常比预剪枝保留更多的分值，一般情况下，后剪枝欠拟合风险很小，泛化性能优于预剪枝，但其训练时间比未剪枝和预剪枝都要大得多</p>
<h3 id="3）-连续与缺失值处理"><a href="#3）-连续与缺失值处理" class="headerlink" title="3） 连续与缺失值处理"></a>3） 连续与缺失值处理</h3><h4 id="连续值处理"><a href="#连续值处理" class="headerlink" title="连续值处理"></a>连续值处理</h4><p> &emsp;&emsp; 前面讨论都是基于离散属性来生成决策树，对于连续属性可取数值不再有限，此时可以用连续属性离散化技术<br> &emsp;&emsp; 最简单的策略:<strong>二分法(bi-partition)</strong>,这正是<code>C4.5</code>算法采用的机制</p>
<p> &emsp;&emsp; 对连续属性a，我们可考察包含n-1个元素的候选划分点集合：</p>
<script type="math/tex; mode=display">
T_a = \{\frac{a_i+a_{i+1}}{2},1\le i \le n-1\}</script><p> &emsp;&emsp; 即把区间$\ [a<em>i,a</em>{i+1})$ 的中位点作为候选划分点，然后就可像离散属性值一样来考察这些点：</p>
<script type="math/tex; mode=display">
Gain(D,a) = max_{t \in T_a} Gain(D,a,t) \\
= max_{t \in T_a} (Ent(D) - \sum_{\lambda \in \{-,+\}} \frac{|D_t^{\lambda}|}{|D|}Ent(D_t))</script><p> &emsp;&emsp; 需要注意的是，与离散属性不同，若当前节点划分属性为连续属性，该属性仍可作为其后代节点的划分属性</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89Decision%20Tree%E5%86%B3%E7%AD%96%E6%A0%91/7.png?raw=true"  alt="7"></p>
<h4 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h4><ul>
<li>如何在属性值缺失的情况下进行划分属性的选择？</li>
<li>给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分？</li>
</ul>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89Decision%20Tree%E5%86%B3%E7%AD%96%E6%A0%91/8.png?raw=true"  alt="8"></p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89Decision%20Tree%E5%86%B3%E7%AD%96%E6%A0%91/9.png?raw=true"  alt="9"></p>
<h1 id="2、模型分析"><a href="#2、模型分析" class="headerlink" title="2、模型分析"></a>2、模型分析</h1><h2 id="2-1-模型优缺点"><a href="#2-1-模型优缺点" class="headerlink" title="2.1 模型优缺点"></a>2.1 模型优缺点</h2><p><strong>优点</strong></p>
<ul>
<li>决策树易于理解和解释，可以<strong>可视化</strong>分析，容易提取出规则；</li>
<li>可以同时处理标称型和数值型数据；</li>
<li>比较<strong>适合处理有缺失属性</strong>的样本；</li>
<li>能够处理不相关的特征；</li>
<li>测试数据集时，运行<strong>速度比较快</strong>；</li>
<li>在相对短的时间内能够对大型数据源做出可行且效果良好的结果。</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>容易发生<strong>过拟合</strong>（随机森林可以很大程度上减少过拟合）；</li>
<li>容易<strong>忽略数据集中属性的相互关联</strong>；</li>
<li>对于那些各类别样本数量不一致的数据，在决策树中，进行属性划分时，不同的判定准则会带来不同的属性选择倾向；信息增益准则对可取数目较多的属性有所偏好（典型代表ID3算法），而增益率准则（CART）则对可取数目较少的属性有所偏好，但CART进行属性划分时候不再简单地直接利用增益率尽心划分，而是采用一种启发式规则）（只要是使用了信息增益，都有这个缺点，如RF）。</li>
</ul>
<h2 id="2-2-多变量决策树"><a href="#2-2-多变量决策树" class="headerlink" title="2.2 多变量决策树"></a>2.2 多变量决策树</h2><p> &emsp;&emsp; 经过上面的分析我们可以发现，决策树在每次决策的时候只考虑了一个属性，其忽略了数据集属性的相互管理。</p>
<p> &emsp;&emsp; 用专业的话讲就是，决策树所形成的分类边界有一个明显的特点：轴平行(axis-parallel),即它的分类边界由若干个与坐标轴平行的分段组成。这样的分类边界有较好的解释性，因为每段划分都直接对应了某个属性取值，但在分类任务比较复杂时，必须使用多段划分才能获得较好的近似。但若能使用斜的划分边界，决策树模型将大大简化。</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89Decision%20Tree%E5%86%B3%E7%AD%96%E6%A0%91/10.png?raw=true"  alt="10"></p>
<p> &emsp;&emsp; <strong>多变量决策树(multivariate decision tree)</strong>就是能实现斜划分甚至更复杂划分的决策树(亦称斜决策树 oblique decision tree)<br> &emsp;&emsp; 在此类决策树中，非叶节点不再是仅针对某个属性，而是针对属性的线性组合进行测试，每个非叶节点是一个形如$\ \sum_{i=1}^d w_ia_i=t$ 的线性分类器，$\ w_i$  和 $\ t$ 可在该结点所含的样本集和属性集上学得，它不是为每个非叶节点寻找一个最优划分属性，而是试图建立一个合适的线性分类器，如图：</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89Decision%20Tree%E5%86%B3%E7%AD%96%E6%A0%91/11.png?raw=true"  alt="11"></p>
<p><strong>参考链接</strong></p>
<ul>
<li><a href="https://www.ibm.com/developerworks/cn/analytics/library/ba-1507-decisiontree-algorithm/index.html" target="_blank" rel="noopener">决策树算法介绍及应用</a></li>
<li><a href="https://lovelyfrog.github.io/2018/03/06/第四章决策树/" target="_blank" rel="noopener">西瓜书笔记——第四章 决策树</a></li>
</ul>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">白丁</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://baidinghub.github.io/2020/04/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89Decision%20Tree%E5%86%B3%E7%AD%96%E6%A0%91/">http://baidinghub.github.io/2020/04/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89Decision%20Tree%E5%86%B3%E7%AD%96%E6%A0%91/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://baidinghub.github.io" target="_blank">BaiDing's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a class="post-meta__tags" href="/tags/Decision-Tree/">Decision Tree</a></div><div class="post_share"><div class="addthis_inline_share_toolbox"></div><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDyIdOn/M/aXcGQOiSDVMqvN2gxDdiESHUfFuB2YMy48fvNN9SZOQUbVlGF4Pk6nDXIAir+br/EWuEnNLtgOCYMo/BTxl29gqS/QGHPiDaIQedzmLcuRZpfDuGit61N/b9pyktpZLagBgbl5Ox9mAgWQxXyhxYB092gyOXqrBULBeZUYQ+H7Eupha10QTQghHv4nLk+oYWo2UXEiijQpE3qMXT32G8v8k0KbRdd1hIFPyNEx6eZ6Buc2ZdbMtoutdGjvdnw5B1+dxgl1egESPChzcwCMxt3NW/3faQ5lsjRhQD4fRU+Ua/aqXe+0xg2+xr0BTjNr/JZIAaNiLuo71lH 1564026260@qq.com" async="async"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/wechat.png?raw=true" alt="微信"/><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/alipay.jpg?raw=true" alt="支付寶"/><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/04/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%94%EF%BC%89K%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/"><img class="prev_cover lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%94%EF%BC%89K%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/cover.png?raw=true" onerror="onerror=null;src='https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/404.jpg?raw=true'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">机器学习（五）K近邻算法</div></div></a></div><div class="next-post pull_right"><a href="/2020/04/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89SVM/"><img class="next_cover lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89SVM/cover.png?raw=true" onerror="onerror=null;src='https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/404.jpg?raw=true'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">机器学习（三）SVM</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/04/03/机器学习（一）Logistic回归/" title="机器学习（一）Logistic回归"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89Logistic%E5%9B%9E%E5%BD%92/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">机器学习（一）Logistic回归</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/机器学习（七）集成学习/" title="机器学习（七）集成学习"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%83%EF%BC%89%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">机器学习（七）集成学习</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/机器学习（二）线性判别分析LDA/" title="机器学习（二）线性判别分析LDA"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90LDA/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">机器学习（二）线性判别分析LDA</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/机器学习（五）K近邻算法/" title="机器学习（五）K近邻算法"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%94%EF%BC%89K%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">机器学习（五）K近邻算法</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/机器学习（三）SVM/" title="机器学习（三）SVM"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89SVM/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">机器学习（三）SVM</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/机器学习（九）集成学习之GDBT/" title="机器学习（九）集成学习之GDBT"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B9%9D%EF%BC%89%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8BGDBT/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">机器学习（九）集成学习之GDBT</div></div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: 'd3948be001a91411dfd9',
  clientSecret: 'ebddf2a2a5a039922fb373a8a8c0efcc439bf6ca',
  repo: 'BaiDingHub.github.io',
  owner: 'BaiDingHub',
  admin: ['BaiDingHub'],
  id: md5(decodeURI(location.pathname)),
  language: 'zh-CN',
  perPage: 10,
  distractionFreeMode: false,
  pagerDirection: 'last',
  createIssueManually: false,
  updateCountCallback: commentCount
})
gitalk.render('gitalk-container')

function commentCount(n){
  try {
    document.getElementsByClassName('gitalk-comment-count')[0].innerHTML= n
  } catch (e) {
    return false
  }
}</script></div></article></main><footer id="footer" style="background-image: url(https://github.com/BaiDingHub/Blog_images/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89Decision%20Tree%E5%86%B3%E7%AD%96%E6%A0%91/cover.png?raw=true)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By 白丁</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://jerryc.me/" target="_blank" rel="noopener">blog</a>!</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js" ></script><script src="/js/utils.js" ></script><script src="/js/main.js" ></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><script src="/js/search/algolia.js"></script></body></html>