<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>AI小知识系列(五) 面试小知识(2) | BaiDing's blog</title><meta name="description" content="AI小知识系列(五) 面试小知识(2)"><meta name="keywords" content="AI小知识,面试小知识"><meta name="author" content="白丁"><meta name="copyright" content="白丁"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="https://raw.githubusercontent.com/BaiDingHub/Blog_images/master/BlogSource/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="crossorigin"/><link rel="preconnect" href="http://ta.qq.com"/><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="x-1ef6P_miWkq-RJn_fmjd3KYumrXANNXYzK1myaLf0"/><meta name="baidu-site-verification" content="guD6l44Chk"/><meta name="twitter:card" content="summary"><meta name="twitter:title" content="AI小知识系列(五) 面试小知识(2)"><meta name="twitter:description" content="AI小知识系列(五) 面试小知识(2)"><meta name="twitter:image" content="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/AI%E5%B0%8F%E7%9F%A5%E8%AF%86/AI%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%BA%94)%20%20%E9%9D%A2%E8%AF%95%E5%B0%8F%E7%9F%A5%E8%AF%86(2)/cover.jpg?raw=true"><meta property="og:type" content="article"><meta property="og:title" content="AI小知识系列(五) 面试小知识(2)"><meta property="og:url" content="http://baidinghub.github.io/2020/04/03/AI%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%BA%94)%20%E9%9D%A2%E8%AF%95%E5%B0%8F%E7%9F%A5%E8%AF%86(2)/"><meta property="og:site_name" content="BaiDing's blog"><meta property="og:description" content="AI小知识系列(五) 面试小知识(2)"><meta property="og:image" content="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/AI%E5%B0%8F%E7%9F%A5%E8%AF%86/AI%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%BA%94)%20%20%E9%9D%A2%E8%AF%95%E5%B0%8F%E7%9F%A5%E8%AF%86(2)/cover.jpg?raw=true"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>var autoChangeMode = '1'
var t = Cookies.get("theme")
if (autoChangeMode == '1'){
  var isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
  var isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
  var isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined){
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport){
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour < 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
  }
  } else if (t == 'light') activateLightMode()
  else activateDarkMode()

} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css" ><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://baidinghub.github.io/2020/04/03/AI%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%BA%94)%20%E9%9D%A2%E8%AF%95%E5%B0%8F%E7%9F%A5%E8%AF%86(2)/"><link rel="prev" title="深度基础知识系列(一) 优化器介绍" href="http://baidinghub.github.io/2020/04/03/%E6%B7%B1%E5%BA%A6%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%B8%80)%20%E4%BC%98%E5%8C%96%E5%99%A8%E4%BB%8B%E7%BB%8D/"><link rel="next" title="AI小知识系列(四)  Matplotlib常用操作" href="http://baidinghub.github.io/2020/04/03/AI%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E5%9B%9B)%20%20Matplotlib%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.js" defer></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-162698439-1', 'auto');
ga('send', 'pageview');
</script><script src="https://tajs.qq.com/stats?sId=66540586" charset="UTF-8"></script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"4JGH9NW4XG","apiKey":"41cdb6d9ec4d21196956524e9c985b36","indexName":"baiding","hits":{"per_page":6},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  highlightShrink: 'false',
  isFontAwesomeV5: false,
  isPhotoFigcaption: false
  
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isSidebar: true  
  }</script><noscript><style>
#page-header {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/avatar.png?raw=true" onerror="onerror=null;src='https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/friend_404.gif?raw=true'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">98</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">61</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">17</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于作者</span></a></div></div></div></div><i class="fa fa-arrow-right on" id="toggle-sidebar" aria-hidden="true">     </i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#AI小知识系列—第二节"><span class="toc-text">AI小知识系列—第二节</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1、机器学习中样本不平衡的处理方法"><span class="toc-text">1、机器学习中样本不平衡的处理方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-欠采样"><span class="toc-text">1.欠采样</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-过采样"><span class="toc-text">2.过采样</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-阈值移动"><span class="toc-text">3.阈值移动</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2、LR为什么使用交叉熵损失函数，其与最大似然函数是什么关系"><span class="toc-text">2、LR为什么使用交叉熵损失函数，其与最大似然函数是什么关系</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3、为什么交叉熵损失可以提高具有sigmoid输出的模型的性能，而使用均方误差损失则会存在很多问题"><span class="toc-text">3、为什么交叉熵损失可以提高具有sigmoid输出的模型的性能，而使用均方误差损失则会存在很多问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4、SVM-和-LR-的联系和区别"><span class="toc-text">4、SVM 和 LR 的联系和区别</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#相同点"><span class="toc-text">相同点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#不同点"><span class="toc-text">不同点</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-两者的本质不同在于其Loss函数不同"><span class="toc-text">1. 两者的本质不同在于其Loss函数不同</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-SVM依赖于数据的测度，而LR则不受影响"><span class="toc-text">2. SVM依赖于数据的测度，而LR则不受影响</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-SVM自带结构风险最小化，LR则是经验风险最小化"><span class="toc-text">3. SVM自带结构风险最小化，LR则是经验风险最小化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-SVM会用核函数而LR一般不用核函数"><span class="toc-text">4. SVM会用核函数而LR一般不用核函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-LR和SVM在实际应用的区别"><span class="toc-text">5. LR和SVM在实际应用的区别</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5、决策树为什么不需要归一化"><span class="toc-text">5、决策树为什么不需要归一化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6、过拟合的原因与解决方案"><span class="toc-text">6、过拟合的原因与解决方案</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#原因"><span class="toc-text">原因</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#解决方案"><span class="toc-text">解决方案</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7、L0、L1、L2范数的区别"><span class="toc-text">7、L0、L1、L2范数的区别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8、参数稀疏的好处"><span class="toc-text">8、参数稀疏的好处</span></a></li></ol></li></ol></div></div></div><div id="body-wrap"><div class="post-bg" id="nav" style="background-image: url(https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/AI%E5%B0%8F%E7%9F%A5%E8%AF%86/AI%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%BA%94)%20%20%E9%9D%A2%E8%AF%95%E5%B0%8F%E7%9F%A5%E8%AF%86(2)/cover.jpg?raw=true)"><div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">BaiDing's blog</a></span><span class="pull_right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于作者</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span></span></div><div id="post-info"><div id="post-title"><div class="posttitle">AI小知识系列(五) 面试小知识(2)</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2020-04-03 11:15:05"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2020-04-03</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-12-02 11:11:09"><i class="fa fa-history" aria-hidden="true"></i> 更新于 2020-12-02</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><i class="fa fa-angle-right post-meta__separator" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/AI%E5%B0%8F%E7%9F%A5%E8%AF%86/">AI小知识</a></span></div><div class="meta-secondline"> <span class="post-meta-wordcount"><i class="post-meta__icon fa fa-file-word-o" aria-hidden="true"></i><span>字数总计:</span><span class="word-count">3k</span><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-clock-o" aria-hidden="true"></i><span>阅读时长: 10 分钟</span></span></div><div class="meta-thirdline"><span class="post-meta-pv-cv"><span class="post-meta__separator">|</span><i class="fa fa-eye post-meta__icon" aria-hidden="true"> </i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-comment-o" aria-hidden="true"></i><span>评论数:</span><a href="/2020/04/03/AI%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%BA%94)%20%E9%9D%A2%E8%AF%95%E5%B0%8F%E7%9F%A5%E8%AF%86(2)/#post-comment"><span class="gitalk-comment-count comment-count"></span></a></span></div></div></div></div><main class="layout_post" id="content-inner"><article id="post"><div id="article-container"><meta name="referrer" content="no-referrer"/>



<h1 id="AI小知识系列—第二节"><a href="#AI小知识系列—第二节" class="headerlink" title="AI小知识系列—第二节"></a>AI小知识系列—第二节</h1><h2 id="1、机器学习中样本不平衡的处理方法"><a href="#1、机器学习中样本不平衡的处理方法" class="headerlink" title="1、机器学习中样本不平衡的处理方法"></a>1、机器学习中样本不平衡的处理方法</h2><p> &emsp;&emsp;  处理样本不均衡的方法主要有以下三种：</p>
<h3 id="1-欠采样"><a href="#1-欠采样" class="headerlink" title="1.欠采样"></a>1.欠采样</h3><p> &emsp;&emsp;  欠采样（undersampling）法是去除训练集内一些多数样本，使得两类数据量级接近，然后在正常进行学习。</p>
<p> &emsp;&emsp;  这种方法的缺点是就是放弃了很多反例，这会导致平衡后的训练集小于初始训练集。而且如果采样随机丢弃反例，会损失已经收集的信息，往往还会丢失重要信息。</p>
<p><strong>欠采样改进方法1</strong></p>
<p> &emsp;&emsp;  我们可以更改抽样方法来改进欠抽样方法，比如把多数样本分成核心样本和非核心样本，非核心样本为对预测目标较低概率达成的样本，可以考虑从非核心样本中删除而非随机欠抽样，这样保证了需要机器学习判断的核心样本数据不会丢失。</p>
<p><strong>欠采样改进方法2</strong></p>
<p> &emsp;&emsp;  另外一种欠采样的改进方法是 EasyEnsemble 提出的继承学习制度，它将多数样本划分成若 N个集合，然后将划分过后的集合与少数样本组合，这样就形成了N个训练集合，而且每个训练结合都进行了欠采样，但从全局来看却没有信息丢失。</p>
<h3 id="2-过采样"><a href="#2-过采样" class="headerlink" title="2.过采样"></a>2.过采样</h3><p> &emsp;&emsp;  过采样（oversampling）是对训练集内的少数样本进行扩充，既增加少数样本使得两类数据数目接近，然后再进行学习。</p>
<p> &emsp;&emsp;  简单粗暴的方法是复制少数样本，缺点是虽然引入了额外的训练数据，但没有给少数类样本增加任何新的信息，非常容易造成过拟合。</p>
<p><strong>过采样改进方法1</strong></p>
<p> &emsp;&emsp;  使用数据增强方法</p>
<p><strong>过采样代表算法：SMOTE 算法</strong></p>
<p> &emsp;&emsp;  SMOTE[Chawla et a., 2002]是通过对少数样本进行插值来获取新样本的。比如对于每个少数类样本a，从 a最邻近的样本中选取 样本b，然后在对 ab 中随机选择一点作为新样本。</p>
<h3 id="3-阈值移动"><a href="#3-阈值移动" class="headerlink" title="3.阈值移动"></a>3.阈值移动</h3><p> &emsp;&emsp;  这类方法的中心思想不是对样本集和做再平衡设置，而是对算法的决策过程进行改进。</p>
<p> &emsp;&emsp;  当我们的训练集具有$ m^+$ 个正例，$ m^-$ 个反例时，我们可以修改分类阈值，即只有当$ \frac{y}{1-y} &gt; \frac{m^+}{m^-}$ 时，分类为正例</p>
<p><br></p>
<h2 id="2、LR为什么使用交叉熵损失函数，其与最大似然函数是什么关系"><a href="#2、LR为什么使用交叉熵损失函数，其与最大似然函数是什么关系" class="headerlink" title="2、LR为什么使用交叉熵损失函数，其与最大似然函数是什么关系"></a>2、LR为什么使用交叉熵损失函数，其与最大似然函数是什么关系</h2><p> &emsp;&emsp;  LR通过sigmoid输出的值为样本分类为正例的概率，我们根据最大似然函数，可以得到每个样本分类正确的概率为$ \hat{y_i}^{y_i}(1-\hat{y_i})^{1-y_i}$ ，给概率取Log（防止连乘造成的下溢），得到$ y_ilog(\hat{y_i}) + (1-y_i)log(1-\hat{y_i})$ ，再取负数，就得到了我们的交叉熵损失函数。</p>
<p><br></p>
<h2 id="3、为什么交叉熵损失可以提高具有sigmoid输出的模型的性能，而使用均方误差损失则会存在很多问题"><a href="#3、为什么交叉熵损失可以提高具有sigmoid输出的模型的性能，而使用均方误差损失则会存在很多问题" class="headerlink" title="3、为什么交叉熵损失可以提高具有sigmoid输出的模型的性能，而使用均方误差损失则会存在很多问题"></a>3、为什么交叉熵损失可以提高具有sigmoid输出的模型的性能，而使用均方误差损失则会存在很多问题</h2><p> &emsp;&emsp;  我们知道Sigmoid有一个特点，即当输入很大或者很小时，其梯度接近0，这就使得我们在更新参数时更新的很慢。</p>
<p> &emsp;&emsp;  我们以LR为例，输入为$ x$，权重为$ w$ ，经过线性分类器得到$ z = wx+b$ ，经过Sigmoid函数$ y_i = \sigma(z)$ 。我们看一下交叉熵损失函数和均方差损失函数对于模型的参数的梯度是怎么样的。</p>
<p> &emsp;&emsp;  我们知道交叉熵损失函数为：</p>
<script type="math/tex; mode=display">
L(y_i,\hat{y}_i) = -(y_ilog\ \hat{y_i} + (1-y_i)log (1-\hat{y_i}))</script><p> &emsp;&emsp;  均方差损失函数为：</p>
<script type="math/tex; mode=display">
M(y_i,\hat{y_i}) = \frac{1}{2}(y-\hat{y_i})^2</script><p><strong>分析均方差损失函数</strong></p>
<p> &emsp;&emsp;  我们先来看一下均方差损失函数对于参数$ w$ 的梯度情况：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{split}
\frac{\partial{M(y_i,\hat{y_i})} }{ {\partial \hat{y_i} } }&=\hat{y_i}-y_i \\
\frac{\partial{\hat{y_i} } }{ {\partial z} }&=\sigma'(z) \\
\frac{\partial{z} }{ {\partial w} }&=x\\
\end{split}
\end{equation}</script><p> &emsp;&emsp;  于是我们可以得到：</p>
<script type="math/tex; mode=display">
\frac{\partial{M(y_i,\hat{y_i})} }{ {\partial w}}=\frac{\partial{M(y_i,\hat{y_i})} }{ {\partial \hat{y_i} } }·\frac{\partial{\hat{y_i} } }{ {\partial z} }·\frac{\partial{z} }{ {\partial w} } = (\hat{y_i}-y_i)·\sigma'(z)·x</script><p> &emsp;&emsp;  也就是说，均方差损失函数对输入x的梯度于Sigmoid的梯度相似，同样包含了Sigmoid的梯度消失问题。</p>
<p><strong>分析交叉熵损失函数</strong></p>
<p> &emsp;&emsp;  我们先来看一下交叉熵损失函数对于Sigmoid的输入x的梯度情况：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{split}
\frac{\partial{L(y_i,\hat{y_i})} }{ {\partial \hat{y_i} } }&=\frac{1-y_i}{1-\hat{y_i} }-\frac{y_i}{\hat{y_i} }\\
\frac{\partial{\hat{y_i}} }{ {\partial z} }&=\sigma'(z)\\
\frac{\partial{z} }{ {\partial w} }&=x\\
\end{split}
\end{equation}</script><p> &emsp;&emsp;  于是我们可以得到：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{split}
\frac{\partial{L(y_i,\hat{y_i})} }{ {\partial w} }&=\frac{\partial{L(y_i,\hat{y_i})} }{ {\partial \hat{y_i} } }·\frac{\partial{\hat{y_i} } }{ {\partial z} }·\frac{\partial{z} }{ {\partial w} } = (\frac{1-y_i}{1-\hat{y_i} }-\frac{y_i}{\hat{y_i} })·\sigma'(z)·x \\
&=(\frac{1-y_i}{1-\sigma(z)}-\frac{y_i}{\sigma(z)})·\sigma'(z)·x \\
故\frac{\partial{L(y_i,\hat{y_i})} }{ {\partial w} }&=[(1-y_i)\sigma(z)-y_i(1-\sigma(z))]x \\
&=(\sigma(z)-y_i)x \\
&=(\hat{y}_i-y_i)x \\
\end{split}
\end{equation}</script><p> &emsp;&emsp;  我们可以根据Sigmoid函数得到：</p>
<script type="math/tex; mode=display">
\sigma'(z)= \frac{-e^{(-z)} }{(1+e^{(-z)})^2} = \sigma(z)(1-\sigma(z)) \\</script><p> &emsp;&emsp;  所以我们最后的结果为：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{split}
\frac{\partial{L(y_i,\hat{y_i})} }{ {\partial w} }&=[(1-y_i)\sigma(z)-y_i(1-\sigma(z))]x \\
&=(\sigma(z)-y_i)x \\
&=(\hat{y}_i-y_i)x \\
\end{split}
\end{equation}</script><p> &emsp;&emsp;  因此，交叉熵损失函数解决了Sigmoid造成的梯度消失问题，加快了参数更新的速度。</p>
<p><br></p>
<h2 id="4、SVM-和-LR-的联系和区别"><a href="#4、SVM-和-LR-的联系和区别" class="headerlink" title="4、SVM 和 LR 的联系和区别"></a>4、SVM 和 LR 的联系和区别</h2><h3 id="相同点"><a href="#相同点" class="headerlink" title="相同点"></a>相同点</h3><ol>
<li>都是监督的分类算法</li>
<li>都是线性分类方法</li>
<li>都是判别模型</li>
</ol>
<h3 id="不同点"><a href="#不同点" class="headerlink" title="不同点"></a>不同点</h3><h4 id="1-两者的本质不同在于其Loss函数不同"><a href="#1-两者的本质不同在于其Loss函数不同" class="headerlink" title="1. 两者的本质不同在于其Loss函数不同"></a>1. 两者的本质不同在于其Loss函数不同</h4><p> &emsp;&emsp;  LR的损失函数时<strong>交叉熵损失函数</strong>：</p>
<script type="math/tex; mode=display">
L_{LR} = -\frac{1}{N}\sum_{i=1}^N(y_ilog\ \hat{y_i} + (1-y_i)log(1-\hat{y_i}))</script><p> &emsp;&emsp;  SVM的损失函数是<strong>hinge Loss:</strong></p>
<script type="math/tex; mode=display">
L_{SVM} = \sum_{i=1}^Nmax(0,1-w^Tx_iy_i)</script><p> &emsp;&emsp;  不同的loss function代表了不同的假设前提，也就代表了不同的分类原理。</p>
<p> &emsp;&emsp;  LR方法基于<strong>概率理论</strong>，SVM基于几何间隔最大化原理。</p>
<p> &emsp;&emsp;  <strong>SVM只考虑分类面上的点，而LR考虑所有点</strong>，<strong>Linear SVM不直接依赖于数据分布</strong>，分类平面不受异常点影响；LR则是受所有数据点的影响，所以受数据本身分布影响的。</p>
<h4 id="2-SVM依赖于数据的测度，而LR则不受影响"><a href="#2-SVM依赖于数据的测度，而LR则不受影响" class="headerlink" title="2. SVM依赖于数据的测度，而LR则不受影响"></a>2. SVM依赖于数据的测度，而LR则不受影响</h4><p> &emsp;&emsp;  因为<strong>SVM是基于距离的</strong>，而<strong>LR是基于概率的</strong>，所以LR是不受数据不同维度测度不同的影响，而SVM因为要最小化$ \frac{1}{2}||w||^2$ 所以其依赖于不同维度测度的不同，如果差别较大需要做normalization。如果不归一化，各维特征的跨度差距很大，目标函数就会是“扁”的，在进行梯度下降的时候，梯度的方向就会偏离最小值的方向，走很多弯路。</p>
<p><br></p>
<h4 id="3-SVM自带结构风险最小化，LR则是经验风险最小化"><a href="#3-SVM自带结构风险最小化，LR则是经验风险最小化" class="headerlink" title="3. SVM自带结构风险最小化，LR则是经验风险最小化"></a>3. SVM自带结构风险最小化，LR则是经验风险最小化</h4><p> &emsp;&emsp;  因为SVM本身就是优化$ \frac{1}{2}||w||^2$ 最小化的，所以其优化的目标函数本身就含有结构风险最小化，所以不需要加正则项<br> &emsp;&emsp;  而LR不加正则化的时候，其优化的目标是经验风险最小化，所以最后需要加入正则化，增强模型的泛化能力。</p>
<h4 id="4-SVM会用核函数而LR一般不用核函数"><a href="#4-SVM会用核函数而LR一般不用核函数" class="headerlink" title="4. SVM会用核函数而LR一般不用核函数"></a>4. SVM会用核函数而LR一般不用核函数</h4><p> &emsp;&emsp;  SVM转化为对偶问题后，分类只需要计算与少数几个支持向量的距离，这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算量。<br> &emsp;&emsp;  而LR则每个点都需要两两计算核函数，计算量太过庞大。</p>
<h4 id="5-LR和SVM在实际应用的区别"><a href="#5-LR和SVM在实际应用的区别" class="headerlink" title="5. LR和SVM在实际应用的区别"></a>5. LR和SVM在实际应用的区别</h4><p> &emsp;&emsp;  根据经验来看，对于小规模数据集，SVM的效果要好于LR，但是大数据中，SVM的计算复杂度受到限制，而LR因为训练简单，可以在线训练，所以经常会被大量采用。</p>
<p><br></p>
<h2 id="5、决策树为什么不需要归一化"><a href="#5、决策树为什么不需要归一化" class="headerlink" title="5、决策树为什么不需要归一化"></a>5、决策树为什么不需要归一化</h2><p> &emsp;&emsp;  因为数值缩放不影响分裂点位置，对树模型的结构不造成影响。</p>
<p><br></p>
<h2 id="6、过拟合的原因与解决方案"><a href="#6、过拟合的原因与解决方案" class="headerlink" title="6、过拟合的原因与解决方案"></a>6、过拟合的原因与解决方案</h2><h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><ol>
<li>训练集的数量级和模型的复杂度不匹配。训练集的数量级要小于模型的复杂度；</li>
<li>训练集和测试集特征分布不一致；</li>
<li>样本里的噪音数据干扰过大，大到模型过分记住了噪音特征，反而忽略了真实的输入输出间的关系；</li>
<li>权值学习迭代次数足够多(Overtraining)，拟合了训练数据中的噪声和训练样例中没有代表性的特征。</li>
</ol>
<h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><ol>
<li>调小模型复杂度，使其适合自己训练集的数量级（缩小宽度和减小深度）</li>
<li>数据增强来扩大数据集</li>
<li>添加正则化项（L1 L2范数）</li>
<li>dropout</li>
<li>early stopping</li>
<li>集成学习</li>
<li>清洗数据，将一些脏数据去除，去除无效值和缺失值</li>
</ol>
<p><br></p>
<h2 id="7、L0、L1、L2范数的区别"><a href="#7、L0、L1、L2范数的区别" class="headerlink" title="7、L0、L1、L2范数的区别"></a>7、L0、L1、L2范数的区别</h2><p> &emsp;&emsp;  <strong>L0范数是指向量中非0的元素的个数</strong>。如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0即让参数W是稀疏的。 </p>
<p> &emsp;&emsp;  L1范数是指向量中各个元素绝对值之和，L1也可以使权重稀疏，是L0范数的最优凸近似</p>
<p> &emsp;&emsp;  L2范数被称为<strong>岭回归</strong>或者<strong>权重衰减</strong>，它使得目标函数变为凸函数，L2范数可以使得权重比较小。一般来说权重比较小意味着模型比较简单，泛化性越强。</p>
<p><br></p>
<h2 id="8、参数稀疏的好处"><a href="#8、参数稀疏的好处" class="headerlink" title="8、参数稀疏的好处"></a>8、参数稀疏的好处</h2><p> <strong>1. 特征选择(Feature Selection)</strong></p>
<p> &emsp;&emsp;  大家对稀疏规则化趋之若鹜的一个关键原因在于它能实现特征的自动选择。一般来说，样本$ x_i$ 的大部分元素（也就是特征）都是和最终的输出$ y_i$ 没有关系或者不提供任何信息的，在最小化目标函数的时候考虑$ x_i$ 这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确$ y_i$ 的预测。稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0。</p>
<p><strong>2. 可解释性(Interpretability)</strong></p>
<p> &emsp;&emsp;  另一个青睐于稀疏的理由是，模型更容易解释。例如患某种病的概率是y，然后我们收集到的数据x是1000维的，也就是我们需要寻找这1000种因素到底是怎么影响患上这种病的概率的。假设我们这个是个回归模型：y=w1<em>x1+w2</em>x2+…+w1000<em>x1000+b（当然了，为了让y限定在[0,1]的范围，一般还得加个Logistic函数）。通过学习，如果最后学习到的w</em>就只有很少的非零元素，例如只有5个非零的wi，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨大的，决策性的。也就是说，患不患这种病只和这5个因素有关，那医生就好分析多了。但如果1000个wi都非0，医生面对这1000种因素，累觉不爱。</p>
<p><strong>参考链接</strong></p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/28850865" target="_blank" rel="noopener">机器学习中样本不平衡的处理方法</a></li>
<li><a href="https://blog.csdn.net/haolexiao/article/details/70191667" target="_blank" rel="noopener">【机器学习】Linear SVM 和 LR 的联系和区别</a></li>
<li><a href="https://blog.csdn.net/NIGHT_SILENT/article/details/80795640" target="_blank" rel="noopener">过拟合（定义、出现的原因4种、解决方案7种）</a></li>
</ul>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">白丁</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://baidinghub.github.io/2020/04/03/AI%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%BA%94)%20%E9%9D%A2%E8%AF%95%E5%B0%8F%E7%9F%A5%E8%AF%86(2)/">http://baidinghub.github.io/2020/04/03/AI%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%BA%94)%20%E9%9D%A2%E8%AF%95%E5%B0%8F%E7%9F%A5%E8%AF%86(2)/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://baidinghub.github.io" target="_blank">BaiDing's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI%E5%B0%8F%E7%9F%A5%E8%AF%86/">AI小知识</a><a class="post-meta__tags" href="/tags/%E9%9D%A2%E8%AF%95%E5%B0%8F%E7%9F%A5%E8%AF%86/">面试小知识</a></div><div class="post_share"><div class="addthis_inline_share_toolbox"></div><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDyIdOn/M/aXcGQOiSDVMqvN2gxDdiESHUfFuB2YMy48fvNN9SZOQUbVlGF4Pk6nDXIAir+br/EWuEnNLtgOCYMo/BTxl29gqS/QGHPiDaIQedzmLcuRZpfDuGit61N/b9pyktpZLagBgbl5Ox9mAgWQxXyhxYB092gyOXqrBULBeZUYQ+H7Eupha10QTQghHv4nLk+oYWo2UXEiijQpE3qMXT32G8v8k0KbRdd1hIFPyNEx6eZ6Buc2ZdbMtoutdGjvdnw5B1+dxgl1egESPChzcwCMxt3NW/3faQ5lsjRhQD4fRU+Ua/aqXe+0xg2+xr0BTjNr/JZIAaNiLuo71lH 1564026260@qq.com" async="async"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/wechat.png?raw=true" alt="微信"/><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/alipay.jpg?raw=true" alt="支付寶"/><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/04/03/%E6%B7%B1%E5%BA%A6%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%B8%80)%20%E4%BC%98%E5%8C%96%E5%99%A8%E4%BB%8B%E7%BB%8D/"><img class="prev_cover lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97/%E6%B7%B1%E5%BA%A6%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%B8%80)%20%E4%BC%98%E5%8C%96%E5%99%A8%E4%BB%8B%E7%BB%8D/cover.gif?raw=true" onerror="onerror=null;src='https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/404.jpg?raw=true'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">深度基础知识系列(一) 优化器介绍</div></div></a></div><div class="next-post pull_right"><a href="/2020/04/03/AI%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E5%9B%9B)%20%20Matplotlib%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/"><img class="next_cover lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/AI%E5%B0%8F%E7%9F%A5%E8%AF%86/AI%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E5%9B%9B)%20%20Matplotlib%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/cover.jpg?raw=true" onerror="onerror=null;src='https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/404.jpg?raw=true'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">AI小知识系列(四)  Matplotlib常用操作</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/04/03/AI小知识系列(一)  面试小知识(1)/" title="AI小知识系列(一) 面试小知识(1)"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/AI%E5%B0%8F%E7%9F%A5%E8%AF%86/AI%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%B8%80)%20%20%E9%9D%A2%E8%AF%95%E5%B0%8F%E7%9F%A5%E8%AF%86(1)/cover.jpg?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">AI小知识系列(一) 面试小知识(1)</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/AI小知识系列(三)  Pandas常用操作/" title="AI小知识系列(三)  Pandas常用操作"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/AI%E5%B0%8F%E7%9F%A5%E8%AF%86/AI%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%B8%89)%20%20Pandas%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/cover.jpg?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">AI小知识系列(三)  Pandas常用操作</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/AI小知识系列(二) 训练过程Trick合集/" title="AI小知识系列(二) 训练过程Trick合集"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/AI%E5%B0%8F%E7%9F%A5%E8%AF%86/AI%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%BA%8C)%20%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8BTrick%E5%90%88%E9%9B%86/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">AI小知识系列(二) 训练过程Trick合集</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/AI小知识系列(四)  Matplotlib常用操作/" title="AI小知识系列(四)  Matplotlib常用操作"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/AI%E5%B0%8F%E7%9F%A5%E8%AF%86/AI%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E5%9B%9B)%20%20Matplotlib%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/cover.jpg?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">AI小知识系列(四)  Matplotlib常用操作</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/03/具体的训练Trick/" title="具体的训练小技巧"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/AI%E5%B0%8F%E7%9F%A5%E8%AF%86/%E5%85%B7%E4%BD%93%E7%9A%84%E8%AE%AD%E7%BB%83Trick/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-03</div><div class="relatedPosts_title">具体的训练小技巧</div></div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: 'd3948be001a91411dfd9',
  clientSecret: 'ebddf2a2a5a039922fb373a8a8c0efcc439bf6ca',
  repo: 'BaiDingHub.github.io',
  owner: 'BaiDingHub',
  admin: ['BaiDingHub'],
  id: md5(decodeURI(location.pathname)),
  language: 'zh-CN',
  perPage: 10,
  distractionFreeMode: false,
  pagerDirection: 'last',
  createIssueManually: false,
  updateCountCallback: commentCount
})
gitalk.render('gitalk-container')

function commentCount(n){
  try {
    document.getElementsByClassName('gitalk-comment-count')[0].innerHTML= n
  } catch (e) {
    return false
  }
}</script></div></article></main><footer id="footer" style="background-image: url(https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/AI%E5%B0%8F%E7%9F%A5%E8%AF%86/AI%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%B3%BB%E5%88%97(%E4%BA%94)%20%20%E9%9D%A2%E8%AF%95%E5%B0%8F%E7%9F%A5%E8%AF%86(2)/cover.jpg?raw=true)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By 白丁</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://jerryc.me/" target="_blank" rel="noopener">blog</a>!</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js" ></script><script src="/js/utils.js" ></script><script src="/js/main.js" ></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><script src="/js/search/algolia.js"></script></body></html>