<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>推荐系统（四）新闻推荐系统综述 | BaiDing's blog</title><meta name="description" content="推荐系统（四）新闻推荐系统综述"><meta name="keywords" content="深度学习,推荐系统"><meta name="author" content="白丁"><meta name="copyright" content="白丁"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="https://raw.githubusercontent.com/BaiDingHub/Blog_images/master/BlogSource/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="crossorigin"/><link rel="preconnect" href="http://ta.qq.com"/><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="x-1ef6P_miWkq-RJn_fmjd3KYumrXANNXYzK1myaLf0"/><meta name="baidu-site-verification" content="guD6l44Chk"/><meta name="twitter:card" content="summary"><meta name="twitter:title" content="推荐系统（四）新闻推荐系统综述"><meta name="twitter:description" content="推荐系统（四）新闻推荐系统综述"><meta name="twitter:image" content="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E5%9B%9B%EF%BC%89%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0/cover.png?raw=true"><meta property="og:type" content="article"><meta property="og:title" content="推荐系统（四）新闻推荐系统综述"><meta property="og:url" content="http://baidinghub.github.io/2020/08/11/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E5%9B%9B%EF%BC%89%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0/"><meta property="og:site_name" content="BaiDing's blog"><meta property="og:description" content="推荐系统（四）新闻推荐系统综述"><meta property="og:image" content="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E5%9B%9B%EF%BC%89%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0/cover.png?raw=true"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>var autoChangeMode = '1'
var t = Cookies.get("theme")
if (autoChangeMode == '1'){
  var isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
  var isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
  var isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined){
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport){
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour < 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
  }
  } else if (t == 'light') activateLightMode()
  else activateDarkMode()

} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css" ><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://baidinghub.github.io/2020/08/11/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E5%9B%9B%EF%BC%89%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0/"><link rel="prev" title="NLP词向量篇（一）Word2Vec" href="http://baidinghub.github.io/2020/09/01/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%B8%80%EF%BC%89Word2Vec/"><link rel="next" title="推荐系统（三）推荐系统综述-2018" href="http://baidinghub.github.io/2020/08/10/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%B8%89%EF%BC%89%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0-2018/"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.js" defer></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-162698439-1', 'auto');
ga('send', 'pageview');
</script><script src="https://tajs.qq.com/stats?sId=66540586" charset="UTF-8"></script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"4JGH9NW4XG","apiKey":"41cdb6d9ec4d21196956524e9c985b36","indexName":"baiding","hits":{"per_page":6},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  highlightShrink: 'false',
  isFontAwesomeV5: false,
  isPhotoFigcaption: false
  
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isSidebar: true  
  }</script><noscript><style>
#page-header {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/avatar.png?raw=true" onerror="onerror=null;src='https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/friend_404.gif?raw=true'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">94</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">60</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">17</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于作者</span></a></div></div></div></div><i class="fa fa-arrow-right on" id="toggle-sidebar" aria-hidden="true">     </i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#MIND-A-Large-scale-Dataset-for-News-Recommendation"><span class="toc-text">MIND: A Large-scale Dataset for News Recommendation</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#论文内容总结"><span class="toc-text">论文内容总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1、Introduction"><span class="toc-text">1、Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2、Related-Work"><span class="toc-text">2、Related Work</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-News-Recommendation"><span class="toc-text">2.1  News Recommendation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Existing-Datasets"><span class="toc-text">2.2 Existing Datasets</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3、MIND-Dataset"><span class="toc-text">3、MIND Dataset</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-Dataset-Construction"><span class="toc-text">3.1 Dataset Construction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Dataset-Analysis"><span class="toc-text">3.2 Dataset Analysis</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4、Method"><span class="toc-text">4、Method</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-General-Recommendation-Methods"><span class="toc-text">4.1 General Recommendation Methods</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-1-LibFM"><span class="toc-text">4.1.1 LibFM</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-2-DSSM"><span class="toc-text">4.1.2 DSSM</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-3-Wide-amp-Deep"><span class="toc-text">4.1.3 Wide&amp;Deep</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-4-DeepFM"><span class="toc-text">4.1.4 DeepFM</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-News-Recommendation-Methods"><span class="toc-text">4.2 News Recommendation Methods</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-1-DFM"><span class="toc-text">4.2.1 DFM</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-2-GRU"><span class="toc-text">4.2.2 GRU</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-3-DKN"><span class="toc-text">4.2.3 DKN</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-4-NPA"><span class="toc-text">4.2.4 NPA</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-5-NAML"><span class="toc-text">4.2.5 NAML</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-6-LSTUR"><span class="toc-text">4.2.6 LSTUR</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-7-NRMS"><span class="toc-text">4.2.7 NRMS</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5、Experiments"><span class="toc-text">5、Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-Experimental-Settings"><span class="toc-text">5.1 Experimental Settings</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-Performance-Comparison"><span class="toc-text">5.2 Performance Comparison</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-News-Content-Understanding"><span class="toc-text">5.3 News Content Understanding</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-3-1-News-Representation-Model"><span class="toc-text">5.3.1 News Representation Model</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-3-2-Pre-trained-Language-Models"><span class="toc-text">5.3.2 Pre-trained Language Models</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-3-Different-News-Information"><span class="toc-text">5.3.3 Different News Information</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-4-User-Interest-Modeling"><span class="toc-text">5.4 User Interest Modeling</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6、Conclusion-and-Discussion"><span class="toc-text">6、Conclusion and Discussion</span></a></li></ol></li></ol></div></div></div><div id="body-wrap"><div class="post-bg" id="nav" style="background-image: url(https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E5%9B%9B%EF%BC%89%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0/cover.png?raw=true)"><div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">BaiDing's blog</a></span><span class="pull_right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于作者</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span></span></div><div id="post-info"><div id="post-title"><div class="posttitle">推荐系统（四）新闻推荐系统综述</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2020-08-11 09:50:00"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2020-08-11</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-12-02 11:11:09"><i class="fa fa-history" aria-hidden="true"></i> 更新于 2020-12-02</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><i class="fa fa-angle-right post-meta__separator" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">推荐系统</a></span></div><div class="meta-secondline"> <span class="post-meta-wordcount"><i class="post-meta__icon fa fa-file-word-o" aria-hidden="true"></i><span>字数总计:</span><span class="word-count">7.6k</span><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-clock-o" aria-hidden="true"></i><span>阅读时长: 24 分钟</span></span></div><div class="meta-thirdline"><span class="post-meta-pv-cv"><span class="post-meta__separator">|</span><i class="fa fa-eye post-meta__icon" aria-hidden="true"> </i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-comment-o" aria-hidden="true"></i><span>评论数:</span><a href="/2020/08/11/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E5%9B%9B%EF%BC%89%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0/#post-comment"><span class="gitalk-comment-count comment-count"></span></a></span></div></div></div></div><main class="layout_post" id="content-inner"><article id="post"><div id="article-container"><meta name="referrer" content="no-referrer"/>

<h1 id="MIND-A-Large-scale-Dataset-for-News-Recommendation"><a href="#MIND-A-Large-scale-Dataset-for-News-Recommendation" class="headerlink" title="MIND: A Large-scale Dataset for News Recommendation"></a>MIND: A Large-scale Dataset for News Recommendation</h1><blockquote>
<p>时间：2020年</p>
<p>关键词：News Recommender systems · Large-scale Dataset · Survey  </p>
<p>论文位置：<a href="https://www.aclweb.org/anthology/2020.acl-main.331.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/2020.acl-main.331.pdf</a></p>
<p>引用：Wu F, Qiao Y, Chen J H, et al. MIND: A Large-scale Dataset for News Recommendation[C]//Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020: 3597-3606.</p>
</blockquote>
<p><strong>摘要：</strong>新闻推荐是实现个性化新闻服务的一项重要技术。与已经被广泛研究的产品推荐和电影推荐相比，新闻推荐的研究非常有限，主要是因为缺乏高质量的基准数据集。在本文中，我们提出了一个名为MIND的新闻推荐大型数据集。MIND由微软新闻的用户点击日志构建而成，<strong>包含100万用户和16万多篇英文新闻文章</strong>，每篇文章都有丰富的标题、摘要、正文等文本内容。通过对几种最先进的新闻推荐方法的比较研究，我们证明了MIND为新闻推荐提供了一个良好的实验平台，这些方法最初是在不同的专有数据集上开发的。研究结果表明，新闻推荐的性能在很大程度上依赖于新闻内容理解和用户兴趣建模的质量。有效的文本表示方法和预先训练好的语言模型等自然语言处理技术可以有效地提高新闻推荐的性能。MIND数据集可以在<a href="https://msnews.github.io下载。" target="_blank" rel="noopener">https://msnews.github.io下载。</a></p>
<h2 id="论文内容总结"><a href="#论文内容总结" class="headerlink" title="论文内容总结"></a>论文内容总结</h2><ul>
<li>提出了一个名为<strong>MIND的新闻推荐大型数据集</strong>。MIND由微软新闻的用户点击日志构建而成，<strong>包含100万用户和16万多篇英文新闻文章</strong>，每篇文章都有丰富的标题、摘要、正文等文本内容。MIND数据集的具体介绍请看Sec.3</li>
<li><strong>比较了传统的推荐方法LibFM、DSSM、Wide&amp;Deep、DeepFM以及深度推荐方法DFM、GRU、NPA、NAML、LSTUR、NRMS在该大型数据集上的性能（评价标准采用AUC、MRR、nDCG）</strong>，实验结果表明NRMS性能最好。发现了，采用AUC评价标准得到的网络往往对不可见的用户（数据集中不存在的用户）预测效果比可见的用户差，但MRR和nDCG就没有这种情况。Sec.5.1</li>
<li><strong>比较了不同的news representation model（LDA、TF-IDF、 average of word embedding (Avg-Emb), CNN, LSTM and multi-head self-attention (Self-Att)）的性能。</strong>实验结果表明，CNN、LSTM、Self-Att这类深度模型比传统模型的效果好，同时，Self-Att和LSTM在新闻表现上优于CNN。Sec.5.3.1</li>
<li><strong>研究了不同种类的新闻信息对模型产生的影响</strong>，实验表明，整合多种新闻信息（title、abstract、body）可以提高模型的性能，同时采用多视角学习（AMV）来整合这些信息能进一步的提升性能，这是因为，这是因为不同的新闻文本通常都有不同的特点，所以使用不同的神经网络学习他们的representaion，使用不同的注意力机制为不同的分布建模会更好。Sec.5.3.3</li>
<li><strong>研究了六种不同的用户兴趣建模方法</strong>，average of the representations of previously clicked news (Average)、attention mechanism（Attention）、candidate-aware attention(Candidate-Att)、gated recurrent unit(GRU)、 long- and short-term user representation  (LSTUR)和multi-head self-attention(Self-Att)。实验结果表明，Attention,  Candidate-Att和GRU的表现都好于平均水平，而LSTUR的效果最好。同时实验表明，当用户的新闻历史点击数目越多的时候，性能越好。</li>
</ul>
<h2 id="1、Introduction"><a href="#1、Introduction" class="headerlink" title="1、Introduction"></a>1、Introduction</h2><p> &emsp;&emsp; Google news、Microsoft  news等在线新闻服务已经成为广大用户获取新闻信息的重要平台(Das et al.，2007; Wu et al.，2019a)。每天都有大量的新闻生成并发布在网上，使得用户很难快速找到感兴趣的新闻(Okura  et al.， 2017)。个性化新闻推荐可以帮助用户缓解信息过载，提高新闻阅读体验(Wu et al.，  2019b)。因此，它被许多网络新闻平台广泛使用(Li et al.， 2011; Okura et al.，2017年; An et al.，2019年)。</p>
<p> &emsp;&emsp; 在传统的推荐系统中，用户和项目通常使用ID表示，他们之间的交互，如评级分数，通过协同过滤等方法来学习ID表示(Koren,  2008)。然而，新闻推荐也面临着一些特殊的挑战。首先，新闻网站上的新闻文章更新非常快。新的新闻文章不断发布，现有的新闻文章会在短时间内过期(Das等，2007)。因此，<strong>新闻推荐中的冷启动问题非常严重</strong>。其次，<strong>新闻文章包含标题、正文等丰富的文本信息。简单地用id表示他们是不合适的，从他们的文本中理解他们的内容是很重要的</strong>(Kompan和Bielikov’a,  2010)。第三，<strong>对用户在新闻平台上发布的新闻文章没有明确的评分。因此，在新闻推荐中，用户对新闻的兴趣通常是从用户的点击行为中含蓄地推断出来的(Ilievski和Roy,  2013)</strong>。</p>
<p> &emsp;&emsp; 大规模和高质量的数据集可以显著促进某一领域的研究，例如ImageNet用于图像分类(Deng et al.， 2009)和机器阅读理解SQuAD(Rajpurkar  et al.， 2016)。有几个公共数据集用于传统的推荐任务，比如Amazon dataset用于产品推荐，MovieLens  dataset用于电影推荐。基于这些数据集，许多著名的推荐方法被开发出来。然而，现有的关于新闻推荐的研究较少，很多都是在专有数据集上进行的(Okura et  al.， 2017;Wang et al.，  2018;吴等，2019a)。虽然有一些公共的新闻推荐数据集，但这些数据集通常规模较小，而且大部分都不是英文的。因此,公共的大型英文新闻推荐数据集对这一领域的研究具有重要的价值。</p>
<p> &emsp;&emsp; 在本文中，我们提出了一个大规模的微软新闻数据集(MIND)用于新闻推荐研究，该数据集是从Microsoft News的用户行为日志中收集的。它包含了100万用户以及他们点击超过16万篇英语新闻的行为。我们在不同的专有数据集上实现了许多最新的新闻推荐方法，并比较它们在思维数据集上的表现，为新闻推荐研究提供基准。实验结果表明，利用自然语言处理技术对新闻文章进行深入理解对新闻推荐具有重要意义。有效的文本表示方法和预先训练好的语言模型都有助于提高新闻推荐的性能。此外，适当地对用户兴趣进行建模也很有用。我们希望MIND可以作为新闻推荐的基准数据集，促进这一领域的研究。</p>
<h2 id="2、Related-Work"><a href="#2、Related-Work" class="headerlink" title="2、Related Work"></a>2、Related Work</h2><h3 id="2-1-News-Recommendation"><a href="#2-1-News-Recommendation" class="headerlink" title="2.1  News Recommendation"></a>2.1  News Recommendation</h3><p> &emsp;&emsp; <strong>新闻推荐旨在从大量候选新闻中寻找用户感兴趣的新闻文章</strong>(Das et al.，2007)。新闻推荐中存在两个重要的问题，即<strong>如何表示文本内容丰富的新闻文章</strong>，以及<strong>如何从用户之前的行为中建模用户对新闻的兴趣</strong>(Okura  et al.， 2017)。传统的新闻推荐方法通常依靠特征工程来代表新闻文章和用户兴趣。例如，Li等人(2010)使用url和类别来表示新闻文章，使用人口统计数据、地理信息和从他们在雅虎上的消费记录推断出的行为类别来表示用户。</p>
<p> &emsp;&emsp; 近年来，一些基于深度学习的新闻推荐方法被提出，以<strong>端到端方式学习新闻文章的表示和用户兴趣</strong>(Okura et al.，  2017;Wu et al. ，2019a ; An et al.，2019年)。例如，Okura等人(2017)<strong>使用去噪自动编码器模型表示来自新闻内容的新闻文章</strong>，<strong>使用GRU模型表示来自历史点击新闻文章的用户兴趣</strong>。他们在日本雅虎平台的实验显示，通过深度学习模型学习到的新闻和用户表示很有希望用于新闻推荐。Wang  et al.(2018)提出通过合并从知识图中推导出的词向量和实体向量，<strong>利用CNN网络从新闻标题中学习知识感知的新闻表示</strong>。Wu et al.  (2019a)提出了一种<strong>专注的多视角学习框架来表示标题、正文、类别等不同文本的新闻文章</strong>。他们使用了一个<strong>注意力模型</strong>，通过选择信息丰富的文章，从用户点击的新闻文章中推断出用户的兴趣。这些工作通常开发和验证的专有数据集，这是不可公开的，使它为其他研究人员验证这些方法和发展自己的方法困难。</p>
<p> &emsp;&emsp; 新闻推荐与自然语言处理有着丰富的内在关联。首先，新闻是一种常见的文本形式，文本建模技术CNN和Transformer可以很自然地被用来表示新闻文章(Wu et al.，  2019a;Ge et al.，2020)。其次，从之前点击的新闻文章中学习用户兴趣表示，与从句子中学习文档表示有相似之处。第三，新闻推荐可以表述为一个特殊的文本匹配问题，即在某个新闻阅读兴趣空间中，一篇候选新闻文章与一组之前点击过的新闻文章之间的匹配。因此，新闻推荐越来越受到NLP社区的关注(An  et al.， 2019;Wu et al.，2019c)。</p>
<h3 id="2-2-Existing-Datasets"><a href="#2-2-Existing-Datasets" class="headerlink" title="2.2 Existing Datasets"></a>2.2 Existing Datasets</h3><p> &emsp;&emsp; 新闻推荐的公共数据集很少，总结见表1。Kille等人(2013)通过收集在13个德国新闻门户网站上发表的新闻文章以及用户对其的点击日志，构建了Plista数据集。它包含70,353篇新闻文章和1,095,323次点击事件。该数据集中的新闻文章是德语的，用户主要来自德语世界。Gulla等人(2017)在十周内发布了Adressa数据集，该数据来源于Adresseavisen网站的日志。它拥有48,486篇新闻文章，3,083,438个用户和27,223,576个点击事件。每个单击事件包含几个特性，如会话时间、新闻标题、新闻类别和用户ID。每个新闻文章都与一些详细信息相关联，如作者、实体和主体。这个数据集中的新闻文章是挪威语的。Moreira等人(2018)从巴西热门新闻门户Globo.com构建了一个新闻推荐数据库。这个数据集包含大约314,000个用户，46,000篇新闻文章和300万次点击记录。每个单击记录都包含用户ID、新闻ID和会话时间等字段。每个新闻文章都有ID、分类、发布者、创建时间以及由一个在news metadata分类任务中预训练的神经网络得到的词向量。</p>
<h2 id="3、MIND-Dataset"><a href="#3、MIND-Dataset" class="headerlink" title="3、MIND Dataset"></a>3、MIND Dataset</h2><h3 id="3-1-Dataset-Construction"><a href="#3-1-Dataset-Construction" class="headerlink" title="3.1 Dataset Construction"></a>3.1 Dataset Construction</h3><p> &emsp;&emsp; 为了便于新闻推荐的研究，我们构建了MIcrosoft news Dataset (MIND)。它是从Microsoft  News的用户行为日志中收集的。我们随机抽取了100万用户，他们在2019年10月12日至11月22日的6周内至少有5条新闻点击记录。为了保护用户隐私，使用one-time salt mapping.将每个用户安全地散列到一个匿名ID中，然后从生产系统解除链接。我们收集了这些用户在此期间的行为日志（behaviorlogs ），并将其格式化为印象日志（impression logs）。impression logs记录用户在特定时间访问新闻网站主页时显示给用户的新闻文章，以及用户对这些新闻文章的点击行为。由于在新闻推荐中，我们通常根据用户之前的行为推断出的个人兴趣来预测用户是否会点击候选新闻文章，因此我们将用户的新闻点击历史记录添加到他们的印象日志中，构造带标签的样本（labeled sample），用于训练和验证新闻推荐模型。<strong>每个带标签的示例的格式是[uID, t, ClickHist, ImpLog]</strong>，其中<strong>uID是用户的匿名ID</strong>,  <strong>t是这个impression的时间戳</strong>。<strong>ClickHist是该用户先前单击的新闻文章的ID列表(按单击时间排序)</strong>。<strong>ImpLog包含显示在此impression中的新闻文章的id和指示是否单击它们的标签，即[(nID1,  label1)，(nID2, label2)，…]</strong>，其中<strong>nID为新闻文章ID, label为单击标签(1表示单击，0表示未单击)</strong>。我们使用上周的样本进行测试，使用第五周的样本进行训练。对于训练集中的样本，我们使用前四周的点击行为来构建新闻的点击历史。对于测试集中的示例，提取新闻点击历史记录的时间段是前五周。我们只保留了带有非空新闻点击历史记录的示例。在训练数据中，我们使用第五周最后一天的样本作为验证集。</p>
<p> &emsp;&emsp; MIND数据集中的<strong>每一篇新闻文章都包含一个新闻ID、一个title、一个abstract、一个body和一个category label</strong>，比如由编辑手动标记的“Sports”。此外，我们发现这些新闻文本包含丰富的实体。例如，在图1所示的新闻标题中，“Mike  Tomlin: Steelers ‘ accept responsibility ‘ For role in brawl with Browns”，“Mike  Tomlin”是一个person  entity，“Steelers”和“Browns”是美式足球队的entity。为了便于知识感知型新闻推荐的研究，<strong>我们将新闻文章的title、abstract和body中的实体提取到MIND数据集中</strong>，并使用内部的NER和实体链接工具链接到WikiData中的实体。我们还从WikiData中提取了这些实体的知识三元组，并使用TransE  (Bordes et al.， 2013)方法学习实体和关系的embeddings。<strong>这些实体、知识三元组以及实体和关系embeddings也包含在MIND数据集中。</strong></p>
<h3 id="3-2-Dataset-Analysis"><a href="#3-2-Dataset-Analysis" class="headerlink" title="3.2 Dataset Analysis"></a>3.2 Dataset Analysis</h3><p> &emsp;&emsp; MIND数据集的详细统计如表2和图2所示。该数据集包含1,000,000个用户和161,013篇新闻文章。在训练集中有2186683个样本，验证集中有365200个样本，测试集有2341619个样本，这样可以增强数据密集型新闻推荐模型的训练。图二展示了新闻title、abstract、body的长度分布。我们可以看到，新闻标题通常很短，平均长度只有11.52个字。相比之下，新闻摘要和正文要长得多，可以包含更丰富的新闻内容信息。因此，整合标题、摘要、正文等不同类型的新闻信息有助于更好地理解新闻文章。</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E5%9B%9B%EF%BC%89%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0/1.png?raw=true"  alt="1"></p>
<p> &emsp;&emsp; 图2(d)显示了新闻文章的生存时间分布。这里使用新闻文章在数据集中首次出现和最后一次出现之间的时间间隔估计新闻文章的生存时间。我们发现，超过84.5%的新闻文章存活时间不到两天。这是由于新闻信息的性质，新闻媒体总是追求最新的新闻，现有的新闻文章很快就会过时。因此，冷启动问题是新闻推荐中常见的现象，传统的基于id的推荐系统(Koren,  2008)并不适合这项任务。使用文本内容表示新闻文章对新闻推荐至关重要。</p>
<h2 id="4、Method"><a href="#4、Method" class="headerlink" title="4、Method"></a>4、Method</h2><p> &emsp;&emsp; 在这一部分中，我们简要介绍了几种新闻推荐的方法，包括一般的推荐方法和针对新闻的推荐方法。这些方法是在不同的设置和不同的数据集上开发的。它们的一些实现可以在Microsoft  recommendations开源仓库中找到。我们会在MIND数据库里对它们进行比较。</p>
<h3 id="4-1-General-Recommendation-Methods"><a href="#4-1-General-Recommendation-Methods" class="headerlink" title="4.1 General Recommendation Methods"></a>4.1 General Recommendation Methods</h3><h4 id="4-1-1-LibFM"><a href="#4-1-1-LibFM" class="headerlink" title="4.1.1 LibFM"></a>4.1.1 LibFM</h4><p> &emsp;&emsp; 2012年，Rendle发表。LibFM是一个基于因子分解机（factorization machine）的经典推荐方法。除了用户ID和新闻ID之外，我们还使用从先前点击的新闻和候选新闻中提取的内容特性作为附加特性来表示用户和候选新闻。</p>
<h4 id="4-1-2-DSSM"><a href="#4-1-2-DSSM" class="headerlink" title="4.1.2 DSSM"></a>4.1.2 DSSM</h4><p> &emsp;&emsp; 2013年，Huang等人发表。 深度结构化语义模型（ deep structured semantic model），采用三格哈希（ tri-gram hashes）和多前馈神经网络进行查询文档匹配。我们使用从先前点击的新闻中提取的内容特性作为查询，从候选新闻中提取的内容特性作为文档。</p>
<h4 id="4-1-3-Wide-amp-Deep"><a href="#4-1-3-Wide-amp-Deep" class="headerlink" title="4.1.3 Wide&amp;Deep"></a>4.1.3 Wide&amp;Deep</h4><p> &emsp;&emsp; 2016年，Cheng等人发表。提出了一种宽线性变换通道和深神经网络通道（wide linear transformation channel）的双通道神经网络推荐方法。我们为两个channel使用相同的用户和候选新闻内容特性。</p>
<h4 id="4-1-4-DeepFM"><a href="#4-1-4-DeepFM" class="headerlink" title="4.1.4 DeepFM"></a>4.1.4 DeepFM</h4><p> &emsp;&emsp; 2017年，Guo等人发表。另一种流行的神经推荐方法是将深度神经网络和分解机结合起来。为两个组件提供相同的用户和候选新闻内容特性。</p>
<h3 id="4-2-News-Recommendation-Methods"><a href="#4-2-News-Recommendation-Methods" class="headerlink" title="4.2 News Recommendation Methods"></a>4.2 News Recommendation Methods</h3><h4 id="4-2-1-DFM"><a href="#4-2-1-DFM" class="headerlink" title="4.2.1 DFM"></a>4.2.1 DFM</h4><p> &emsp;&emsp; 2018年，Lian等人发表。深度融合模型(deep fusion  model)是一种新闻推荐方法，它采用inception网络将不同深度的神经网络结合起来，捕捉特征之间复杂的交互作用。我们使用了与上述方法相同的用户和候选新闻特性。</p>
<h4 id="4-2-2-GRU"><a href="#4-2-2-GRU" class="headerlink" title="4.2.2 GRU"></a>4.2.2 GRU</h4><p> &emsp;&emsp; 2017年，Okura等人发表。一种神经新闻推荐方法，利用自动编码器从新闻内容中学习潜在的新闻表示，并利用GRU网络从点击的新闻序列中学习用户表示。</p>
<h4 id="4-2-3-DKN"><a href="#4-2-3-DKN" class="headerlink" title="4.2.3 DKN"></a>4.2.3 DKN</h4><p> &emsp;&emsp; 2018年，Wang等人发表。一种知识感知（ knowledge-aware ）的新闻推荐方法。使用CNN从包含word embedding和entity embedding的新闻标题中提取新闻的representaion(从知识图中推断)，并根据候选新闻与之前点击的新闻之间的相似性来学习用户representaion。</p>
<h4 id="4-2-4-NPA"><a href="#4-2-4-NPA" class="headerlink" title="4.2.4 NPA"></a>4.2.4 NPA</h4><p> &emsp;&emsp; 2019年，Wu等人发表。一种具有个性化注意机制的神经新闻推荐方法，根据用户偏好选择重要词汇和新闻文章，以获取更多信息的新闻和用户表示。</p>
<h4 id="4-2-5-NAML"><a href="#4-2-5-NAML" class="headerlink" title="4.2.5 NAML"></a>4.2.5 NAML</h4><p> &emsp;&emsp; 2019年，Wu等人发表。一种专注多视角学习的神经新闻推荐方法，将不同种类的新闻信息合并到新闻文章的表示中。</p>
<h4 id="4-2-6-LSTUR"><a href="#4-2-6-LSTUR" class="headerlink" title="4.2.6 LSTUR"></a>4.2.6 LSTUR</h4><p> &emsp;&emsp; 2019年，An等人发表。一种具有长期和短期用户兴趣的神经新闻推荐方法。它利用GRU从用户最近点击的新闻中塑造短期用户兴趣，从整个点击历史中塑造长期用户兴趣。</p>
<h4 id="4-2-7-NRMS"><a href="#4-2-7-NRMS" class="headerlink" title="4.2.7 NRMS"></a>4.2.7 NRMS</h4><p> &emsp;&emsp; 2019年，Wu等人发表。一种神经新闻推荐方法，利用多头自注意（multi-head self-attention）从新闻文本中的单词中学习新闻表示，从已点击的新闻文章中学习用户表示。</p>
<h2 id="5、Experiments"><a href="#5、Experiments" class="headerlink" title="5、Experiments"></a>5、Experiments</h2><h3 id="5-1-Experimental-Settings"><a href="#5-1-Experimental-Settings" class="headerlink" title="5.1 Experimental Settings"></a>5.1 Experimental Settings</h3><p> &emsp;&emsp; 在我们的实验中，我们验证和比较了第4节中介绍的方法。由于这些新闻推荐方法大部分是基于新闻标题的，为了公平比较，我们在实验中只使用了新闻标题，除非另有说明。我们将探讨不同新闻文本的有用性，如5.3.3节中的正文。为了模拟实际的新闻推荐场景，训练数据中总是有看不见的用户，我们随机抽取一半的用户进行训练，并使用所有用户进行测试。对于那些需要单词嵌入的方法，我们使用了Glove  (Pennington et al.， 2014)作为初始化。Adam被用作优化器。由于在每个impression日志中，未被点击的新闻通常比被点击的新闻多得多，下面(Wu  et al.，  2019b)我们将负抽样技术应用于模型训练。根据验证集上的结果选择所有超参数，我们实验中使用的度量标准为AUC、MRR、nDCG@5和nDCG@10，这是推荐结果评价的标准度量标准。每个实验重复10次。</p>
<h3 id="5-2-Performance-Comparison"><a href="#5-2-Performance-Comparison" class="headerlink" title="5.2 Performance Comparison"></a>5.2 Performance Comparison</h3><p> &emsp;&emsp;  不同方法对MIND数据集的实验结果如表3所示。我们从这些结果中得出了几个结论。</p>
<p> &emsp;&emsp; 首先，<strong>针对特定新闻的推荐方法，如NAML、LSTUR和NRMS，通常比一般的推荐方法，如Wide&amp;Deep、LibFM和DeepFM表现得更好。</strong>这是因为在这些特定于新闻的推荐方法中，新闻文章和用户兴趣的表示是以端到端的方式学习的，而在一般的推荐方法中，它们通常使用手工提取的特性来表示。这一结果验证了使用神经网络从原始数据中学习新闻内容和用户兴趣的表示比特征工程更有效。唯一的例外是DFM，它是为新闻推荐而设计的，但不能超过一些通用的推荐方法，如DSSM。这是因为在DFM中，新闻和用户的特性也是手动设计。</p>
<p> &emsp;&emsp; 其次，<strong>在神经新闻推荐方法中，NRMS的性能最好。</strong>NRMS使用multi-head self-attention捕捉单词之间的相关性，以了解新闻表示，并捕捉之前点击的新闻文章之间的交互，以了解用户表示。这一结果表明，先进的自然语言处理模型如multi-head self-attention模型能有效地提高对新闻内容的理解和对用户兴趣的建模。LSTUR的性能也很强大。LSTUR可以通过GRU网络模拟用户最近点击新闻的短期兴趣，也可以通过整个新闻点击历史模拟用户的长期兴趣。结果表明，适当的用户兴趣建模对于新闻推荐也很重要。</p>
<p> &emsp;&emsp; 第三，<strong>从AUC标准来说，在测试时对不可见用户的测试效果比重叠用户的测试效果差。然而，两种用户在MRR和nDCG指标上的性能没有显著差异。</strong>这一结果表明，通过对用户先前点击的新闻内容的推断，可以有效地将由部分用户训练的新闻推荐模型应用于剩余用户和未来的新用户。</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E5%9B%9B%EF%BC%89%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0/2.png?raw=true"  alt="2"></p>
<h3 id="5-3-News-Content-Understanding"><a href="#5-3-News-Content-Understanding" class="headerlink" title="5.3 News Content Understanding"></a>5.3 News Content Understanding</h3><p> &emsp;&emsp; 接下来，我们将探讨如何从文本内容中学习准确的新闻representation。由于MIND数据集非常大，我们从训练集和测试集中随机采样了100k个样本，用于本节和后面几节的实验。</p>
<h4 id="5-3-1-News-Representation-Model"><a href="#5-3-1-News-Representation-Model" class="headerlink" title="5.3.1 News Representation Model"></a>5.3.1 News Representation Model</h4><p> &emsp;&emsp; 首先，我们比较了学习新闻representation的不同文本representation方法。我们选取了三种性能较好的新闻推荐方法，即NAML、LSTUR和NRMS，并将其原有的新闻representation模块替换为不同的文本表示方法，如LDA、TF-IDF、 average of word embedding (Avg-Emb), CNN, LSTM and multi-head self-attention (Self-Att)。由于注意机制是NLP中的一项重要技术(Yang et al.， 2016)，我们也将其应用于上述神经文本representation方法中。结果见表4。</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E5%9B%9B%EF%BC%89%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0/3.png?raw=true"  alt="3"></p>
<p> &emsp;&emsp; 我们从这些结果中得出了几个结论。首先，<strong>神经文本表示方法如CNN、Self-Att和LSTM的性能优于传统文本表示方法如TF-IDF和LDA</strong>。这是因为神经文本表示模型可以通过新闻推荐任务学习，它们可以捕捉文本的上下文来生成更好的新闻表示。其次，<strong>Self-Att和LSTM在新闻表现上优于CNN</strong>。这是因为<strong>multi-head  self-attention和LSTM可以捕捉单词的远距离上下文，而CNN只能模拟局部上下文</strong>。第三，<strong>注意机制可以有效提高CNN和LSTM等不同神经文本表示方法的新闻推荐性能</strong>。这表明，在新闻文本中使用注意力选择重要词汇有助于学习信息更丰富的新闻表征。另一个有趣的发现是，<strong>LSTM和attention的结合可以获得最佳性能</strong>。但是，据我们所知，现有的新闻推荐方法中并没有用到它。</p>
<h4 id="5-3-2-Pre-trained-Language-Models"><a href="#5-3-2-Pre-trained-Language-Models" class="headerlink" title="5.3.2 Pre-trained Language Models"></a>5.3.2 Pre-trained Language Models</h4><p> &emsp;&emsp; 接下来，我们探讨了BERT (Devlin et al.，  2019)等在不同的NLP任务中取得巨大成功的预训练语言模型能否进一步提高新闻表征的质量。我们将BERT应用于三种最新新闻推荐方法的新闻representation模块，即NAML、LSTUR和NRMS。结果如图3所示，我们发现<strong>用BERT模型替换原有的词向量模块，可以提升新闻推荐系统的性能</strong>。</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E5%9B%9B%EF%BC%89%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0/4.png?raw=true"  alt="4"></p>
<p> &emsp;&emsp; 在大规模语料库(如维基百科)上预训练的BERT模型可以为新闻的representation提供有用的语义信息。我们还发现，利用新闻推荐任务对预先训练好的BERT模型进行微调可以进一步提高性能。这些结果验证了预训练语言模型对理解新闻文章有很大的帮助。</p>
<h3 id="5-3-3-Different-News-Information"><a href="#5-3-3-Different-News-Information" class="headerlink" title="5.3.3 Different News Information"></a>5.3.3 Different News Information</h3><p> &emsp;&emsp; 接下来，我们探究是否可以通过整合更多的新闻信息，比如abstract和body，来学习更好的新闻表征。本文尝试了两种新闻文本组合方法。第一个是直接连接(记为Con)，其中我们将不同的新闻文本组合成一个长文档。第二种是专注多视角学习(AMV)  (Wu et al.，  2019a)，它对各种新闻文本独立建模，并将它们与注意网络结合。结果如表5所示。我们发现<strong>新闻主体比新闻标题和摘要更能有效地表达新闻</strong>。这是因为新闻主体更长，<strong>包含更丰富的新闻内容信息。整合标题、正文、摘要等不同类型的新闻文本可以有效提高新闻推荐的性能</strong>，说明不同的新闻文本包含了对新闻表示的补充信息。<strong>在新闻文本中加入类别标签（category）和实体（entity）可以进一步提高性能</strong>。这是因为类别标签可以提供一般的主题信息，而实体则是理解新闻内容的关键字。另一个发现是<strong>注意多视角学习方法在合并不同的新闻文本时优于直接的文本组合</strong>。<strong>这是因为不同的新闻文本通常都有不同的特点，所以使用不同的神经网络学习他们的representaion，使用不同的注意力机制为不同的分布建模会更好</strong></p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E5%9B%9B%EF%BC%89%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0/5.png?raw=true"  alt="5"></p>
<h4 id="5-4-User-Interest-Modeling"><a href="#5-4-User-Interest-Modeling" class="headerlink" title="5.4 User Interest Modeling"></a>5.4 User Interest Modeling</h4><p> &emsp;&emsp; <strong>大多数最新的新闻推荐方法都是从用户之前点击的新闻文章中推断出用户对新闻的兴趣</strong>(Wu et al.，  2019c;An等人，2019年)。在本节中，我们将研究<strong>不同的用户兴趣建模方法</strong>的有效性。我们比较六种方法,包括<strong>average of the representations of previously clicked news (Average)、attention mechanism（Attention）、candidate-aware attention(Candidate-Att)、gated recurrent unit(GRU)、 long- and short-term user representation  (LSTUR)和multi-head self-attention(Self-Att)</strong>。为了公平比较，这些方法中的新闻表示都是使用LSTM生成的。结果如表6所示。我们发现<strong>Attention,  Candidate-Att和GRU的表现都好于平均水平</strong>。这是因为<strong>Attention可以选择信息行为来形成用户表示，Candidate-Att可以结合候选新闻信息来选择信息行为，GRU可以捕捉行为的顺序信息</strong>。<strong>LSTUR的性能优于上述所有方法，因为它可以使用不同时间范围内的行为对长期和短期用户兴趣进行建模。Self-Att也可以实现较强的性能，因为它可以建模用户历史行为之间的长期关联，从而更好地进行用户建模。</strong>我们还研究了点击历史长度对用户兴趣建模的影响。在图4中，我们展示了三种新闻推荐方法，即LSTUR、NAML和NRMS对不同新闻点击历史长度的用户的性能。我们发现，<strong>当用户有更多的新闻点击记录时，他们的表现总体上有所改善。这个结果很直观，因为更多的新闻点击记录可以为用户兴趣建模提供更多的线索</strong>。研究结果还表明，<strong>对新闻平台上行为稀缺的用户，即冷启动用户的兴趣进行推断具有一定的挑战性。</strong></p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E5%9B%9B%EF%BC%89%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0/6.png?raw=true"  alt="6"></p>
<h2 id="6、Conclusion-and-Discussion"><a href="#6、Conclusion-and-Discussion" class="headerlink" title="6、Conclusion and Discussion"></a>6、Conclusion and Discussion</h2><p> &emsp;&emsp; 在本文中，我们提出了一个用于新闻推荐研究的MIND数据集，它是由微软新闻的用户行为日志构建的。拥有100万用户，拥有标题、摘要、正文等丰富文本内容的英文新闻文章16万余篇。我们在这个数据集上进行了广泛的实验。结果表明，准确的新闻内容理解和用户兴趣建模对新闻推荐的重要性。许多自然语言处理和机器学习技术，如文本建模、注意机制和预训练的语言模型都有助于提高新闻推荐的性能。在未来，我们计划扩展MIND，将图像和视频信息融入新闻以及不同语言的新闻中，支持多模式、多语言新闻推荐的研究。此外，除了点击行为之外，我们计划结合其他用户行为，如阅读和参与，以支持更准确的用户建模和性能评估。在MIND  dataset上可以进行很多有趣的研究，比如设计更好的新闻和用户建模方法，提高新闻推荐结果的多样性、公平性和可解释性，探索保护隐私的新闻推荐。除了新闻推荐，MIND  dataset还可以用于其他自然语言处理任务，如主题分类、文本摘要和新闻标题生成。</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">白丁</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://baidinghub.github.io/2020/08/11/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E5%9B%9B%EF%BC%89%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0/">http://baidinghub.github.io/2020/08/11/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E5%9B%9B%EF%BC%89%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://baidinghub.github.io" target="_blank">BaiDing's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">推荐系统</a></div><div class="post_share"><div class="addthis_inline_share_toolbox"></div><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDyIdOn/M/aXcGQOiSDVMqvN2gxDdiESHUfFuB2YMy48fvNN9SZOQUbVlGF4Pk6nDXIAir+br/EWuEnNLtgOCYMo/BTxl29gqS/QGHPiDaIQedzmLcuRZpfDuGit61N/b9pyktpZLagBgbl5Ox9mAgWQxXyhxYB092gyOXqrBULBeZUYQ+H7Eupha10QTQghHv4nLk+oYWo2UXEiijQpE3qMXT32G8v8k0KbRdd1hIFPyNEx6eZ6Buc2ZdbMtoutdGjvdnw5B1+dxgl1egESPChzcwCMxt3NW/3faQ5lsjRhQD4fRU+Ua/aqXe+0xg2+xr0BTjNr/JZIAaNiLuo71lH 1564026260@qq.com" async="async"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/wechat.png?raw=true" alt="微信"/><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/alipay.jpg?raw=true" alt="支付寶"/><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/09/01/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%B8%80%EF%BC%89Word2Vec/"><img class="prev_cover lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%B8%80%EF%BC%89Word2Vec/cover.png?raw=true" onerror="onerror=null;src='https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/404.jpg?raw=true'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">NLP词向量篇（一）Word2Vec</div></div></a></div><div class="next-post pull_right"><a href="/2020/08/10/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%B8%89%EF%BC%89%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0-2018/"><img class="next_cover lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%B8%89%EF%BC%89%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0-2018/cover.png?raw=true" onerror="onerror=null;src='https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/404.jpg?raw=true'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">推荐系统（三）推荐系统综述-2018</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/08/10/推荐系统（三）推荐系统综述-2018/" title="推荐系统（三）推荐系统综述-2018"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%B8%89%EF%BC%89%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0-2018/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-08-10</div><div class="relatedPosts_title">推荐系统（三）推荐系统综述-2018</div></div></a></div><div class="relatedPosts_item"><a href="/2020/08/09/推荐系统（二）推荐系统综述-2017/" title="推荐系统（二）推荐系统综述-2017"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%BA%8C%EF%BC%89%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0-2017/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-08-09</div><div class="relatedPosts_title">推荐系统（二）推荐系统综述-2017</div></div></a></div><div class="relatedPosts_item"><a href="/2020/08/08/推荐系统（一）推荐系统综述/" title="推荐系统（一）推荐系统综述"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E4%B8%80%EF%BC%89%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-08-08</div><div class="relatedPosts_title">推荐系统（一）推荐系统综述</div></div></a></div><div class="relatedPosts_item"><a href="/2020/09/11/NLP特征提取器篇（一）RNN/" title="NLP特征提取器篇（一）RNN"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%99%A8%E7%AF%87%EF%BC%88%E4%B8%80%EF%BC%89RNN/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-09-11</div><div class="relatedPosts_title">NLP特征提取器篇（一）RNN</div></div></a></div><div class="relatedPosts_item"><a href="/2020/09/12/NLP特征提取器篇（三）Transformer/" title="NLP特征提取器篇（三）Transformer"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%99%A8%E7%AF%87%EF%BC%88%E4%B8%89%EF%BC%89Transformer/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-09-12</div><div class="relatedPosts_title">NLP特征提取器篇（三）Transformer</div></div></a></div><div class="relatedPosts_item"><a href="/2020/09/22/NLP词向量篇（七）RoBERTa/" title="NLP词向量篇（七）RoBERTa"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%B8%83%EF%BC%89RoBERTa/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-09-22</div><div class="relatedPosts_title">NLP词向量篇（七）RoBERTa</div></div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: 'd3948be001a91411dfd9',
  clientSecret: 'ebddf2a2a5a039922fb373a8a8c0efcc439bf6ca',
  repo: 'BaiDingHub.github.io',
  owner: 'BaiDingHub',
  admin: ['BaiDingHub'],
  id: md5(decodeURI(location.pathname)),
  language: 'zh-CN',
  perPage: 10,
  distractionFreeMode: false,
  pagerDirection: 'last',
  createIssueManually: false,
  updateCountCallback: commentCount
})
gitalk.render('gitalk-container')

function commentCount(n){
  try {
    document.getElementsByClassName('gitalk-comment-count')[0].innerHTML= n
  } catch (e) {
    return false
  }
}</script></div></article></main><footer id="footer" style="background-image: url(https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%88%E5%9B%9B%EF%BC%89%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0/cover.png?raw=true)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By 白丁</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://jerryc.me/" target="_blank" rel="noopener">blog</a>!</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js" ></script><script src="/js/utils.js" ></script><script src="/js/main.js" ></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><script src="/js/search/algolia.js"></script></body></html>