<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>NLP词向量篇（三）FastText | BaiDing's blog</title><meta name="description" content="NLP词向量篇（三）FastText"><meta name="keywords" content="深度学习,NLP基础知识"><meta name="author" content="白丁"><meta name="copyright" content="白丁"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="https://raw.githubusercontent.com/BaiDingHub/Blog_images/master/BlogSource/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="crossorigin"/><link rel="preconnect" href="http://ta.qq.com"/><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="x-1ef6P_miWkq-RJn_fmjd3KYumrXANNXYzK1myaLf0"/><meta name="baidu-site-verification" content="guD6l44Chk"/><meta name="twitter:card" content="summary"><meta name="twitter:title" content="NLP词向量篇（三）FastText"><meta name="twitter:description" content="NLP词向量篇（三）FastText"><meta name="twitter:image" content="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%B8%89%EF%BC%89FastText/cover.png?raw=true"><meta property="og:type" content="article"><meta property="og:title" content="NLP词向量篇（三）FastText"><meta property="og:url" content="http://baidinghub.github.io/2020/09/02/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%B8%89%EF%BC%89FastText/"><meta property="og:site_name" content="BaiDing's blog"><meta property="og:description" content="NLP词向量篇（三）FastText"><meta property="og:image" content="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%B8%89%EF%BC%89FastText/cover.png?raw=true"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>var autoChangeMode = '1'
var t = Cookies.get("theme")
if (autoChangeMode == '1'){
  var isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
  var isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
  var isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined){
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport){
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour < 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
  }
  } else if (t == 'light') activateLightMode()
  else activateDarkMode()

} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css" ><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://baidinghub.github.io/2020/09/02/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%B8%89%EF%BC%89FastText/"><link rel="prev" title="NLP特征提取器篇（一）RNN" href="http://baidinghub.github.io/2020/09/11/NLP%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%99%A8%E7%AF%87%EF%BC%88%E4%B8%80%EF%BC%89RNN/"><link rel="next" title="NLP词向量篇（二）Glove" href="http://baidinghub.github.io/2020/09/01/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%BA%8C%EF%BC%89Glove/"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.js" defer></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-162698439-1', 'auto');
ga('send', 'pageview');
</script><script src="https://tajs.qq.com/stats?sId=66540586" charset="UTF-8"></script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"4JGH9NW4XG","apiKey":"41cdb6d9ec4d21196956524e9c985b36","indexName":"baiding","hits":{"per_page":6},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  highlightShrink: 'false',
  isFontAwesomeV5: false,
  isPhotoFigcaption: false
  
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isSidebar: true  
  }</script><noscript><style>
#page-header {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/avatar.png?raw=true" onerror="onerror=null;src='https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/friend_404.gif?raw=true'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">94</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">60</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">17</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于作者</span></a></div></div></div></div><i class="fa fa-arrow-right on" id="toggle-sidebar" aria-hidden="true">     </i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Bag-of-Tricks-for-Efficient-Text-Classification"><span class="toc-text">Bag of Tricks for Efficient Text Classification</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#预备知识"><span class="toc-text">预备知识</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#什么是N-gram"><span class="toc-text">什么是N-gram</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#内容分析"><span class="toc-text">内容分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Hierarchical-softmax"><span class="toc-text">Hierarchical softmax</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#N-grams-features"><span class="toc-text">N-grams features</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模型"><span class="toc-text">模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#为什么fastText如此快，效果还挺好？"><span class="toc-text">为什么fastText如此快，效果还挺好？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1、Intorduction"><span class="toc-text">1、Intorduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2、Model-architecture"><span class="toc-text">2、Model architecture</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Hierarchical-softmax"><span class="toc-text">2.1 Hierarchical softmax</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-N-gram-features"><span class="toc-text">2.2 N-gram features</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3、Experiments"><span class="toc-text">3、Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-Sentiment-analysis"><span class="toc-text">3.1 Sentiment analysis</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-1-Datasets-and-baselines"><span class="toc-text">3.1.1 Datasets  and  baselines</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-2-Results"><span class="toc-text">3.1.2 Results</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-3-Tranning-time"><span class="toc-text">3.1.3 Tranning time</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Tag-prediction"><span class="toc-text">3.2 Tag prediction</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4、Discussion-and-conclusion"><span class="toc-text">4、Discussion and conclusion</span></a></li></ol></li></ol></div></div></div><div id="body-wrap"><div class="post-bg" id="nav" style="background-image: url(https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%B8%89%EF%BC%89FastText/cover.png?raw=true)"><div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">BaiDing's blog</a></span><span class="pull_right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于作者</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span></span></div><div id="post-info"><div id="post-title"><div class="posttitle">NLP词向量篇（三）FastText</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2020-09-02 05:20:00"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2020-09-02</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-12-02 11:11:09"><i class="fa fa-history" aria-hidden="true"></i> 更新于 2020-12-02</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><i class="fa fa-angle-right post-meta__separator" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/">NLP基础知识</a></span></div><div class="meta-secondline"> <span class="post-meta-wordcount"><i class="post-meta__icon fa fa-file-word-o" aria-hidden="true"></i><span>字数总计:</span><span class="word-count">3.6k</span><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-clock-o" aria-hidden="true"></i><span>阅读时长: 12 分钟</span></span></div><div class="meta-thirdline"><span class="post-meta-pv-cv"><span class="post-meta__separator">|</span><i class="fa fa-eye post-meta__icon" aria-hidden="true"> </i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-comment-o" aria-hidden="true"></i><span>评论数:</span><a href="/2020/09/02/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%B8%89%EF%BC%89FastText/#post-comment"><span class="gitalk-comment-count comment-count"></span></a></span></div></div></div></div><main class="layout_post" id="content-inner"><article id="post"><div id="article-container"><meta name="referrer" content="no-referrer"/>



<h1 id="Bag-of-Tricks-for-Efficient-Text-Classification"><a href="#Bag-of-Tricks-for-Efficient-Text-Classification" class="headerlink" title="Bag of Tricks for Efficient Text Classification"></a>Bag of Tricks for Efficient Text Classification</h1><blockquote>
<p>时间：2016年</p>
<p>关键词：NLP, Word Embedding</p>
<p>论文位置：<a href="https://arxiv.org/pdf/1607.01759" target="_blank" rel="noopener">https://arxiv.org/pdf/1607.01759</a></p>
<p>引用：Joulin A, Grave E, Bojanowski P, et al. Bag of tricks for efficient text classification[J]. arXiv preprint arXiv:1607.01759, 2016.</p>
</blockquote>
<p><strong>摘要：</strong>本文探讨了一种简单有效的用于文本分类的baseline。实验表明，<strong>fastText分类器在准确率方面与深度学习分类器相当，在训练和评估方面比深度学习分类器快很多个数量级。</strong>使用标准的多核CPU，我们可以在不到10分钟的时间内对fastText进行10亿个单词的训练，并在不到一分钟的时间内对312K个类中的50万个句子进行分类</p>
<p><strong>索引</strong>- 自然语言处理，词向量</p>
<h2 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h2><h3 id="什么是N-gram"><a href="#什么是N-gram" class="headerlink" title="什么是N-gram"></a>什么是N-gram</h3><p>该部分转自<a href="https://zhuanlan.zhihu.com/p/32965521" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/32965521</a></p>
<p> &emsp;&emsp; 在文本特征提取中，常常能看到N-gram的身影。它是一种基于语言模型的算法，基本思想是将文本内容按照字节顺序进行大小为N的滑动窗口操作，最终形成长度为N的字节片段序列。看下面的例子：</p>
<p> &emsp;&emsp; 我来到达观数据参观</p>
<p> &emsp;&emsp; 相应的bigram特征为：我来 来到 到达 达观 观数 数据 据参 参观</p>
<p> &emsp;&emsp; 相应的trigram特征为：我来到 来到达 到达观 达观数 观数据 数据参 据参观</p>
<p> &emsp;&emsp; 注意一点：N-gram中的gram根据<strong>粒度不同，有不同的含义</strong>。<strong>它可以是字粒度、词粒度，也可以是字符粒度的</strong>。</p>
<p> &emsp;&emsp; 上面所举的例子属于<strong>字粒度</strong>的n-gram，<strong>词粒度</strong>的n-gram看下面例子：</p>
<p> &emsp;&emsp; 我 来到 达观数据 参观</p>
<p> &emsp;&emsp; 相应的bigram特征为：我/来到 来到/达观数据 达观数据/参观</p>
<p> &emsp;&emsp; 相应的trigram特征为：我/来到/达观数据 来到/达观数据/参观</p>
<p> &emsp;&emsp; <strong>字符粒度的N-gram是用来表示一个单词</strong>的，用字符粒度的N-gram来表示单词”apple”，设其超参数$ n=3$ ，则以每个字符作为中心，得到其trigram为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">“&lt;ap”, “app”, “ppl”, “ple”, “le&gt;”</span><br></pre></td></tr></table></figure>
<p> &emsp;&emsp; 其中，&lt;表示前缀，&gt;表示后缀。于是，我们可以用这些trigram来表示“apple”这个单词，进一步，我们可以用这5个trigram的向量叠加来表示“apple”的词向量。</p>
<p> &emsp;&emsp; 字符粒度的N-gram带来两点<strong>好处</strong>：</p>
<ol>
<li><p>对于低频词生成的词向量效果会更好。因为它们的n-gram可以和其它词共享。</p>
</li>
<li><p>对于训练词库之外的单词，仍然可以构建它们的词向量。我们可以叠加它们的字符级n-gram向量。</p>
<p>&emsp;&emsp; N-gram产生的特征只是作为文本特征的候选集，你后面可能会采用信息熵、卡方统计、IDF等文本特征选择方式筛选出比较重要特征。</p>
</li>
</ol>
<h2 id="内容分析"><a href="#内容分析" class="headerlink" title="内容分析"></a>内容分析</h2><p> &emsp;&emsp; fastText这篇论文主要是讲解了一些在文本分类任务中的trick，来加速模型的训练，有些任务能够加速特别特别多，主要用到的trick有以下，两个字符级N-gram的引入和分层Sofrmax分类的使用：</p>
<h3 id="Hierarchical-softmax"><a href="#Hierarchical-softmax" class="headerlink" title="Hierarchical softmax"></a>Hierarchical softmax</h3><p> &emsp;&emsp; <strong>使用了基于Huffman编码树的hierarchical softmax</strong>  (Goodman, 2001) (Mikolov et al.， 2013)。在训练过程中，计算复杂度$ O(hk)$ 降至$ O(hlog_2(k))$ ，其中$ k$ 表示分类数目，$ h$ 表示词向量的维度。</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%B8%89%EF%BC%89FastText/1.png?raw=true"  alt="1"></p>
<h3 id="N-grams-features"><a href="#N-grams-features" class="headerlink" title="N-grams features"></a>N-grams features</h3><p> &emsp;&emsp; 针对一个句子而言，其作为模型的输入时，我们会将其每一个单词作为一个输入特征$ x_i$ ，该输入特征是指的他的词向量，同时，我们要得到该词的字符级N-grams特征，作为该特征的附加特征，一同输入到模型中。</p>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%B8%89%EF%BC%89FastText/2.png?raw=true"  alt="2"></p>
<p> &emsp;&emsp; 模型与(Mikolov et al.， 2013)的CBOW十分类似，都包含输入层、隐藏层和输出层，同时CBOW也考虑了分层Softmax，唯一的不同就是字符级N-gram的引入。</p>
<p> &emsp;&emsp; 以文本分类为例，fastText是一个监督学习任务，其词向量作为模型的隐式参数而被学习到。对于一个文本，我们会将其每一个单词作为一个输入特征$ x_i$ ，该输入特征是指的他的词向量，同时，我们要得到该词的字符级N-grams特征，作为该特征的附加特征，一同输入到模型中。</p>
<p> &emsp;&emsp; 输入层到隐藏层这一部分的作用是生成用来表征文本的向量，在fastText中，将所有的词向量以及字符级N-gram词向量取平均，得到的就是表征该文本的向量，该取平均的思想就是词袋法。之后将该向量送到隐藏层，之后通过分层Softmax进行分类。</p>
<h3 id="为什么fastText如此快，效果还挺好？"><a href="#为什么fastText如此快，效果还挺好？" class="headerlink" title="为什么fastText如此快，效果还挺好？"></a>为什么fastText如此快，效果还挺好？</h3><p> &emsp;&emsp; 假设我们有两段文本：</p>
<p> &emsp;&emsp; 我 来到 达观数据</p>
<p> &emsp;&emsp; 俺 去了 达而观信息科技</p>
<p> &emsp;&emsp; 这两段文本意思几乎一模一样，如果要分类，肯定要分到同一个类中去。但在传统的分类器中，用来表征这两段文本的向量可能差距非常大。传统的文本分类中，你需要计算出每个词的权重，比如TFIDF值， “我”和“俺” 算出的TFIDF值相差可能会比较大，其它词类似，于是，VSM（向量空间模型）中用来表征这两段文本的文本向量差别可能比较大。但是fastText就不一样了，它是用单词的embedding叠加获得的文档向量，词向量的重要特点就是向量的距离可以用来衡量单词间的语义相似程度，于是，在fastText模型中，这两段文本的向量应该是非常相似的，于是，它们很大概率会被分到同一个类中。</p>
<p> &emsp;&emsp; <strong>使用词embedding而非词本身作为特征</strong>，这是fastText效果好的一个原因；另一个原因就是<strong>字符级n-gram特征的引入对分类效果会有一些提升</strong> 。</p>
<h2 id="1、Intorduction"><a href="#1、Intorduction" class="headerlink" title="1、Intorduction"></a>1、Intorduction</h2><p> &emsp;&emsp; 文本分类是自然语言处理中的一项重要任务，应用广泛，如web搜索、信息检索、排序和文档分类(Deerwester et al., 1990;Pang and Lee, 2008)。最近，基于神经网络的模型越来越受欢迎(Kim, 2014; Zhang and LeCun, 2015;Conneau et al., 2016)。虽然这些模型在实践中取得了非常好的性能，但它们在训练和测试时往往相对较慢，这限制了它们在非常大的数据集上的使用。</p>
<p> &emsp;&emsp; 同时，线性分类器通常被认为是文本分类问题中比较强的baseline(Joachims, 1998;McCallum and Nigam, 1998; Fan et al., 2008)。尽管它们很简单，但如果使用了正确的特征，它们往往能获得SOTA的表现(Wang和Manning,  2012)。它们还具有拓展到非常大的语料库的潜力(Agarwal等人，2014)。</p>
<p> &emsp;&emsp; 在本研究中，我们探索了在文本分类的背景下，如何将这些baseline扩展到具有大输出空间的超大语料库。受最近高效的词向量学习工作的启发(Mikolov et al., 2013; Levy et al., 2015))，我们证明，带有秩约束和快速损失近似的线性模型可以在十分钟内训练10亿个单词，同时实现SOTA的性能。我们在两个不同的任务上评估我们的fasttext1方法的质量，即标签预测和情绪分析。</p>
<h2 id="2、Model-architecture"><a href="#2、Model-architecture" class="headerlink" title="2、Model architecture"></a>2、Model architecture</h2><p> &emsp;&emsp; 一个简单而有效的句子分类的baseline是将句子表示为bag of words(BoW)，然后训练一个线性分类器，例如logistic回归或SVM (Joachims, 1998;Fan et al., 2008)。然而，线性分类器不共享特征和类之间的参数。这可能限制了它们在大输出空间情况下的泛化，因为有些类只有很少的示例。通常的解决方法是将线性分类器分解为低秩矩阵(Schutze, 1992;Mikolov et al., 2013)或使用多层神经网络(Collobert and Weston, 2008;Zhang et al., 2015)。</p>
<p> &emsp;&emsp; 图1显示了一个带有秩约束的简单线性模型。第一个权重矩阵A是单词的查找表。然后将词向量取平均得到一个文本表示，然后将文本表示送给线性分类器。文本表示是一个潜在的可重用的隐藏变量。这种架构类似于Mikolov等人(2013)的CBOW模型（中间的单词被一个标签所替代）。我们使用softmax函数$ f$ 来计算预先定义类别的概率分布。对于一组N个文档，这会使类的负对数似然值最小化：</p>
<script type="math/tex; mode=display">
-\frac{1}{N} \sum_{n=1}^N y_n \log(f(BAx_n))</script><p> &emsp;&emsp; 其中$ x_n$ 是将第n个文本特征规范化后的词向量，$ y_n$ 是标签，$ A,B$ 为权重矩阵。该模型采用随机梯度下降和线性衰减学习率在多个cpu上进行异步训练。</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%B8%89%EF%BC%89FastText/2.png?raw=true"  alt="2"></p>
<h3 id="2-1-Hierarchical-softmax"><a href="#2-1-Hierarchical-softmax" class="headerlink" title="2.1 Hierarchical softmax"></a>2.1 Hierarchical softmax</h3><p> &emsp;&emsp; 当类的数量很大时，计算线性分类器的计算开销很大。更准确地说，计算复杂度为$ O(kh)$ ，其中$ k$ 为类的数量，$ h$ 为词向量的维数。为了提高我们的运行时间，我们<strong>使用了基于Huffman编码树的hierarchical softmax</strong>  (Goodman, 2001) (Mikolov et al.， 2013)。在训练过程中，计算复杂度降至$ O(hlog_2(k))$ 。</p>
<p> &emsp;&emsp; 在寻找最接近的类时，hierarchical softmax在测试阶段也是有益的。Huffman编码树的每个节点都与一个概率相关联，叶子节点的概率就是从根节点到该节点的概率乘积。如果深度为$ l+1$ 处节点的父节点为$ n_1,…,n_l$ ，那么他的概率值为：</p>
<script type="math/tex; mode=display">
P(n_{l+1}) = \prod_{i=1}^l P(n_i)</script><p> &emsp;&emsp; 这意味着节点的概率总是低于其父节点的概率。对树进行深度优先搜索并跟踪叶节点之间的最大概率，这样我们就可以丢弃任何概率很小的分支。在实践中，我们观察到复杂度在测试时降低到$ O(hlog2(k))$ 。这种方法被进一步扩展到使用二进制堆以$ O(log(T))$ 为代价计算T-top目标。</p>
<h3 id="2-2-N-gram-features"><a href="#2-2-N-gram-features" class="headerlink" title="2.2 N-gram features"></a>2.2 N-gram features</h3><p> &emsp;&emsp; 词袋的方法不会考虑词的顺序，但是简单的考虑词的顺序会使得计算代价非常大。<strong>我们使用一个包含n个字母的包作为附加特性，以捕获关于局部词序的部分信息</strong>。这在实践中是非常有效的，可以获与显式使用顺序的方法相当的结果(Wang和Manning, 2012)。</p>
<p> &emsp;&emsp; 通过使用hash技巧 (Weinberger et al., 2009)（与（Mikolov et al. (2011）中的技巧相同），我们保持对N-grams快速和高效的内存映射。如果使用bigram只需要10M的bins，否则需要100M的bins。</p>
<h2 id="3、Experiments"><a href="#3、Experiments" class="headerlink" title="3、Experiments"></a>3、Experiments</h2><p> &emsp;&emsp; 我们在两个不同的任务上评估fastText。首先，我们将其与现有的文本分类器在情感分析问题上进行了比较。然后，我们评估它在标签预测数据集中拓展到大输出空间的能力。注意我们的模型可以用Vowpal Wabbit库来实现，但是我们在实践中观察到，我们的定制实现至少要快2-5倍。</p>
<h3 id="3-1-Sentiment-analysis"><a href="#3-1-Sentiment-analysis" class="headerlink" title="3.1 Sentiment analysis"></a>3.1 Sentiment analysis</h3><h4 id="3-1-1-Datasets-and-baselines"><a href="#3-1-1-Datasets-and-baselines" class="headerlink" title="3.1.1 Datasets  and  baselines"></a>3.1.1 Datasets  and  baselines</h4><p> &emsp;&emsp; 我们采用了（Zhang et al. (2015））中使用过的8个数据集以及评价策略，采用了n-grams和TFIDF baseline与character level convolutional model (char-CNN) of Zhang and LeCun (2015)，character based convolution recurrent network (char-CRNN) of (Xiao and Cho, 2016) ，very deep convolutional network (VDCNN) of Conneau et al. (2016)。我们还采用了（Tang et al. (2015) ）的评价方法，使用了他们的两个主要baseline，Conv-GRNN and LSTM-GRNN。</p>
<h4 id="3-1-2-Results"><a href="#3-1-2-Results" class="headerlink" title="3.1.2 Results"></a>3.1.2 Results</h4><p> &emsp;&emsp; 我们在表1中展示了结果。我们使用10个隐藏神经元，在验证集上运行fastText 5个epoch。从{0.05,0.1,0.25,0.5}中选择的验证集的学习速率运行5个epoch的fastText。在这个任务中，添加三元信息可以提升1  -  4%的性能。总的来说，我们的准确率比char-CNN和char-CRNN稍好，比VDCNN稍差。注意，我们可以通过使用更多的n-grams略微提高精度，例如使用三元组，搜狗的性能上升到97.1%。最后，图3显示了我们的方法与Tang  et al.(2015)提出的方法是有竞争力的。我们调优了验证集上的超参数，并观察到使用n-grams到5可以获得最佳性能。与Tang et  al.(2015)不同，fastText没有使用预先训练好的词向量，这可以解释1%的准确率差</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%B8%89%EF%BC%89FastText/3.png?raw=true"  alt="3"></p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%B8%89%EF%BC%89FastText/4.png?raw=true"  alt="4"></p>
<h4 id="3-1-3-Tranning-time"><a href="#3-1-3-Tranning-time" class="headerlink" title="3.1.3 Tranning time"></a>3.1.3 Tranning time</h4><p> &emsp;&emsp; char-Cnn和VDCNN都是在NVIDIA Tesla K40  GPU上训练的，而我们的模型是在使用20个线程的CPU上训练的。表2显示了使用卷积的方法比fastText慢几个数量级。虽然通过使用最近CUDA的卷积实现，可以为char-CNN提速10倍，但是fastText只需要不到一分钟的时间就能在这些数据集上进行训练。Tang等人(2015)的GRNNs方法在CPU上使用单线程时，每个epoch大约需要12小时。相比于那些神经网络模型，我们的模型可以提高15000倍的速度。</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%B8%89%EF%BC%89FastText/5.png?raw=true"  alt="5"></p>
<h3 id="3-2-Tag-prediction"><a href="#3-2-Tag-prediction" class="headerlink" title="3.2 Tag prediction"></a>3.2 Tag prediction</h3><p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%B8%89%EF%BC%89FastText/6.png?raw=true"  alt="6"></p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%B8%89%EF%BC%89FastText/7.png?raw=true"  alt="7"></p>
<h2 id="4、Discussion-and-conclusion"><a href="#4、Discussion-and-conclusion" class="headerlink" title="4、Discussion and conclusion"></a>4、Discussion and conclusion</h2><p> &emsp;&emsp; 在本文中，我们提出了一种简单的文本分类方法的baseline。与来自word2vec的未经监督训练的词向量不同，我们的单词特征可以平均在一起形成好的句子表征。在一些任务中，fastText的性能与最近提出的基于深度学习的方法相当，但速度要快得多。尽管从理论上讲，深度神经网络比浅层模型具有更高的代表性，但像情绪分析这样的简单文本分类问题是否适合用来评估它们还不清楚。我们将发布我们的代码，以便研究社区可以轻松地在我们的工作基础上进行构建。</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">白丁</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://baidinghub.github.io/2020/09/02/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%B8%89%EF%BC%89FastText/">http://baidinghub.github.io/2020/09/02/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%B8%89%EF%BC%89FastText/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://baidinghub.github.io" target="_blank">BaiDing's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/">NLP基础知识</a></div><div class="post_share"><div class="addthis_inline_share_toolbox"></div><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDyIdOn/M/aXcGQOiSDVMqvN2gxDdiESHUfFuB2YMy48fvNN9SZOQUbVlGF4Pk6nDXIAir+br/EWuEnNLtgOCYMo/BTxl29gqS/QGHPiDaIQedzmLcuRZpfDuGit61N/b9pyktpZLagBgbl5Ox9mAgWQxXyhxYB092gyOXqrBULBeZUYQ+H7Eupha10QTQghHv4nLk+oYWo2UXEiijQpE3qMXT32G8v8k0KbRdd1hIFPyNEx6eZ6Buc2ZdbMtoutdGjvdnw5B1+dxgl1egESPChzcwCMxt3NW/3faQ5lsjRhQD4fRU+Ua/aqXe+0xg2+xr0BTjNr/JZIAaNiLuo71lH 1564026260@qq.com" async="async"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/wechat.png?raw=true" alt="微信"/><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/alipay.jpg?raw=true" alt="支付寶"/><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/09/11/NLP%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%99%A8%E7%AF%87%EF%BC%88%E4%B8%80%EF%BC%89RNN/"><img class="prev_cover lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%99%A8%E7%AF%87%EF%BC%88%E4%B8%80%EF%BC%89RNN/cover.png?raw=true" onerror="onerror=null;src='https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/404.jpg?raw=true'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">NLP特征提取器篇（一）RNN</div></div></a></div><div class="next-post pull_right"><a href="/2020/09/01/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%BA%8C%EF%BC%89Glove/"><img class="next_cover lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%BA%8C%EF%BC%89Glove/cover.png?raw=true" onerror="onerror=null;src='https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/404.jpg?raw=true'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">NLP词向量篇（二）Glove</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/09/11/NLP特征提取器篇（一）RNN/" title="NLP特征提取器篇（一）RNN"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%99%A8%E7%AF%87%EF%BC%88%E4%B8%80%EF%BC%89RNN/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-09-11</div><div class="relatedPosts_title">NLP特征提取器篇（一）RNN</div></div></a></div><div class="relatedPosts_item"><a href="/2020/09/11/NLP特征提取器篇（二）LSTM/" title="NLP特征提取器篇（二）LSTM"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%99%A8%E7%AF%87%EF%BC%88%E4%BA%8C%EF%BC%89LSTM/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-09-11</div><div class="relatedPosts_title">NLP特征提取器篇（二）LSTM</div></div></a></div><div class="relatedPosts_item"><a href="/2020/09/01/NLP词向量篇（一）Word2Vec/" title="NLP词向量篇（一）Word2Vec"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%B8%80%EF%BC%89Word2Vec/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-09-01</div><div class="relatedPosts_title">NLP词向量篇（一）Word2Vec</div></div></a></div><div class="relatedPosts_item"><a href="/2020/09/12/NLP特征提取器篇（三）Transformer/" title="NLP特征提取器篇（三）Transformer"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%99%A8%E7%AF%87%EF%BC%88%E4%B8%89%EF%BC%89Transformer/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-09-12</div><div class="relatedPosts_title">NLP特征提取器篇（三）Transformer</div></div></a></div><div class="relatedPosts_item"><a href="/2020/09/22/NLP词向量篇（七）RoBERTa/" title="NLP词向量篇（七）RoBERTa"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%B8%83%EF%BC%89RoBERTa/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-09-22</div><div class="relatedPosts_title">NLP词向量篇（七）RoBERTa</div></div></a></div><div class="relatedPosts_item"><a href="/2020/09/01/NLP词向量篇（二）Glove/" title="NLP词向量篇（二）Glove"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%BA%8C%EF%BC%89Glove/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-09-01</div><div class="relatedPosts_title">NLP词向量篇（二）Glove</div></div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: 'd3948be001a91411dfd9',
  clientSecret: 'ebddf2a2a5a039922fb373a8a8c0efcc439bf6ca',
  repo: 'BaiDingHub.github.io',
  owner: 'BaiDingHub',
  admin: ['BaiDingHub'],
  id: md5(decodeURI(location.pathname)),
  language: 'zh-CN',
  perPage: 10,
  distractionFreeMode: false,
  pagerDirection: 'last',
  createIssueManually: false,
  updateCountCallback: commentCount
})
gitalk.render('gitalk-container')

function commentCount(n){
  try {
    document.getElementsByClassName('gitalk-comment-count')[0].innerHTML= n
  } catch (e) {
    return false
  }
}</script></div></article></main><footer id="footer" style="background-image: url(https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%B8%89%EF%BC%89FastText/cover.png?raw=true)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By 白丁</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://jerryc.me/" target="_blank" rel="noopener">blog</a>!</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js" ></script><script src="/js/utils.js" ></script><script src="/js/main.js" ></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><script src="/js/search/algolia.js"></script></body></html>