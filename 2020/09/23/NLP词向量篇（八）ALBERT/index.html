<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>NLP词向量篇（八）ALBERT | BaiDing's blog</title><meta name="description" content="NLP词向量篇（八）ALBERT"><meta name="keywords" content="深度学习,NLP基础知识"><meta name="author" content="白丁"><meta name="copyright" content="白丁"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="https://raw.githubusercontent.com/BaiDingHub/Blog_images/master/BlogSource/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="crossorigin"/><link rel="preconnect" href="http://ta.qq.com"/><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="x-1ef6P_miWkq-RJn_fmjd3KYumrXANNXYzK1myaLf0"/><meta name="baidu-site-verification" content="guD6l44Chk"/><meta name="twitter:card" content="summary"><meta name="twitter:title" content="NLP词向量篇（八）ALBERT"><meta name="twitter:description" content="NLP词向量篇（八）ALBERT"><meta name="twitter:image" content="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E5%85%AB%EF%BC%89ALBERT/cover.png?raw=true"><meta property="og:type" content="article"><meta property="og:title" content="NLP词向量篇（八）ALBERT"><meta property="og:url" content="http://baidinghub.github.io/2020/09/23/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E5%85%AB%EF%BC%89ALBERT/"><meta property="og:site_name" content="BaiDing's blog"><meta property="og:description" content="NLP词向量篇（八）ALBERT"><meta property="og:image" content="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E5%85%AB%EF%BC%89ALBERT/cover.png?raw=true"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>var autoChangeMode = '1'
var t = Cookies.get("theme")
if (autoChangeMode == '1'){
  var isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
  var isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
  var isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined){
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport){
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour < 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
  }
  } else if (t == 'light') activateLightMode()
  else activateDarkMode()

} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css" ><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://baidinghub.github.io/2020/09/23/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E5%85%AB%EF%BC%89ALBERT/"><link rel="next" title="NLP词向量篇（七）RoBERTa" href="http://baidinghub.github.io/2020/09/22/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%B8%83%EF%BC%89RoBERTa/"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.js" defer></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-162698439-1', 'auto');
ga('send', 'pageview');
</script><script src="https://tajs.qq.com/stats?sId=66540586" charset="UTF-8"></script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"4JGH9NW4XG","apiKey":"41cdb6d9ec4d21196956524e9c985b36","indexName":"baiding","hits":{"per_page":6},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  highlightShrink: 'false',
  isFontAwesomeV5: false,
  isPhotoFigcaption: false
  
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isSidebar: true  
  }</script><noscript><style>
#page-header {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/avatar.png?raw=true" onerror="onerror=null;src='https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/friend_404.gif?raw=true'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">92</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">57</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">16</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于作者</span></a></div></div></div></div><i class="fa fa-arrow-right on" id="toggle-sidebar" aria-hidden="true">     </i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#ALBERT-A-Lite-BERT-for-Self-supervised-Learning-of-Language-Representations"><span class="toc-text">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Paper-Information"><span class="toc-text">1. Paper Information</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Motivation"><span class="toc-text">2. Motivation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Main-Arguments"><span class="toc-text">3. Main Arguments</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Framework"><span class="toc-text">4. Framework</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Factorized-embedding-parameterization"><span class="toc-text">4.1 Factorized embedding parameterization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Cross-layer-parameter-sharing"><span class="toc-text">4.2 Cross-layer parameter sharing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-Inter-sentence-coherence-loss"><span class="toc-text">4.3 Inter-sentence coherence loss</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Result"><span class="toc-text">5. Result</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-Factorized-embedding-parameterization"><span class="toc-text">5.1 Factorized embedding parameterization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-Cross-layer-parameter-sharing"><span class="toc-text">5.2 Cross-layer parameter sharing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-Sentence-order-prediction-SOP"><span class="toc-text">5.3 Sentence-order prediction (SOP)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-相同训练时间下的效果（大致相同的参数量）"><span class="toc-text">5.4 相同训练时间下的效果（大致相同的参数量）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-5-其他训练策略"><span class="toc-text">5.5 其他训练策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-6-SOTA"><span class="toc-text">5.6 SOTA</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Argument"><span class="toc-text">6. Argument</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-Further-research"><span class="toc-text">7. Further research</span></a></li></ol></li></ol></div></div></div><div id="body-wrap"><div class="post-bg" id="nav" style="background-image: url(https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E5%85%AB%EF%BC%89ALBERT/cover.png?raw=true)"><div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">BaiDing's blog</a></span><span class="pull_right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于作者</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span></span></div><div id="post-info"><div id="post-title"><div class="posttitle">NLP词向量篇（八）ALBERT</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2020-09-23 05:20:00"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2020-09-23</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-12-02 11:11:09"><i class="fa fa-history" aria-hidden="true"></i> 更新于 2020-12-02</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><i class="fa fa-angle-right post-meta__separator" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/">NLP基础知识</a></span></div><div class="meta-secondline"> <span class="post-meta-wordcount"><i class="post-meta__icon fa fa-file-word-o" aria-hidden="true"></i><span>字数总计:</span><span class="word-count">1.5k</span><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-clock-o" aria-hidden="true"></i><span>阅读时长: 5 分钟</span></span></div><div class="meta-thirdline"><span class="post-meta-pv-cv"><span class="post-meta__separator">|</span><i class="fa fa-eye post-meta__icon" aria-hidden="true"> </i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-comment-o" aria-hidden="true"></i><span>评论数:</span><a href="/2020/09/23/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E5%85%AB%EF%BC%89ALBERT/#post-comment"><span class="gitalk-comment-count comment-count"></span></a></span></div></div></div></div><main class="layout_post" id="content-inner"><article id="post"><div id="article-container"><meta name="referrer" content="no-referrer"/>



<h1 id="ALBERT-A-Lite-BERT-for-Self-supervised-Learning-of-Language-Representations"><a href="#ALBERT-A-Lite-BERT-for-Self-supervised-Learning-of-Language-Representations" class="headerlink" title="ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"></a>ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</h1><h2 id="1-Paper-Information"><a href="#1-Paper-Information" class="headerlink" title="1. Paper Information"></a>1. Paper Information</h2><blockquote>
<p>时间：2019年</p>
<p>关键词：NLP, Word Embedding</p>
<p>论文位置：<a href="https://arxiv.org/pdf/1909.11942.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1909.11942.pdf</a></p>
<p>引用：Lan Z, Chen M, Goodman S, et al. Albert: A lite bert for self-supervised learning of language representations[J]. arXiv preprint arXiv:1909.11942, 2019.</p>
</blockquote>
<h2 id="2-Motivation"><a href="#2-Motivation" class="headerlink" title="2. Motivation"></a>2. Motivation</h2><p> &emsp;&emsp; 我们都知道，增大BERT模型的尺寸会使BERT的效果变得更好，当BERT越来越大时，训练代价也会越来越大。目前，SOTA的NLP模型有着上亿的参数量，我们的GPU越来越难以承担，内存开销越来越大，训练时间越来越长，那么，<strong>怎么才能够在保持BERT效果的情况下，减少模型的参数量呢？</strong>本篇论文就是来解决这个问题。</p>
<h2 id="3-Main-Arguments"><a href="#3-Main-Arguments" class="headerlink" title="3. Main Arguments"></a>3. Main Arguments</h2><p> &emsp;&emsp; 作者<strong>使用了两种参数简化的方法来降低内存消耗，并增加BERT的训练速度</strong>。同时，<strong>使用了一个自监督Loss，来建模句子间的联系</strong>，这种Loss使得BERT模型，在具有多个句子输入的任务上表现得更好。</p>
<p> &emsp;&emsp; 这两种参数简化方法如下：</p>
<ul>
<li>第一种是<strong>factorized embedding parameterization</strong>。通过将原来的大的embedding矩阵分解成两个小的矩阵，我们将隐藏层的大小和词向量的大小的关系解耦，这样我们就可以在增大隐藏层大小的同时，而不会过度的增加词向量矩阵的参数大小。</li>
<li><p>第二种是<strong>cross-layer parameter sharing</strong>。这个方法防止了我们的参数随着网络深度的增加而增加。</p>
<p>&emsp;&emsp; 在BERT-large模型上，ALBERT的参数减少了18倍，运行时间加快了1.7倍。</p>
<p>&emsp;&emsp; 为了进一步提升ALBERT的性能，我们还引入了一种用于sentence-order prediction (SOP)任务的自监督Loss。SOP任务主要用来关注句间的关系，用来解决NSP任务中的无效性的问题。</p>
<p>&emsp;&emsp; 最后，本篇论文的模型在GLUE、RACE、SQuAD上都取得了SOTA的效果，但却有着更少的参数量。这个模型被称为ALBERT。</p>
</li>
</ul>
<h2 id="4-Framework"><a href="#4-Framework" class="headerlink" title="4. Framework"></a>4. Framework</h2><p> &emsp;&emsp; 记BERT的超参数为：词向量维度为$\ E$ ，enoder的层数为$\ L$ ，隐藏层输出维度为$\ H$ ，attention head数目为$\ H/64$ ，前馈网络中的神经元的维度为$\ 4H$ ，这样看的话，attention的输入Q、K、V的维度就是$\ 4 * 64 = 256$ 。 </p>
<h3 id="4-1-Factorized-embedding-parameterization"><a href="#4-1-Factorized-embedding-parameterization" class="headerlink" title="4.1 Factorized embedding parameterization"></a>4.1 Factorized embedding parameterization</h3><p> &emsp;&emsp; 由于残差连接的关系，我们一般取$\ H = E$ ，但，这种决策无论是在建模角度还是实际角度都不是最优的。我们知道，增大$\ H$ 可以提高模型的容量，进而提升性能，如果我们能够<strong>解开$\ H$ 和$\ E$ 的关系</strong>，我们就可以取比较大的$\ H$ ，而对词向量矩阵的大小没影响，进而减少了大量的参数增加。</p>
<p> &emsp;&emsp; ALBERT采用了词向量矩阵的因式分解，将原词向量矩阵$\ V \times H$ ，分解成两个小的矩阵$\ V \times E$ 和$\ E \times H$ 。当$\ H \gg E$ 时，这种参数简化是很重要的。</p>
<h3 id="4-2-Cross-layer-parameter-sharing"><a href="#4-2-Cross-layer-parameter-sharing" class="headerlink" title="4.2 Cross-layer parameter sharing"></a>4.2 Cross-layer parameter sharing</h3><p> &emsp;&emsp; 跨层的参数共享有很多方法，比如只共享前馈神经网络参数，或者只共享attention的参数，或者共享所有的参数。ALBERT选择共享所有的参数。</p>
<p> &emsp;&emsp; 作者对某一层的输入和输出进行L2和cos距离测量，来测算输入和输出的相似性，其结果如下：</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E5%85%AB%EF%BC%89ALBERT/1.png?raw=true"  alt="1"></p>
<p> &emsp;&emsp; 我们可以看到，当网络层数增加时，输入和输出是比较相近的，这有利于增加网络参数的稳定性。而且，相比于之前的 Cross-layer parameter sharing方法（DQE），我们最后的输入和输出并不会完全一致，也就是说，我们的方法跟DQE（Deep Equilibrium Models，用在Transformer上）是完全不同的，</p>
<h3 id="4-3-Inter-sentence-coherence-loss"><a href="#4-3-Inter-sentence-coherence-loss" class="headerlink" title="4.3 Inter-sentence coherence loss"></a>4.3 Inter-sentence coherence loss</h3><p> &emsp;&emsp; BERT的预训练中有两个任务，MLM和NSP，RoBERTa证明了NSP用处不大，还会损失BERT的精度，作者猜想是因为NSP这个任务太简单了。NSP任务包含了topic预测和句子相关性预测，但是topic预测相比于相关性预测来说太简单了，而且与MLM重叠较大。</p>
<p> &emsp;&emsp; 所以，作者就提出了一个单独基于相关性的预测loss，即句序预测（sentence-order prediction (SOP)）。</p>
<h2 id="5-Result"><a href="#5-Result" class="headerlink" title="5. Result"></a>5. Result</h2><p> &emsp;&emsp; ALBERT-large相比于BERT-large，参数量小了18倍，只有18M，BERT-large有334M。ALBERT-xlarge（$\ H = 2048$） 有60M，ALBERT-xxlarge（$\ H = $4096） 有233M。ALBERT-xxlarge只有BERT-large的70%的参数，但是在很多下游任务上都超越了BERT-large，SQuAD v1.1 (+1.9%), SQuAD v2.0 (+3.1%), MNLI (+1.4%), SST-2 (+2.2%), and RACE (+8.4%)。在速度上，ALBERT-large比BERT-large快1.5倍，但是ALBERT-xxlarge要比BERT-large慢3倍。</p>
<h3 id="5-1-Factorized-embedding-parameterization"><a href="#5-1-Factorized-embedding-parameterization" class="headerlink" title="5.1 Factorized embedding parameterization"></a>5.1 Factorized embedding parameterization</h3><p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E5%85%AB%EF%BC%89ALBERT/2.png?raw=true"  alt="2"></p>
<p> &emsp;&emsp; 不同的词向量维度$\ E$ 以及cross-layer sharing带来的影响：</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E5%85%AB%EF%BC%89ALBERT/3.png?raw=true"  alt="3"></p>
<h3 id="5-2-Cross-layer-parameter-sharing"><a href="#5-2-Cross-layer-parameter-sharing" class="headerlink" title="5.2 Cross-layer parameter sharing"></a>5.2 Cross-layer parameter sharing</h3><p> &emsp;&emsp; 不同的共享策略带来的影响</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E5%85%AB%EF%BC%89ALBERT/4.png?raw=true"  alt="4"></p>
<h3 id="5-3-Sentence-order-prediction-SOP"><a href="#5-3-Sentence-order-prediction-SOP" class="headerlink" title="5.3 Sentence-order prediction (SOP)"></a>5.3 Sentence-order prediction (SOP)</h3><p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E5%85%AB%EF%BC%89ALBERT/5.png?raw=true"  alt="5"></p>
<h3 id="5-4-相同训练时间下的效果（大致相同的参数量）"><a href="#5-4-相同训练时间下的效果（大致相同的参数量）" class="headerlink" title="5.4 相同训练时间下的效果（大致相同的参数量）"></a>5.4 相同训练时间下的效果（大致相同的参数量）</h3><p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E5%85%AB%EF%BC%89ALBERT/6.png?raw=true"  alt="6"></p>
<h3 id="5-5-其他训练策略"><a href="#5-5-其他训练策略" class="headerlink" title="5.5 其他训练策略"></a>5.5 其他训练策略</h3><p> &emsp;&emsp; 增加的是RoBERTa和XLNet中使用的数据集</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E5%85%AB%EF%BC%89ALBERT/7.png?raw=true"  alt="7"></p>
<p> &emsp;&emsp; </p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E5%85%AB%EF%BC%89ALBERT/8.png?raw=true"  alt="8"></p>
<h3 id="5-6-SOTA"><a href="#5-6-SOTA" class="headerlink" title="5.6 SOTA"></a>5.6 SOTA</h3><p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E5%85%AB%EF%BC%89ALBERT/9.png?raw=true"  alt="9"></p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E5%85%AB%EF%BC%89ALBERT/10.png?raw=true"  alt="10"></p>
<h2 id="6-Argument"><a href="#6-Argument" class="headerlink" title="6. Argument"></a>6. Argument</h2><p> &emsp;&emsp; ALBERT通过两种网络压缩方法极大的简化了BERT的参数量。但如果想要超过BERT的效果，我们需要堆叠更高的深度，这样就会导致训练时间更拉跨。从压缩的角度来看，ALBERT是一个巨大的提升。但我们还需要找到一个方法，在保持性能和速度的同时压缩模型。</p>
<h2 id="7-Further-research"><a href="#7-Further-research" class="headerlink" title="7. Further research"></a>7. Further research</h2><p> &emsp;&emsp; 除了作者提到的几种方法，我们还可以使用一些其他的方法来进行加速，比如sparse attention和block attention。另外，我们可以通过难样本挖掘和更有效的训练方法来提供更强的性能。另外，作者提到，在词向量中，可能会有很多维度尚未被当前的模型挖掘，而如果能进一步处理这些问题，可以进一步的提升模型的性能。</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">白丁</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://baidinghub.github.io/2020/09/23/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E5%85%AB%EF%BC%89ALBERT/">http://baidinghub.github.io/2020/09/23/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E5%85%AB%EF%BC%89ALBERT/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://baidinghub.github.io" target="_blank">BaiDing's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/">NLP基础知识</a></div><div class="post_share"><div class="addthis_inline_share_toolbox"></div><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDyIdOn/M/aXcGQOiSDVMqvN2gxDdiESHUfFuB2YMy48fvNN9SZOQUbVlGF4Pk6nDXIAir+br/EWuEnNLtgOCYMo/BTxl29gqS/QGHPiDaIQedzmLcuRZpfDuGit61N/b9pyktpZLagBgbl5Ox9mAgWQxXyhxYB092gyOXqrBULBeZUYQ+H7Eupha10QTQghHv4nLk+oYWo2UXEiijQpE3qMXT32G8v8k0KbRdd1hIFPyNEx6eZ6Buc2ZdbMtoutdGjvdnw5B1+dxgl1egESPChzcwCMxt3NW/3faQ5lsjRhQD4fRU+Ua/aqXe+0xg2+xr0BTjNr/JZIAaNiLuo71lH 1564026260@qq.com" async="async"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/wechat.png?raw=true" alt="微信"/><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/alipay.jpg?raw=true" alt="支付寶"/><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="next-post pull-full"><a href="/2020/09/22/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%B8%83%EF%BC%89RoBERTa/"><img class="next_cover lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%B8%83%EF%BC%89RoBERTa/cover.png?raw=true" onerror="onerror=null;src='https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/404.jpg?raw=true'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">NLP词向量篇（七）RoBERTa</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/09/11/NLP特征提取器篇（一）RNN/" title="NLP特征提取器篇（一）RNN"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%99%A8%E7%AF%87%EF%BC%88%E4%B8%80%EF%BC%89RNN/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-09-11</div><div class="relatedPosts_title">NLP特征提取器篇（一）RNN</div></div></a></div><div class="relatedPosts_item"><a href="/2020/09/12/NLP特征提取器篇（三）Transformer/" title="NLP特征提取器篇（三）Transformer"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%99%A8%E7%AF%87%EF%BC%88%E4%B8%89%EF%BC%89Transformer/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-09-12</div><div class="relatedPosts_title">NLP特征提取器篇（三）Transformer</div></div></a></div><div class="relatedPosts_item"><a href="/2020/09/22/NLP词向量篇（七）RoBERTa/" title="NLP词向量篇（七）RoBERTa"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%B8%83%EF%BC%89RoBERTa/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-09-22</div><div class="relatedPosts_title">NLP词向量篇（七）RoBERTa</div></div></a></div><div class="relatedPosts_item"><a href="/2020/09/11/NLP特征提取器篇（二）LSTM/" title="NLP特征提取器篇（二）LSTM"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%99%A8%E7%AF%87%EF%BC%88%E4%BA%8C%EF%BC%89LSTM/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-09-11</div><div class="relatedPosts_title">NLP特征提取器篇（二）LSTM</div></div></a></div><div class="relatedPosts_item"><a href="/2020/09/01/NLP词向量篇（一）Word2Vec/" title="NLP词向量篇（一）Word2Vec"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%B8%80%EF%BC%89Word2Vec/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-09-01</div><div class="relatedPosts_title">NLP词向量篇（一）Word2Vec</div></div></a></div><div class="relatedPosts_item"><a href="/2020/09/02/NLP词向量篇（三）FastText/" title="NLP词向量篇（三）FastText"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E4%B8%89%EF%BC%89FastText/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-09-02</div><div class="relatedPosts_title">NLP词向量篇（三）FastText</div></div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: 'd3948be001a91411dfd9',
  clientSecret: 'ebddf2a2a5a039922fb373a8a8c0efcc439bf6ca',
  repo: 'BaiDingHub.github.io',
  owner: 'BaiDingHub',
  admin: ['BaiDingHub'],
  id: md5(decodeURI(location.pathname)),
  language: 'zh-CN',
  perPage: 10,
  distractionFreeMode: false,
  pagerDirection: 'last',
  createIssueManually: false,
  updateCountCallback: commentCount
})
gitalk.render('gitalk-container')

function commentCount(n){
  try {
    document.getElementsByClassName('gitalk-comment-count')[0].innerHTML= n
  } catch (e) {
    return false
  }
}</script></div></article></main><footer id="footer" style="background-image: url(https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/NLP%E8%AF%8D%E5%90%91%E9%87%8F%E7%AF%87%EF%BC%88%E5%85%AB%EF%BC%89ALBERT/cover.png?raw=true)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2020 By 白丁</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://jerryc.me/" target="_blank" rel="noopener">blog</a>!</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js" ></script><script src="/js/utils.js" ></script><script src="/js/main.js" ></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><script src="/js/search/algolia.js"></script></body></html>