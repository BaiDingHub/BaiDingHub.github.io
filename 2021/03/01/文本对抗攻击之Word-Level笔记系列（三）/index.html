<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>文本对抗攻击之Word-Level笔记系列（三） | BaiDing's blog</title><meta name="description" content="文本对抗攻击之Word-Level笔记系列（三）"><meta name="keywords" content="文本对抗攻击,论文笔记,Word-Level"><meta name="author" content="白丁"><meta name="copyright" content="白丁"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="https://raw.githubusercontent.com/BaiDingHub/Blog_images/master/BlogSource/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="crossorigin"/><link rel="preconnect" href="http://ta.qq.com"/><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="x-1ef6P_miWkq-RJn_fmjd3KYumrXANNXYzK1myaLf0"/><meta name="baidu-site-verification" content="guD6l44Chk"/><meta name="twitter:card" content="summary"><meta name="twitter:title" content="文本对抗攻击之Word-Level笔记系列（三）"><meta name="twitter:description" content="文本对抗攻击之Word-Level笔记系列（三）"><meta name="twitter:image" content="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B9%8BWord-Level%E7%AC%94%E8%AE%B0%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89/cover.png?raw=true"><meta property="og:type" content="article"><meta property="og:title" content="文本对抗攻击之Word-Level笔记系列（三）"><meta property="og:url" content="http://baidinghub.github.io/2021/03/01/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B9%8BWord-Level%E7%AC%94%E8%AE%B0%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89/"><meta property="og:site_name" content="BaiDing's blog"><meta property="og:description" content="文本对抗攻击之Word-Level笔记系列（三）"><meta property="og:image" content="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B9%8BWord-Level%E7%AC%94%E8%AE%B0%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89/cover.png?raw=true"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>var autoChangeMode = '1'
var t = Cookies.get("theme")
if (autoChangeMode == '1'){
  var isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
  var isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
  var isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined){
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport){
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour < 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
  }
  } else if (t == 'light') activateLightMode()
  else activateDarkMode()

} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css" ><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://baidinghub.github.io/2021/03/01/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B9%8BWord-Level%E7%AC%94%E8%AE%B0%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89/"><link rel="next" title="文本对抗攻击之Word-Level笔记系列（二）" href="http://baidinghub.github.io/2021/03/01/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B9%8BWord-Level%E7%AC%94%E8%AE%B0%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%8C%EF%BC%89/"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.js" defer></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-162698439-1', 'auto');
ga('send', 'pageview');
</script><script src="https://tajs.qq.com/stats?sId=66540586" charset="UTF-8"></script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"4JGH9NW4XG","apiKey":"41cdb6d9ec4d21196956524e9c985b36","indexName":"baiding","hits":{"per_page":6},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  highlightShrink: 'false',
  isFontAwesomeV5: false,
  isPhotoFigcaption: false
  
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isSidebar: true  
  }</script><noscript><style>
#page-header {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/avatar.png?raw=true" onerror="onerror=null;src='https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/friend_404.gif?raw=true'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">98</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">61</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">17</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于作者</span></a></div></div></div></div><i class="fa fa-arrow-right on" id="toggle-sidebar" aria-hidden="true">     </i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#一、Robust-Neural-Machine-Translation-with-Doubly-Adversarial-Inputs"><span class="toc-text">一、Robust Neural Machine Translation with Doubly Adversarial Inputs</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Paper-Information"><span class="toc-text">1. Paper Information</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Motivation"><span class="toc-text">2. Motivation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Main-Arguments"><span class="toc-text">3. Main Arguments</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Framework"><span class="toc-text">4. Framework</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Neural-Machine-Translation"><span class="toc-text">4.1 Neural Machine Translation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Approach"><span class="toc-text">4.2 Approach</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-1-Attack-with-Adversarial-Source-Inputs"><span class="toc-text">4.2.1 Attack with Adversarial Source Inputs</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-2-Defense-with-Adversarial-Target-Inputs"><span class="toc-text">4.2.2 Defense with Adversarial Target Inputs</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Result"><span class="toc-text">5.Result</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Performance"><span class="toc-text">4.1 Performance</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Argument"><span class="toc-text">6. Argument</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-Further-research"><span class="toc-text">7. Further research</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#二、Generating-Fluent-Adversarial-Examples-for-Natural-Languages"><span class="toc-text">二、Generating Fluent Adversarial Examples for Natural Languages</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Paper-Information-1"><span class="toc-text">1. Paper Information</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Motivation-1"><span class="toc-text">2. Motivation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Main-Arguments-1"><span class="toc-text">3. Main Arguments</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Framework-1"><span class="toc-text">4. Framework</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Metropolis-Hastings-Sampling"><span class="toc-text">4.1 Metropolis-Hastings Sampling</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Black-Box-Attack"><span class="toc-text">4.2 Black-Box Attack</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-White-Box-Attack"><span class="toc-text">4.3 White-Box Attack</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Result-1"><span class="toc-text">5.Result</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-Adversarial-Attack-Performance"><span class="toc-text">5.1 Adversarial Attack Performance</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-Adversarial-Training-Performance"><span class="toc-text">5.2 Adversarial Training Performance</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Argument-1"><span class="toc-text">6. Argument</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-Further-research-1"><span class="toc-text">7. Further research</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#三、Generating-Natural-Language-Adversarial-Examples-through-Probability-Weighted-Word-Saliency"><span class="toc-text">三、Generating Natural Language Adversarial Examples through Probability Weighted Word Saliency</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Paper-Information-2"><span class="toc-text">1. Paper Information</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Motivation-2"><span class="toc-text">2. Motivation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Main-Arguments-2"><span class="toc-text">3. Main Arguments</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Framework-2"><span class="toc-text">4. Framework</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Word-Substitution-Strategy"><span class="toc-text">4.1 Word Substitution Strategy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Replacement-Order-Strategy"><span class="toc-text">4.2 Replacement Order Strategy</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Result-2"><span class="toc-text">5.Result</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-Setup"><span class="toc-text">5.1 Setup</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-攻击成功率和替换率"><span class="toc-text">5.2 攻击成功率和替换率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-迁移性"><span class="toc-text">5.3 迁移性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-对抗训练"><span class="toc-text">5.4 对抗训练</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Argument-2"><span class="toc-text">6. Argument</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-Further-research-2"><span class="toc-text">7. Further research</span></a></li></ol></li></ol></div></div></div><div id="body-wrap"><div class="post-bg" id="nav" style="background-image: url(https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B9%8BWord-Level%E7%AC%94%E8%AE%B0%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89/cover.png?raw=true)"><div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">BaiDing's blog</a></span><span class="pull_right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 存档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于作者</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span></span></div><div id="post-info"><div id="post-title"><div class="posttitle">文本对抗攻击之Word-Level笔记系列（三）</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2021-03-01 05:22:00"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2021-03-01</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2021-07-03 10:45:03"><i class="fa fa-history" aria-hidden="true"></i> 更新于 2021-07-03</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><i class="fa fa-angle-right post-meta__separator" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97/">文本对抗</a></span></div><div class="meta-secondline"> <span class="post-meta-wordcount"><i class="post-meta__icon fa fa-file-word-o" aria-hidden="true"></i><span>字数总计:</span><span class="word-count">3.1k</span><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-clock-o" aria-hidden="true"></i><span>阅读时长: 12 分钟</span></span></div><div class="meta-thirdline"><span class="post-meta-pv-cv"><span class="post-meta__separator">|</span><i class="fa fa-eye post-meta__icon" aria-hidden="true"> </i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-comment-o" aria-hidden="true"></i><span>评论数:</span><a href="/2021/03/01/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B9%8BWord-Level%E7%AC%94%E8%AE%B0%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89/#post-comment"><span class="gitalk-comment-count comment-count"></span></a></span></div></div></div></div><main class="layout_post" id="content-inner"><article id="post"><div id="article-container"><meta name="referrer" content="no-referrer"/>



<h1 id="一、Robust-Neural-Machine-Translation-with-Doubly-Adversarial-Inputs"><a href="#一、Robust-Neural-Machine-Translation-with-Doubly-Adversarial-Inputs" class="headerlink" title="一、Robust Neural Machine Translation with Doubly Adversarial Inputs"></a>一、Robust Neural Machine Translation with Doubly Adversarial Inputs</h1><h2 id="1-Paper-Information"><a href="#1-Paper-Information" class="headerlink" title="1. Paper Information"></a>1. Paper Information</h2><blockquote>
<p>时间：2019年</p>
<p>关键词：Adversarial Attack，Neural Machine Translation，Gradient</p>
<p>论文位置：<a href="https://arxiv.org/pdf/1906.02443.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1906.02443.pdf</a></p>
<p>引用：Yong Cheng, Lu Jiang, Wolfgang Macherey:Robust Neural Machine Translation with Doubly Adversarial Inputs. ACL (1) 2019: 4324-4333</p>
<p>出处：Google AI</p>
</blockquote>
<h2 id="2-Motivation"><a href="#2-Motivation" class="headerlink" title="2. Motivation"></a>2. Motivation</h2><p> &emsp;&emsp; 机器翻译也是容易受到对抗噪声干扰的。我们提出了一种方法生成NMT的对抗样本并提高NMT模型的鲁棒性。</p>
<h2 id="3-Main-Arguments"><a href="#3-Main-Arguments" class="headerlink" title="3. Main Arguments"></a>3. Main Arguments</h2><p> &emsp;&emsp; 我们提出了一种NMT的对抗攻击方法AdvGen，通过translation loss来生成对抗样本。AdvGen被应用在encoding和decoding阶段。并利用生成的对抗样本提高了NMT模型的鲁棒性。</p>
<h2 id="4-Framework"><a href="#4-Framework" class="headerlink" title="4. Framework"></a>4. Framework</h2><h3 id="4-1-Neural-Machine-Translation"><a href="#4-1-Neural-Machine-Translation" class="headerlink" title="4.1 Neural Machine Translation"></a>4.1 Neural Machine Translation</h3><p> &emsp;&emsp; NMT模型首先将输入$ x = x_1,…,x_I$ 映射成词向量$ e(x) = e(x_1),…,e(x_I)$ ，将其作为encoder的输入，得到对应的连续空间的隐藏层representation $ h$ 。之后，decoder的输入是$ z = z_1,…,z_J$ 和$ h$ ，输出是$ y = y_1,…,y_{J-1}$ 。实际上，$ z$ 是$ y$ 偏移后的版本，即$ z = <sos>,y_1,…,y_{J-1}$ ，其中$ <sos>$ 表示开始标志，这说明decoder用到了句子之前的信息。NMT的输出为：</p>
<script type="math/tex; mode=display">
P(y|x;\theta_{mt}) = \prod_{j=1}^J P(y_j | z_{\le j},h;\theta_{mt})</script><p> &emsp;&emsp; NMT的loss为：</p>
<script type="math/tex; mode=display">
L_{clean}(\theta_{mt}) = \frac{1}{|S|} \sum_{(x,y) \in S} - \log P(y|x;\theta_{mt})</script><h3 id="4-2-Approach"><a href="#4-2-Approach" class="headerlink" title="4.2 Approach"></a>4.2 Approach</h3><p> &emsp;&emsp; NMT的攻击和防御在这个端到端的训练过程中执行。我们首先使用AdvGen生成对抗样本$ x’$ 来攻击NMT模型。之后使用AdvGen找到对抗目标输入$ z’$ 来提高NMT模型的鲁棒性。</p>
<h4 id="4-2-1-Attack-with-Adversarial-Source-Inputs"><a href="#4-2-1-Attack-with-Adversarial-Source-Inputs" class="headerlink" title="4.2.1 Attack with Adversarial Source Inputs"></a>4.2.1 Attack with Adversarial Source Inputs</h4><p> &emsp;&emsp; 在NMT中，我们的对抗攻击要解决的目标模型为：</p>
<script type="math/tex; mode=display">
\{x'|R(x',x)\le \epsilon, \arg\max_{x'} -\log P(y|x';\theta_{mt})\}</script><p> &emsp;&emsp; 要解决上面的问题是很难的，所以采用了贪心算法来解决。对于原始输入$ x$ ，我们要为单词$ x_i$ 找到一个可能的对抗单词$ x_i’$ ，即：</p>
<script type="math/tex; mode=display">
x_i' = \arg\max_{x \in V_x}\ sim(e(x) - e(x_i), g_{x_i}) \\
g_{x_i} = \nabla_{e(x_i)} -\log P(y|x;\theta)</script><p> &emsp;&emsp; 论文中使用了语言模型的输出来进行候选替代单词的筛选，但论文中描述算法很乱，这里就不讲解。</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B9%8BWord-Level%E7%AC%94%E8%AE%B0%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89/1.png?raw=true"  alt="1" style="zoom:50%;" /></p>
<h4 id="4-2-2-Defense-with-Adversarial-Target-Inputs"><a href="#4-2-2-Defense-with-Adversarial-Target-Inputs" class="headerlink" title="4.2.2 Defense with Adversarial Target Inputs"></a>4.2.2 Defense with Adversarial Target Inputs</h4><p> &emsp;&emsp; 我们使用$ x’$ ，利用了上面的算法生成了$ z’$ ，将$ (x’,z’)$ 作为训练样本加入到训练集中，来提高模型的鲁棒性。</p>
<h2 id="5-Result"><a href="#5-Result" class="headerlink" title="5.Result"></a>5.Result</h2><h3 id="4-1-Performance"><a href="#4-1-Performance" class="headerlink" title="4.1 Performance"></a>4.1 Performance</h3><p> &emsp;&emsp; 可以看出，经过对抗训练后，NMT模型的性能提升了</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B9%8BWord-Level%E7%AC%94%E8%AE%B0%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89/2.png?raw=true"  alt="2"></p>
<p> &emsp;&emsp; 使用对抗攻击后，模型的性能变差</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B9%8BWord-Level%E7%AC%94%E8%AE%B0%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89/3.png?raw=true"  alt="3"></p>
<h2 id="6-Argument"><a href="#6-Argument" class="headerlink" title="6. Argument"></a>6. Argument</h2><h2 id="7-Further-research"><a href="#7-Further-research" class="headerlink" title="7. Further research"></a>7. Further research</h2><h1 id="二、Generating-Fluent-Adversarial-Examples-for-Natural-Languages"><a href="#二、Generating-Fluent-Adversarial-Examples-for-Natural-Languages" class="headerlink" title="二、Generating Fluent Adversarial Examples for Natural Languages"></a>二、Generating Fluent Adversarial Examples for Natural Languages</h1><h2 id="1-Paper-Information-1"><a href="#1-Paper-Information-1" class="headerlink" title="1. Paper Information"></a>1. Paper Information</h2><blockquote>
<p>时间：2019年</p>
<p>关键词：Adversarial Attack，fluency，</p>
<p>论文位置：<a href="https://arxiv.org/pdf/2007.06174.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2007.06174.pdf</a></p>
<p>引用：Huangzhao Zhang, Hao Zhou, Ning Miao, Lei Li: Generating Fluent Adversarial Examples for Natural Languages. ACL (1) 2019: 5564-5569</p>
<p>出处：Peking University, ByteDance AI Lab</p>
</blockquote>
<h2 id="2-Motivation-1"><a href="#2-Motivation-1" class="headerlink" title="2. Motivation"></a>2. Motivation</h2><p> &emsp;&emsp; 由于文本的离散型，文本对抗样本难以利用梯度的信息，而梯度信息又十分重要。另外，对抗样本的流畅度通常是不够的，而不流畅的文本（对抗文本）会容易被分类器鉴别，而且使用他们进行对抗训练的效果也不是很好。而目前的方法，比如HotFlip翻转字符容易将一个词转换成毫无意义的单词，比如mood -&gt; moop，而GA虽然生成了较为流畅的句子，但是效果也不够，而且没有使用梯度信息，效率较低。为了解决上述问题，有效率的生成流畅性高的对抗样本，我们提出了Metropolis-Hastingsattack(MHA)方法，通过进行 Metropolis-Hastings采样来解决上述的问题。</p>
<h2 id="3-Main-Arguments-1"><a href="#3-Main-Arguments-1" class="headerlink" title="3. Main Arguments"></a>3. Main Arguments</h2><p> &emsp;&emsp; MH采样属于传统的MCMC采样方法，其是根据梯度信息来进行采样。另外，我们提出了两个MHA的变体，即黑盒MHA（b-MHA）和白盒MHA（w-MHA）</p>
<h2 id="4-Framework-1"><a href="#4-Framework-1" class="headerlink" title="4. Framework"></a>4. Framework</h2><h3 id="4-1-Metropolis-Hastings-Sampling"><a href="#4-1-Metropolis-Hastings-Sampling" class="headerlink" title="4.1 Metropolis-Hastings Sampling"></a>4.1 Metropolis-Hastings Sampling</h3><p> &emsp;&emsp; M-H采样是一种经典的马尔可夫链蒙特卡罗采样方法。给定一个<strong>Stationary distribution$ \pi(x)$</strong> 和<strong>transition proposal</strong>（从一个文本转变到另一个文本的依据）。M-H会根据分布$ \pi(x)$ 来生成样本。在每次迭代中，基于proposal分布$ g(x’|x)$ ，我们会给出将$ x$ 转变成$ x’$ 的proposal。该proposal是否被接受，需要依据下面的概率值：</p>
<script type="math/tex; mode=display">
\alpha(x'|x) = \min\{1,\frac{\pi(x')g(x|x')}{\pi(x)g(x'|x)} \}</script><p> &emsp;&emsp; 一旦该propsoal被接受，那么算法将跳到$ x’$ 上继续进行，否则留在$ x$ 上。</p>
<h3 id="4-2-Black-Box-Attack"><a href="#4-2-Black-Box-Attack" class="headerlink" title="4.2 Black-Box Attack"></a>4.2 Black-Box Attack</h3><p> &emsp;&emsp; 在黑盒攻击中，我们希望我们生成的样本满足三个要求：1、读起来要流畅，2、能欺骗分类器，3、尽可能少的调用分类器。</p>
<p> &emsp;&emsp; 对于<strong>Stationary distribution</strong>，我们可以这样设计：</p>
<script type="math/tex; mode=display">
\pi(x|\tilde{y}) \propto LM(x)·C(\tilde{y}|x)</script><p> &emsp;&emsp; 其中，$ LM(x)$ 表示对于预训练语言模型LM在句子$ x$ 上的概率值，$ C(\tilde{y}|x)$ 表示在victim model上句子$ x$ 在目标标签（错误标签）上的概率值。其中，$ LM(x)$ 保证了流畅性，$ C(\tilde{y}|x)$ 保证了攻击目标。</p>
<p> &emsp;&emsp; 对于<strong>Transition proposal</strong>，有三种word-level的转变操作，替换、插入和删除。我们使用Traversal indexing来记录要执行哪一个操作。假设第t次proposal中，MHA选择了第i个单词$ w_i$ 。那么，第t+1次proposal，选择的单词$ w^*$会是：（就是选择下一个单词）</p>
<script type="math/tex; mode=display">
w^* = 
\begin{cases}
w_{i+1} & & if\ i\ne n\\
w_1 & & otherwise
\end{cases} \\</script><p> &emsp;&emsp; 对于替换操作$ T_r^B(x’|x)$ ，我们的函数方程如公式3所示，其中$ w_m$ 是选中被替换的单词，$ Q$ 表示预训练的候选词集。插入操作$ T_i^B(x’|x)$ 包含两步，首先在该位置插入一个随机的单词，之后在该位置进行替换操作。对于删除操作$ T_d^B(x’|x)$ ，$ T_d^B(x’|x) =1 if x’ = x_{-m}$ ，其中$ x_{-m}$ 表示删除了第m个单词后的句子。否则，$ T_d^B(x’|x) = 0$ 。</p>
<script type="math/tex; mode=display">
T_r^B(x'|x) = I\{w^c \in Q\}·\frac{\pi(w_1,...,w_{m-1},w^c,w_{m+1},...,w_n|\tilde{y})}{\sum_{w\in Q}\pi(w_1,...,w_{m-1},w,w_{m+1},...,w_n|\tilde{y})}</script><p> &emsp;&emsp; 注意，$ x$ 表示操作前的文本，$ x’$ 表示操作后的文本。$ T^B(x’|x)$ 表示进行某项操作将$ x$ 转变成$ x’$ 的proposal值。</p>
<p> &emsp;&emsp; 而proposal分布$ g(x’|x)$ 是上述操作的权重和：</p>
<script type="math/tex; mode=display">
g(x'|x) = p_rT_r^B(x'|x) +  p_iT_i^B(x'|x) +  p_dT_d^B(x'|x)</script><p> &emsp;&emsp; 其中，$ p_r,p_i,p_d$ 是预先定义的各种操作的权重。</p>
<p> &emsp;&emsp; 注意，该方法中候选词集是通过语言模型LM生成的，即根据分数$ S^B(w|x) = LM(w|x_{[1:m-1]}) · LM_b(w|x_{m+1:n})$ 。</p>
<h3 id="4-3-White-Box-Attack"><a href="#4-3-White-Box-Attack" class="headerlink" title="4.3 White-Box Attack"></a>4.3 White-Box Attack</h3><p> &emsp;&emsp; 白盒攻击和黑盒攻击的区别在于候选词集的选择。</p>
<p> &emsp;&emsp; 我们根据分数$ S^W(w|x)$ 进行选择：</p>
<script type="math/tex; mode=display">
S^W(w|x) = S^B(w|x) · S(\frac{\partial \tilde{L}}{\partial e_m},e_m - e)</script><p> &emsp;&emsp; 其中，$ S$ 是余弦相似度函数，$ \tilde{L} = L(\tilde{y}|x,C)$（扰动）。$ e_m$ 表示第m个词$ w_m$ 的词向量，$ e$ 表示替换词$ w$ 的词向量。其中，梯度$ \frac{\partial \tilde{L}}{\partial e_m}$ 确定了扰动方向，$ e_m - e$ 表示了实际的运动方向。这使得，我们会朝着使损失函数最大的方向进行移动，这使得我们的方法效率会更高。</p>
<p>注意，在w-MHA中不进行插入和删除操作，因为很难求梯度。</p>
<h2 id="5-Result-1"><a href="#5-Result-1" class="headerlink" title="5.Result"></a>5.Result</h2><h3 id="5-1-Adversarial-Attack-Performance"><a href="#5-1-Adversarial-Attack-Performance" class="headerlink" title="5.1 Adversarial Attack Performance"></a>5.1 Adversarial Attack Performance</h3><p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B9%8BWord-Level%E7%AC%94%E8%AE%B0%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89/4.png?raw=true"  alt="4"></p>
<p> &emsp;&emsp; 其中PPL表示perplexity，即表示文本的流畅度，越小，困惑越低，流畅度越高。</p>
<h3 id="5-2-Adversarial-Training-Performance"><a href="#5-2-Adversarial-Training-Performance" class="headerlink" title="5.2 Adversarial Training Performance"></a>5.2 Adversarial Training Performance</h3><p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B9%8BWord-Level%E7%AC%94%E8%AE%B0%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89/5.png?raw=true"  alt="5"></p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B9%8BWord-Level%E7%AC%94%E8%AE%B0%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89/6.png?raw=true"  alt="6"></p>
<h2 id="6-Argument-1"><a href="#6-Argument-1" class="headerlink" title="6. Argument"></a>6. Argument</h2><p> &emsp;&emsp; 该方法融合了替换、删除和插入操作，单独跟score-based的GA相比有点不太公平。主要是利用了语言模型选择候选词集，来提高对抗样本的可读性、流畅性。</p>
<h2 id="7-Further-research-1"><a href="#7-Further-research-1" class="headerlink" title="7. Further research"></a>7. Further research</h2><h1 id="三、Generating-Natural-Language-Adversarial-Examples-through-Probability-Weighted-Word-Saliency"><a href="#三、Generating-Natural-Language-Adversarial-Examples-through-Probability-Weighted-Word-Saliency" class="headerlink" title="三、Generating Natural Language Adversarial Examples through Probability Weighted Word Saliency"></a>三、Generating Natural Language Adversarial Examples through Probability Weighted Word Saliency</h1><h2 id="1-Paper-Information-2"><a href="#1-Paper-Information-2" class="headerlink" title="1. Paper Information"></a>1. Paper Information</h2><blockquote>
<p>时间：2019年</p>
<p>关键词：Adversarial Attack，Text Classfication，PWWS</p>
<p>论文位置：<a href="https://www.aclweb.org/anthology/P19-1103.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/P19-1103.pdf</a></p>
<p>引用：Shuhuai Ren, Yihe Deng, Kun He, Wanxiang Che: Generating Natural Language Adversarial Examples through Probability Weighted Word Saliency. ACL (1) 2019: 1085-1097</p>
<p>出处：Huazhong University of Science and Technology</p>
</blockquote>
<h2 id="2-Motivation-2"><a href="#2-Motivation-2" class="headerlink" title="2. Motivation"></a>2. Motivation</h2><p> &emsp;&emsp; 目前，文本对抗领域的难点在于保持词汇正确、语法正确、语义相似的前提下生成对抗样本。</p>
<h2 id="3-Main-Arguments-2"><a href="#3-Main-Arguments-2" class="headerlink" title="3. Main Arguments"></a>3. Main Arguments</h2><p> &emsp;&emsp; 基于同义词替换策略，本文引入了一种新的单词替换顺序，我们利用单词的重要程度和分类概率，提出了一种贪心算法，称之为probability weighted word saliency (PWWS)  。PWWS能够在比较低的替换率下实现攻击。分类概率的变化用来测量替换单词的攻击效果，词的saliency用来测量原始词对分类的影响程度。使用词saliency做权重的词概率变化值，来决定最终的替换词和替换顺序。</p>
<p> &emsp;&emsp; 一般的攻击策略会采用插入、删除、替换策略，但是采用了插入和删除后，为了保持语义上的相似性并避免人为的发现，它需要人工努力，例如在线搜索相关事实以进行插入。因此，本篇论文主要采用了替换策略，来实现自动生成，而不需要人力。</p>
<h2 id="4-Framework-2"><a href="#4-Framework-2" class="headerlink" title="4. Framework"></a>4. Framework</h2><p> &emsp;&emsp; 在PWWS中，我们使用同义词进行替换，使用别的类别中命令实体集替换该样本中的命名实体。这需要解决两个问题：</p>
<ul>
<li>如何选择同义词和命名实体（NE）</li>
<li>如何决定替换顺序</li>
</ul>
<h3 id="4-1-Word-Substitution-Strategy"><a href="#4-1-Word-Substitution-Strategy" class="headerlink" title="4.1 Word Substitution Strategy"></a>4.1 Word Substitution Strategy</h3><p> &emsp;&emsp; 对于$ x$ 中的每一个单词$ w_i$ ，我们使用WoedNet来找到该单词的同义词集合$ \mathbb{L}_i \subseteq \mathbb{D} $ ，如果该单词是一个命名实体，那么，我们就将别的类别的命名实体$ NE_{adv}$ 加入到$ \mathbb{L}_i$ 中。之后，我们从$ \mathbb{L}_i$ 选择一个单词$ w’_i$ ，如果它使得分类概率发生了最大的改变，那么我们就替换他，即：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{split}
w_i ^* &= R(w_i, \mathbb{L}_i)\\
&=\arg \max_{w'_i \in \mathbb{L}_i} {P(y_{true}|x) - p(y_{true}|x_i')} \\
\text{where} \\
x&=w_1w_2...w_i...w_n,\\
x_i' &= w_1w_2...w_i'...w_n
\end{split}
\end{equation}
 \tag{4}</script><p> &emsp;&emsp; 所以，我们就解决了第一个问题。</p>
<p> &emsp;&emsp; 同时，我们得到了该词对分类的影响程度：</p>
<script type="math/tex; mode=display">
\Delta P_i^* = P(y_{true}|x) - P(y_{true}|x_i^*)</script><h3 id="4-2-Replacement-Order-Strategy"><a href="#4-2-Replacement-Order-Strategy" class="headerlink" title="4.2 Replacement Order Strategy"></a>4.2 Replacement Order Strategy</h3><p> &emsp;&emsp; 在输入样本中，不同的词对分类的影响程度不同。我们引入了word saliency来决定替换顺序。word saliency是指将该单词设为unkown后，输出概率的变化程度，即：</p>
<script type="math/tex; mode=display">
S(x, w_i) = P(y_{true}|x) - P(y_{true }| \hat{x}_i)  \\
\hat{x}_i = w_1w_2...unknown...w_d \tag{6}</script><p> &emsp;&emsp; 我们为每一个单词计算他的word saliency，那么我们就得到了$ S(x) $ 。</p>
<p> &emsp;&emsp; 为了得到替换的优先度，我们需要考虑在替换后的分类概率以及每个词的word saliency。所以，我们为每个单词计算出了一个score，即：</p>
<script type="math/tex; mode=display">
H(x, x_i^*, w_i) = \phi(S(x))_i · \Delta P_i^* \\
\phi(z)_i = \frac{e^z_i}{\sum_{k=1}^K e^{z_k}}</script><p> &emsp;&emsp; 根据这个score，我们就得到了替换顺序，就解决了第二个问题，所以我们通过迭代的替换，就得到了对抗样本。</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B9%8BWord-Level%E7%AC%94%E8%AE%B0%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89/7.png?raw=true"  alt="7" style="zoom:50%;" /></p>
<h2 id="5-Result-2"><a href="#5-Result-2" class="headerlink" title="5.Result"></a>5.Result</h2><h3 id="5-1-Setup"><a href="#5-1-Setup" class="headerlink" title="5.1 Setup"></a>5.1 Setup</h3><ul>
<li>数据集：采用了IMDB、AG’s News、Yahoo! Answers</li>
<li>分类器：Word-based CNN、Bi-directional LSTM、Char-based CNN、LSTM</li>
<li>攻击策略：<ul>
<li>Random：随机的选择同义词，然后进行替换直到分类结果发生改变。</li>
<li>Gradient：使用了FGSM的思想，选择同义词使得分类输出的而变化最大，即$ \Delta F(x) = F(x’) - F(x) \approx (x’_i - x_i) \frac{\partial F(x)}{\partial x_i}$ 。</li>
<li>Traversing in word order (TiWO)：该方法是利用等式4来进行词的替换</li>
<li>Word Saliency (WS)：该方法是基于等式6来排序，然后利用等式4进行替换</li>
</ul>
</li>
</ul>
<h3 id="5-2-攻击成功率和替换率"><a href="#5-2-攻击成功率和替换率" class="headerlink" title="5.2 攻击成功率和替换率"></a>5.2 攻击成功率和替换率</h3><p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B9%8BWord-Level%E7%AC%94%E8%AE%B0%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89/8.png?raw=true"  alt="8"></p>
<h3 id="5-3-迁移性"><a href="#5-3-迁移性" class="headerlink" title="5.3 迁移性"></a>5.3 迁移性</h3><p> &emsp;&emsp; 在一个模型上生成对抗样本，送入以下三个模型中。</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B9%8BWord-Level%E7%AC%94%E8%AE%B0%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89/9.png?raw=true"  alt="9"></p>
<h3 id="5-4-对抗训练"><a href="#5-4-对抗训练" class="headerlink" title="5.4 对抗训练"></a>5.4 对抗训练</h3><p> &emsp;&emsp; 利用PWWS生成对抗样本加入到数据集中，在原始样本和对抗样本下的成功率。</p>
<p><img src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/loading.gif?raw=true" class="lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B9%8BWord-Level%E7%AC%94%E8%AE%B0%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89/10.png?raw=true"  alt="10"></p>
<h2 id="6-Argument-2"><a href="#6-Argument-2" class="headerlink" title="6. Argument"></a>6. Argument</h2><p> &emsp;&emsp; 本篇论文主要解决了两个问题，如何选词？先选哪个词？提出的priority策略是别的论文中的整合成策略。</p>
<h2 id="7-Further-research-2"><a href="#7-Further-research-2" class="headerlink" title="7. Further research"></a>7. Further research</h2></div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">白丁</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://baidinghub.github.io/2021/03/01/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B9%8BWord-Level%E7%AC%94%E8%AE%B0%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89/">http://baidinghub.github.io/2021/03/01/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B9%8BWord-Level%E7%AC%94%E8%AE%B0%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://baidinghub.github.io" target="_blank">BaiDing's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB/">文本对抗攻击</a><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><a class="post-meta__tags" href="/tags/Word-Level/">Word-Level</a></div><div class="post_share"><div class="addthis_inline_share_toolbox"></div><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDyIdOn/M/aXcGQOiSDVMqvN2gxDdiESHUfFuB2YMy48fvNN9SZOQUbVlGF4Pk6nDXIAir+br/EWuEnNLtgOCYMo/BTxl29gqS/QGHPiDaIQedzmLcuRZpfDuGit61N/b9pyktpZLagBgbl5Ox9mAgWQxXyhxYB092gyOXqrBULBeZUYQ+H7Eupha10QTQghHv4nLk+oYWo2UXEiijQpE3qMXT32G8v8k0KbRdd1hIFPyNEx6eZ6Buc2ZdbMtoutdGjvdnw5B1+dxgl1egESPChzcwCMxt3NW/3faQ5lsjRhQD4fRU+Ua/aqXe+0xg2+xr0BTjNr/JZIAaNiLuo71lH 1564026260@qq.com" async="async"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/wechat.png?raw=true" alt="微信"/><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/alipay.jpg?raw=true" alt="支付寶"/><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="next-post pull-full"><a href="/2021/03/01/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B9%8BWord-Level%E7%AC%94%E8%AE%B0%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%8C%EF%BC%89/"><img class="next_cover lazyload" data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B9%8BWord-Level%E7%AC%94%E8%AE%B0%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%8C%EF%BC%89/cover.png?raw=true" onerror="onerror=null;src='https://github.com/BaiDingHub/Blog_images/blob/master/BlogSource/404.jpg?raw=true'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">文本对抗攻击之Word-Level笔记系列（二）</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2021/03/01/文本对抗攻击之Word-Level笔记系列（一）/" title="文本对抗攻击之Word-Level笔记系列（一）"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B9%8BWord-Level%E7%AC%94%E8%AE%B0%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%80%EF%BC%89/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2021-03-01</div><div class="relatedPosts_title">文本对抗攻击之Word-Level笔记系列（一）</div></div></a></div><div class="relatedPosts_item"><a href="/2021/03/01/文本对抗攻击之Word-Level笔记系列（二）/" title="文本对抗攻击之Word-Level笔记系列（二）"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B9%8BWord-Level%E7%AC%94%E8%AE%B0%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%8C%EF%BC%89/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2021-03-01</div><div class="relatedPosts_title">文本对抗攻击之Word-Level笔记系列（二）</div></div></a></div><div class="relatedPosts_item"><a href="/2021/03/01/文本对抗之博客列表/" title="文本对抗之博客列表"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B9%8B%E5%8D%9A%E5%AE%A2%E5%88%97%E8%A1%A8/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2021-03-01</div><div class="relatedPosts_title">文本对抗之博客列表</div></div></a></div><div class="relatedPosts_item"><a href="/2021/03/01/文本对抗攻击之综述/" title="文本对抗攻击之综述"><img class="relatedPosts_cover lazyload"data-src="https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B9%8B%E7%BB%BC%E8%BF%B0/cover.png?raw=true"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2021-03-01</div><div class="relatedPosts_title">文本对抗攻击之综述</div></div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: 'd3948be001a91411dfd9',
  clientSecret: 'ebddf2a2a5a039922fb373a8a8c0efcc439bf6ca',
  repo: 'BaiDingHub.github.io',
  owner: 'BaiDingHub',
  admin: ['BaiDingHub'],
  id: md5(decodeURI(location.pathname)),
  language: 'zh-CN',
  perPage: 10,
  distractionFreeMode: false,
  pagerDirection: 'last',
  createIssueManually: false,
  updateCountCallback: commentCount
})
gitalk.render('gitalk-container')

function commentCount(n){
  try {
    document.getElementsByClassName('gitalk-comment-count')[0].innerHTML= n
  } catch (e) {
    return false
  }
}</script></div></article></main><footer id="footer" style="background-image: url(https://github.com/BaiDingHub/Blog_images/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97/%E6%96%87%E6%9C%AC%E5%AF%B9%E6%8A%97%E6%94%BB%E5%87%BB%E4%B9%8BWord-Level%E7%AC%94%E8%AE%B0%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89/cover.png?raw=true)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By 白丁</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">Hi, welcome to my <a href="https://jerryc.me/" target="_blank" rel="noopener">blog</a>!</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js" ></script><script src="/js/utils.js" ></script><script src="/js/main.js" ></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><script src="/js/search/algolia.js"></script></body></html>